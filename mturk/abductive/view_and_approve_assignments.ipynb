{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea0d6ab",
   "metadata": {},
   "source": [
    "# MTurk: Approve / Reject Assignments + Post-Task Completion Processing of HITs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a65b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import pprint\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6058e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = pd.read_json('mturk_data/creation/pilot.json', dtype=dict)\n",
    "mturk = boto3.client(\n",
    "    'mturk', \n",
    "    aws_access_key_id = 'AKIA3HQJKSL4YZUFYGQ4', \n",
    "    aws_secret_access_key = '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',\n",
    "    region_name='us-east-1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ddf9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks['hit_id'] = tasks.posted_hits.map(lambda x: x['HIT'].split(':')[1].split(',')[0].strip()[1:-1]) #hacky since there were problems with json loading\n",
    "tasks['example_data'] = tasks.apply(lambda x: x['tasks']['example_data'], axis=1)\n",
    "tasks['mturk_assignments'] = tasks.hit_id.map(\n",
    "    lambda hit: mturk.list_assignments_for_hit(HITId=hit, MaxResults=5)['Assignments']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf754b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_assignments(row):\n",
    "\n",
    "    processed_assignments = []\n",
    "\n",
    "    for assign in row.mturk_assignments:\n",
    "        assignment = ast.literal_eval(\n",
    "            (re.search('<FreeText>(.*)</FreeText>', assign['Answer']).group(1)))[0]\n",
    "        if row.example_data['label'] == 1: \n",
    "            hyp1_paraphrases = [v for k, v in assignment.items() if k.startswith('paraphrase_correct')]\n",
    "            hyp2_paraphrases = [v for k, v in assignment.items() if k.startswith('paraphrase_incorrect')]\n",
    "        else:\n",
    "            hyp1_paraphrases = [v for k, v in assignment.items() if k.startswith('paraphrase_incorrect')]\n",
    "            hyp2_paraphrases = [v for k, v in assignment.items() if k.startswith('paraphrase_correct')]\n",
    "\n",
    "        processed_assignments.append({'hyp1_paraphrases': hyp1_paraphrases, 'hyp2_paraphrases': hyp2_paraphrases})\n",
    "    \n",
    "    return processed_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef3f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks['processed_assignments'] = tasks.apply(process_assignments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e880ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.to_csv('mturk_data/results/pilot-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mturk.get_assignment(AssignmentId='3R6P78PK7KBM0YDA1EE6CY12GMSGT6')['Assignment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk.get_assignment(AssignmentId='3G2UL9A02DEX57RXCY2JVBWVXD267Q')['Assignment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_assignment(assignment_id):\n",
    "    assignment = mturk.get_assignment(AssignmentId=assignment_id)['Assignment']\n",
    "    result = re.search('<FreeText>(.*)</FreeText>', assignment['Answer']).group(1)\n",
    "    result = ast.literal_eval(result)[0]\n",
    "    pprint.pprint(result)\n",
    "\n",
    "    \n",
    "    hit = int(mturk.get_hit(HITId=assignment['HITId'])['HIT']['RequesterAnnotation'])\n",
    "    assert len(tasks.loc[tasks['example_ids'] == hit]) == 1\n",
    "    \n",
    "    ex = tasks.loc[tasks['example_ids'] == hit].iloc[0]['tasks']['example_data']\n",
    "\n",
    "    plausible = ex['hyp1'] if ex['label'] == 1 else ex['hyp2']\n",
    "    implausible = ex['hyp1'] if ex['label'] == 2 else ex['hyp2']\n",
    "    o1 = ex['obs1']\n",
    "    o2 = ex['obs2']\n",
    "    \n",
    "    print()\n",
    "#    print('O1:', o1)\n",
    "    print('Plausible:', plausible)\n",
    "    print('Implausible:', implausible)\n",
    "#    print('O2:', o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6173639",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_assignment('3M23Y66PO27JO5BP010UQ2SRKOR6SJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "approval_response = mturk.approve_assignment(\n",
    "    AssignmentId='32EYX73OY09SP11DHUYSGY1B188RU0',\n",
    "    RequesterFeedback='This is great! Thanks for being so thorough :)'\n",
    ")\n",
    "approval_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = mturk.reject_assignment(\n",
    "    AssignmentId='3Q8GYXHFEP27YKMFYW0PJRA92Q65CK',\n",
    "    RequesterFeedback='Sorry, these sentences are not coherent.'\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('/Users/nehasrikanth/Documents/paraphrase-nlu/raw-data/anli/test.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = pd.read_csv('/Users/nehasrikanth/Documents/paraphrase-nlu/raw_data/anli/test-labels.lst', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7470206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithHeads, AutoTokenizer\n",
    "\n",
    "model = AutoModelWithHeads.from_pretrained(\"roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "adapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-art\", source=\"hf\")\n",
    "model.active_adapters = adapter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def get_prediction(example):\n",
    "    order = [\"obs1\", \"obs2\", \"hyp1\", \"hyp2\"]\n",
    "    a_s = [example[order[0]] + \" \" + example[order[2]], example[order[0]] + \" \" + example[order[3]]]\n",
    "    b_s = [example[order[1]] for _ in range(2)]\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        a_s,\n",
    "        b_s,\n",
    "        max_length=100,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    probs = softmax(model(**encoded).logits.detach().numpy(), axis=1)\n",
    "    return int(probs.argmax()) + 1\n",
    "\n",
    "\n",
    "def get_paraphrase_prediction(example):\n",
    "    obs1 = example.example_data['obs1']\n",
    "    obs2 = example.example_data['obs2']\n",
    "    hyp1 = example.example_data['hyp1'] #example['processed_assignments'][0]['hyp1_paraphrases'][0]\n",
    "    hyp2 = example['processed_assignments'][0]['hyp2_paraphrases'][0]\n",
    "    \n",
    "    a_s = [obs1 + \" \" + hyp1, obs1 + \" \" + hyp2]\n",
    "    b_s = [obs2 for _ in range(2)]\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        a_s,\n",
    "        b_s,\n",
    "        max_length=100,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    probs = softmax(model(**encoded).logits.detach().numpy(), axis=1)\n",
    "    return int(probs.argmax()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "tasks['original_pred'] = tasks.example_data.progress_map(get_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241f134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tasks['para_pred'] = tasks.progress_apply(get_paraphrase_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2406ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d53379",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(tasks.example_data.map(lambda x: x['label']), tasks.para_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd1364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adapter]",
   "language": "python",
   "name": "conda-env-adapter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
