{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75734bff-49af-4c09-8b7c-1b6b44800de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from experiments.pretraining_vs_finetuning.finetuning_volume.volume_buckets import finetuning_proportion_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af622230-f67a-4fd1-b654-69f54b3bb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "def plot_isocurves(ax: matplotlib.axes.Axes):\n",
    "    \n",
    "    # plot the isocurves\n",
    "    bernoullis = np.linspace(0, 1, 1000)\n",
    "    total_variance = bernoullis * (1-bernoullis)\n",
    "    \n",
    "    pove_percentages = [i / 10 for i in range(10)]\n",
    "    palette = sns.color_palette(\"flare\", len(pove_percentages)) #crest\n",
    "    \n",
    "    for i, pove in enumerate(pove_percentages):\n",
    "        min_pstay = 1 - 2*((1-pove)*total_variance) # 2 * UV, but max is when all variance is UV\n",
    "        ax.plot(bernoullis, min_pstay, '--', alpha=0.3, color=palette[i])\n",
    "        annotation_point = 750\n",
    "        curve_text = f\"{100 - int(100 * pove)}% PVAP\" if pove > 0 else \"Min P(STAY)\"\n",
    "        ax.annotate(\n",
    "            curve_text, \n",
    "            (bernoullis[annotation_point], min_pstay[annotation_point]+ 0.008), \n",
    "            rotation=20, \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='center', \n",
    "            alpha=0.3, \n",
    "            color='black',\n",
    "            fontsize=8\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3921cc-66de-4588-bb00-5df1aef24e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': ':'})\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_finetuning_trend(dataset):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for prop, datasets in finetuning_proportion_buckets.items():\n",
    "    \n",
    "        summary = datasets[f'{dataset}-human'].linguistic_robustness_summary(datasets[f'{dataset}-test'])\n",
    "        acc = summary['paraphrase_accuracy_corrected'] #datasets[f'{dataset}-test'].accuracy\n",
    "        p_stay_c = summary['stay_prob_corrected']\n",
    "        \n",
    "        x.append(acc)\n",
    "        y.append(p_stay_c)\n",
    "    \n",
    "    sns.lineplot(x=x, y=y, marker='o', label=dataset, alpha=0.5)\n",
    "    for i, words in enumerate(list(finetuning_proportion_buckets.keys())):\n",
    "\n",
    "        dataset_offsets = {\n",
    "            'social': {0.01: [0.01, 0], 0.05: [0.01, 0.005], 0.1: [0.01, 0.01],0.5: [0.007, -0.012], 1: [0.014, 0.008]},\n",
    "            'snli': {0.01: [0.01, 0.005], 0.05: [0.01, -0.001], 0.1: [-0.012, -0.001], 0.5: [-0.002, 0.005], 1: [-0.009, 0]},\n",
    "            'atomic': {0.01: [-0.004, 0.005], 0.05: [-0.01, -0.001], 0.1: [-0.012, -0.001], 0.5: [-0.001, 0.007], 1: [-0.001, -0.007]},\n",
    "        }\n",
    "\n",
    "        x_offset = dataset_offsets[dataset][words][0] if words in dataset_offsets[dataset].keys() else 0.01\n",
    "        y_offset = dataset_offsets[dataset][words][1] if words in dataset_offsets[dataset].keys() else -0.01\n",
    "        \n",
    "        \n",
    "        plt.text(x[i]+x_offset, y[i]+y_offset, f'{int(words * 100)}%', fontsize=7, ha='center', va='center', color='black')\n",
    "\n",
    "plot_finetuning_trend('snli')\n",
    "plot_finetuning_trend('atomic')\n",
    "plot_finetuning_trend('social')\n",
    "\n",
    "ax = plt.gca()\n",
    "plot_isocurves(ax)\n",
    "plt.xlabel(\"Model Accuracy on Paraphrased Examples ($\\widetilde{A}_{ðŸª£}$)\")\n",
    "plt.ylabel(r\"Corrected Paraphrastic Consistency ($\\widetilde{P}_C$)\")\n",
    "\n",
    "plt.legend(title='Dataset', loc='upper center')\n",
    "\n",
    "plt.xlim(0.5 - 0.02, 0.75 + 0.02)  # set x-axis limits from -2 to 2\n",
    "plt.ylim(0.83 - 0.02, 1 + 0.02)\n",
    "\n",
    "plt.title(f'Volume of Finetuning Data (RoBERTa)')\n",
    "plt.savefig(f'finetuning.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea3195-e960-4653-af5c-4995c58fa4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80828297-d158-4b45-b976-644eb5de2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "including autogen examples and project label through during finetuning \n",
    "\n",
    "1, 5, 10, 50, 100\n",
    "\n",
    "\n",
    "as the model starts to learn, it is more likely to be inconsistent (wrong distinctions between paraphrased examples)\n",
    "it's possible with more data that we see a dip back up for p(stay) but no guarantees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "para-nlu",
   "language": "python",
   "name": "para-nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
