Processing train_examples_0.01.csv
11/01/2023 17:38:21 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/01/2023 17:38:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/runs/Nov01_17-38-21_clip08.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/01/2023 17:38:21 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/experiments/pretraining-vs-finetuning/finetuning_data/social/train_examples_0.01.csv
11/01/2023 17:38:21 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/social/analysis_model_examples/dev_examples.csv
11/01/2023 17:38:22 - WARNING - datasets.builder - Using custom data configuration default-ef345e03a02ffdf8
11/01/2023 17:38:22 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/01/2023 17:38:22 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-ef345e03a02ffdf8/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
11/01/2023 17:38:22 - WARNING - datasets.builder - Reusing dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-ef345e03a02ffdf8/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
11/01/2023 17:38:22 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-ef345e03a02ffdf8/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 14.88it/s]100%|██████████| 2/2 [00:00<00:00, 14.86it/s]
[INFO|configuration_utils.py:659] 2023-11-01 17:38:22,782 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:38:22,788 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-11-01 17:38:22,826 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-11-01 17:38:22,864 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:38:22,865 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:38:23,102 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:38:23,103 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:38:23,103 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:38:23,103 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:38:23,103 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-11-01 17:38:23,144 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:38:23,144 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-11-01 17:38:23,657 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-11-01 17:38:27,008 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-11-01 17:38:27,008 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
11/01/2023 17:38:27 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f7663a491f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
11/01/2023 17:38:27 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-ef345e03a02ffdf8/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
11/01/2023 17:38:28 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f7663a49040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
11/01/2023 17:38:28 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-ef345e03a02ffdf8/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
hi
11/01/2023 17:38:28 - INFO - __main__ - Sample 281 of the training set: {'sentence1': " You shouldn't ruin your teacher's job.", 'sentence2': 'The teacher won the "Teacher of the Year" award.', 'label': 1, 'input_ids': [0, 370, 4395, 75, 17948, 110, 3254, 18, 633, 4, 2, 2, 133, 3254, 351, 5, 22, 16215, 8365, 9, 5, 2041, 113, 2354, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:38:28 - INFO - __main__ - Sample 250 of the training set: {'sentence1': ' It is OK to ask someone to stop signing if they are being annoying.', 'sentence2': 'A deaf person needs them to keep signing', 'label': 0, 'input_ids': [0, 85, 16, 4954, 7, 1394, 951, 7, 912, 3442, 114, 51, 32, 145, 19887, 4, 2, 2, 250, 17138, 621, 782, 106, 7, 489, 3442, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:38:28 - INFO - __main__ - Sample 228 of the training set: {'sentence1': ' It is unacceptable to send your nudes to your ex-boyfriend.', 'sentence2': "You're trying to flirt with him.", 'label': 0, 'input_ids': [0, 85, 16, 10538, 7, 2142, 110, 295, 21683, 7, 110, 1931, 12, 9902, 18028, 4, 2, 2, 1185, 214, 667, 7, 33743, 19, 123, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
[INFO|trainer.py:622] 2023-11-01 17:38:44,903 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-11-01 17:38:44,925 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-11-01 17:38:44,925 >>   Num examples = 643
[INFO|trainer.py:1421] 2023-11-01 17:38:44,925 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-11-01 17:38:44,925 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-11-01 17:38:44,926 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-11-01 17:38:44,926 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-11-01 17:38:44,926 >>   Total optimization steps = 22
[WARNING|training_args.py:1095] 2023-11-01 17:38:44,943 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/22 [00:00<?, ?it/s]  5%|▍         | 1/22 [00:00<00:20,  1.02it/s]  9%|▉         | 2/22 [00:01<00:15,  1.29it/s] 14%|█▎        | 3/22 [00:02<00:13,  1.40it/s] 18%|█▊        | 4/22 [00:02<00:12,  1.46it/s] 23%|██▎       | 5/22 [00:03<00:11,  1.50it/s] 27%|██▋       | 6/22 [00:04<00:10,  1.52it/s] 32%|███▏      | 7/22 [00:04<00:09,  1.54it/s] 36%|███▋      | 8/22 [00:05<00:09,  1.55it/s] 41%|████      | 9/22 [00:06<00:08,  1.55it/s] 45%|████▌     | 10/22 [00:06<00:07,  1.56it/s][INFO|trainer.py:2340] 2023-11-01 17:38:51,814 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-11
[INFO|configuration_utils.py:446] 2023-11-01 17:38:51,825 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-11/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:38:53,924 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-11/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:38:53,934 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-11/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:38:53,941 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-11/special_tokens_map.json
 55%|█████▍    | 12/22 [00:13<00:18,  1.89s/it] 59%|█████▉    | 13/22 [00:13<00:14,  1.58s/it] 64%|██████▎   | 14/22 [00:14<00:10,  1.33s/it] 68%|██████▊   | 15/22 [00:15<00:07,  1.14s/it] 73%|███████▎  | 16/22 [00:15<00:06,  1.00s/it] 77%|███████▋  | 17/22 [00:16<00:04,  1.11it/s] 82%|████████▏ | 18/22 [00:17<00:03,  1.22it/s] 86%|████████▋ | 19/22 [00:17<00:02,  1.30it/s] 91%|█████████ | 20/22 [00:18<00:01,  1.37it/s] 95%|█████████▌| 21/22 [00:19<00:00,  1.42it/s][INFO|trainer.py:2340] 2023-11-01 17:39:04,192 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-22
[INFO|configuration_utils.py:446] 2023-11-01 17:39:04,206 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-22/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:39:06,121 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-22/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:39:06,129 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-22/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:39:06,140 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/checkpoint-22/special_tokens_map.json
[INFO|trainer.py:1662] 2023-11-01 17:39:10,165 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 22/22 [00:25<00:00,  1.42it/s]100%|██████████| 22/22 [00:25<00:00,  1.15s/it]
[INFO|trainer.py:2340] 2023-11-01 17:39:10,203 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01
[INFO|configuration_utils.py:446] 2023-11-01 17:39:10,212 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:39:12,227 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:39:12,244 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:39:12,252 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.01/special_tokens_map.json
{'train_runtime': 25.2396, 'train_samples_per_second': 50.952, 'train_steps_per_second': 0.872, 'train_loss': 0.6948610652576793, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.6949
  train_runtime            = 0:00:25.23
  train_samples            =        643
  train_samples_per_second =     50.952
  train_steps_per_second   =      0.872
11/01/2023 17:39:12 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-11-01 17:39:12,445 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-11-01 17:39:12,447 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:39:12,447 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-11-01 17:39:12,448 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-11-01 17:39:12,448 >>   Num examples = 8341
[INFO|trainer.py:2595] 2023-11-01 17:39:12,448 >>   Batch size = 64
  0%|          | 0/131 [00:00<?, ?it/s]  2%|▏         | 2/131 [00:00<00:14,  8.94it/s]  2%|▏         | 3/131 [00:00<00:20,  6.31it/s]  3%|▎         | 4/131 [00:00<00:23,  5.45it/s]  4%|▍         | 5/131 [00:00<00:24,  5.06it/s]  5%|▍         | 6/131 [00:01<00:25,  4.84it/s]  5%|▌         | 7/131 [00:01<00:26,  4.72it/s]  6%|▌         | 8/131 [00:01<00:26,  4.63it/s]  7%|▋         | 9/131 [00:01<00:26,  4.58it/s]  8%|▊         | 10/131 [00:02<00:26,  4.54it/s]  8%|▊         | 11/131 [00:02<00:26,  4.52it/s]  9%|▉         | 12/131 [00:02<00:26,  4.50it/s] 10%|▉         | 13/131 [00:02<00:26,  4.49it/s] 11%|█         | 14/131 [00:02<00:26,  4.48it/s] 11%|█▏        | 15/131 [00:03<00:25,  4.47it/s] 12%|█▏        | 16/131 [00:03<00:25,  4.46it/s] 13%|█▎        | 17/131 [00:03<00:25,  4.46it/s] 14%|█▎        | 18/131 [00:03<00:25,  4.46it/s] 15%|█▍        | 19/131 [00:04<00:25,  4.46it/s] 15%|█▌        | 20/131 [00:04<00:24,  4.45it/s] 16%|█▌        | 21/131 [00:04<00:24,  4.45it/s] 17%|█▋        | 22/131 [00:04<00:24,  4.45it/s] 18%|█▊        | 23/131 [00:04<00:24,  4.45it/s] 18%|█▊        | 24/131 [00:05<00:24,  4.46it/s] 19%|█▉        | 25/131 [00:05<00:23,  4.45it/s] 20%|█▉        | 26/131 [00:05<00:23,  4.45it/s] 21%|██        | 27/131 [00:05<00:23,  4.45it/s] 21%|██▏       | 28/131 [00:06<00:23,  4.45it/s] 22%|██▏       | 29/131 [00:06<00:22,  4.45it/s] 23%|██▎       | 30/131 [00:06<00:22,  4.45it/s] 24%|██▎       | 31/131 [00:06<00:22,  4.45it/s] 24%|██▍       | 32/131 [00:06<00:22,  4.44it/s] 25%|██▌       | 33/131 [00:07<00:22,  4.44it/s] 26%|██▌       | 34/131 [00:07<00:21,  4.44it/s] 27%|██▋       | 35/131 [00:07<00:21,  4.44it/s] 27%|██▋       | 36/131 [00:07<00:21,  4.44it/s] 28%|██▊       | 37/131 [00:08<00:21,  4.45it/s] 29%|██▉       | 38/131 [00:08<00:20,  4.44it/s] 30%|██▉       | 39/131 [00:08<00:20,  4.45it/s] 31%|███       | 40/131 [00:08<00:20,  4.45it/s] 31%|███▏      | 41/131 [00:08<00:20,  4.45it/s] 32%|███▏      | 42/131 [00:09<00:20,  4.44it/s] 33%|███▎      | 43/131 [00:09<00:19,  4.43it/s] 34%|███▎      | 44/131 [00:09<00:19,  4.43it/s] 34%|███▍      | 45/131 [00:09<00:19,  4.43it/s] 35%|███▌      | 46/131 [00:10<00:19,  4.42it/s] 36%|███▌      | 47/131 [00:10<00:19,  4.42it/s] 37%|███▋      | 48/131 [00:10<00:18,  4.42it/s] 37%|███▋      | 49/131 [00:10<00:18,  4.42it/s] 38%|███▊      | 50/131 [00:11<00:18,  4.43it/s] 39%|███▉      | 51/131 [00:11<00:18,  4.43it/s] 40%|███▉      | 52/131 [00:11<00:17,  4.43it/s] 40%|████      | 53/131 [00:11<00:17,  4.43it/s] 41%|████      | 54/131 [00:11<00:17,  4.43it/s] 42%|████▏     | 55/131 [00:12<00:17,  4.43it/s] 43%|████▎     | 56/131 [00:12<00:16,  4.43it/s] 44%|████▎     | 57/131 [00:12<00:16,  4.43it/s] 44%|████▍     | 58/131 [00:12<00:16,  4.42it/s] 45%|████▌     | 59/131 [00:13<00:16,  4.42it/s] 46%|████▌     | 60/131 [00:13<00:16,  4.42it/s] 47%|████▋     | 61/131 [00:13<00:15,  4.42it/s] 47%|████▋     | 62/131 [00:13<00:15,  4.42it/s] 48%|████▊     | 63/131 [00:13<00:15,  4.42it/s] 49%|████▉     | 64/131 [00:14<00:15,  4.42it/s] 50%|████▉     | 65/131 [00:14<00:14,  4.42it/s] 50%|█████     | 66/131 [00:14<00:14,  4.42it/s] 51%|█████     | 67/131 [00:14<00:14,  4.42it/s] 52%|█████▏    | 68/131 [00:15<00:14,  4.42it/s] 53%|█████▎    | 69/131 [00:15<00:14,  4.42it/s] 53%|█████▎    | 70/131 [00:15<00:13,  4.43it/s] 54%|█████▍    | 71/131 [00:15<00:13,  4.43it/s] 55%|█████▍    | 72/131 [00:15<00:13,  4.43it/s] 56%|█████▌    | 73/131 [00:16<00:13,  4.43it/s] 56%|█████▋    | 74/131 [00:16<00:12,  4.43it/s] 57%|█████▋    | 75/131 [00:16<00:12,  4.42it/s] 58%|█████▊    | 76/131 [00:16<00:12,  4.42it/s] 59%|█████▉    | 77/131 [00:17<00:12,  4.42it/s] 60%|█████▉    | 78/131 [00:17<00:11,  4.42it/s] 60%|██████    | 79/131 [00:17<00:11,  4.42it/s] 61%|██████    | 80/131 [00:17<00:11,  4.42it/s] 62%|██████▏   | 81/131 [00:18<00:11,  4.42it/s] 63%|██████▎   | 82/131 [00:18<00:11,  4.42it/s] 63%|██████▎   | 83/131 [00:18<00:10,  4.42it/s] 64%|██████▍   | 84/131 [00:18<00:10,  4.42it/s] 65%|██████▍   | 85/131 [00:18<00:10,  4.42it/s] 66%|██████▌   | 86/131 [00:19<00:10,  4.42it/s] 66%|██████▋   | 87/131 [00:19<00:09,  4.42it/s] 67%|██████▋   | 88/131 [00:19<00:09,  4.42it/s] 68%|██████▊   | 89/131 [00:19<00:09,  4.42it/s] 69%|██████▊   | 90/131 [00:20<00:09,  4.41it/s] 69%|██████▉   | 91/131 [00:20<00:09,  4.42it/s] 70%|███████   | 92/131 [00:20<00:08,  4.41it/s] 71%|███████   | 93/131 [00:20<00:08,  4.41it/s] 72%|███████▏  | 94/131 [00:20<00:08,  4.41it/s] 73%|███████▎  | 95/131 [00:21<00:08,  4.41it/s] 73%|███████▎  | 96/131 [00:21<00:07,  4.41it/s] 74%|███████▍  | 97/131 [00:21<00:07,  4.42it/s] 75%|███████▍  | 98/131 [00:21<00:07,  4.42it/s] 76%|███████▌  | 99/131 [00:22<00:07,  4.41it/s] 76%|███████▋  | 100/131 [00:22<00:07,  4.41it/s] 77%|███████▋  | 101/131 [00:22<00:06,  4.41it/s] 78%|███████▊  | 102/131 [00:22<00:06,  4.41it/s] 79%|███████▊  | 103/131 [00:23<00:06,  4.41it/s] 79%|███████▉  | 104/131 [00:23<00:06,  4.40it/s] 80%|████████  | 105/131 [00:23<00:05,  4.41it/s] 81%|████████  | 106/131 [00:23<00:05,  4.41it/s] 82%|████████▏ | 107/131 [00:23<00:05,  4.40it/s] 82%|████████▏ | 108/131 [00:24<00:05,  4.41it/s] 83%|████████▎ | 109/131 [00:24<00:04,  4.41it/s] 84%|████████▍ | 110/131 [00:24<00:04,  4.41it/s] 85%|████████▍ | 111/131 [00:24<00:04,  4.41it/s] 85%|████████▌ | 112/131 [00:25<00:04,  4.40it/s] 86%|████████▋ | 113/131 [00:25<00:04,  4.40it/s] 87%|████████▋ | 114/131 [00:25<00:03,  4.41it/s] 88%|████████▊ | 115/131 [00:25<00:03,  4.41it/s] 89%|████████▊ | 116/131 [00:25<00:03,  4.40it/s] 89%|████████▉ | 117/131 [00:26<00:03,  4.41it/s] 90%|█████████ | 118/131 [00:26<00:02,  4.40it/s] 91%|█████████ | 119/131 [00:26<00:02,  4.40it/s] 92%|█████████▏| 120/131 [00:26<00:02,  4.40it/s] 92%|█████████▏| 121/131 [00:27<00:02,  4.40it/s] 93%|█████████▎| 122/131 [00:27<00:02,  4.40it/s] 94%|█████████▍| 123/131 [00:27<00:01,  4.39it/s] 95%|█████████▍| 124/131 [00:27<00:01,  4.39it/s] 95%|█████████▌| 125/131 [00:28<00:01,  4.39it/s] 96%|█████████▌| 126/131 [00:28<00:01,  4.39it/s] 97%|█████████▋| 127/131 [00:28<00:00,  4.39it/s] 98%|█████████▊| 128/131 [00:28<00:00,  4.40it/s] 98%|█████████▊| 129/131 [00:28<00:00,  4.40it/s] 99%|█████████▉| 130/131 [00:29<00:00,  4.39it/s][WARNING|training_args.py:1095] 2023-11-01 17:39:41,904 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 131/131 [00:29<00:00,  4.48it/s]
[WARNING|training_args.py:1095] 2023-11-01 17:39:41,961 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:39:41,961 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-11-01 17:39:42,139 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5157654881477356}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.5158
  eval_loss               =     0.6927
  eval_runtime            = 0:00:29.45
  eval_samples            =       8341
  eval_samples_per_second =    283.164
  eval_steps_per_second   =      4.447
Processing train_examples_0.05.csv
11/01/2023 17:39:55 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/01/2023 17:39:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/runs/Nov01_17-39-55_clip08.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/01/2023 17:39:55 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/experiments/pretraining-vs-finetuning/finetuning_data/social/train_examples_0.05.csv
11/01/2023 17:39:55 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/social/analysis_model_examples/dev_examples.csv
11/01/2023 17:39:55 - WARNING - datasets.builder - Using custom data configuration default-508787bc796466d1
11/01/2023 17:39:55 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/01/2023 17:39:55 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-508787bc796466d1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
11/01/2023 17:39:55 - WARNING - datasets.builder - Reusing dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-508787bc796466d1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
11/01/2023 17:39:55 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-508787bc796466d1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 20.83it/s]
[INFO|configuration_utils.py:659] 2023-11-01 17:39:55,645 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:39:55,654 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-11-01 17:39:55,696 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-11-01 17:39:55,738 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:39:55,738 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:39:55,968 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:39:55,968 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:39:55,968 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:39:55,968 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:39:55,968 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-11-01 17:39:56,008 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:39:56,009 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-11-01 17:39:56,421 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-11-01 17:39:57,761 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-11-01 17:39:57,761 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
11/01/2023 17:39:58 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f8b97d151f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
11/01/2023 17:39:58 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-508787bc796466d1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
11/01/2023 17:39:59 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f8b97d15040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
11/01/2023 17:39:59 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-508787bc796466d1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
hi
11/01/2023 17:39:59 - INFO - __main__ - Sample 1126 of the training set: {'sentence1': " It's good to branch out and live in a new place with someone you love.", 'sentence2': 'a change of pace with help refresh your mind', 'label': 1, 'input_ids': [0, 85, 18, 205, 7, 6084, 66, 8, 697, 11, 10, 92, 317, 19, 951, 47, 657, 4, 2, 2, 102, 464, 9, 2877, 19, 244, 14240, 110, 1508, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:39:59 - INFO - __main__ - Sample 1003 of the training set: {'sentence1': ' It is rude to ignore someone when they expect a reply.', 'sentence2': 'They asked you to reply right away.', 'label': 1, 'input_ids': [0, 85, 16, 21820, 7, 8861, 951, 77, 51, 1057, 10, 10418, 4, 2, 2, 1213, 553, 47, 7, 10418, 235, 409, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:39:59 - INFO - __main__ - Sample 914 of the training set: {'sentence1': ' You should pay what you owe before giving gifts.', 'sentence2': "You're giving your kid the gifts.", 'label': 0, 'input_ids': [0, 370, 197, 582, 99, 47, 14866, 137, 1311, 7420, 4, 2, 2, 1185, 214, 1311, 110, 4607, 5, 7420, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
[INFO|trainer.py:622] 2023-11-01 17:40:03,862 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-11-01 17:40:03,885 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-11-01 17:40:03,885 >>   Num examples = 3211
[INFO|trainer.py:1421] 2023-11-01 17:40:03,885 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-11-01 17:40:03,885 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-11-01 17:40:03,885 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-11-01 17:40:03,885 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-11-01 17:40:03,885 >>   Total optimization steps = 102
[WARNING|training_args.py:1095] 2023-11-01 17:40:03,908 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/102 [00:00<?, ?it/s]  1%|          | 1/102 [00:00<01:08,  1.47it/s]  2%|▏         | 2/102 [00:01<01:06,  1.51it/s]  3%|▎         | 3/102 [00:01<01:04,  1.53it/s]  4%|▍         | 4/102 [00:02<01:04,  1.53it/s]  5%|▍         | 5/102 [00:03<01:03,  1.53it/s]  6%|▌         | 6/102 [00:03<01:02,  1.53it/s]  7%|▋         | 7/102 [00:04<01:02,  1.53it/s]  8%|▊         | 8/102 [00:05<01:01,  1.53it/s]  9%|▉         | 9/102 [00:05<01:00,  1.53it/s] 10%|▉         | 10/102 [00:06<00:59,  1.53it/s] 11%|█         | 11/102 [00:07<00:59,  1.53it/s] 12%|█▏        | 12/102 [00:07<00:58,  1.53it/s] 13%|█▎        | 13/102 [00:08<00:58,  1.53it/s] 14%|█▎        | 14/102 [00:09<00:57,  1.53it/s] 15%|█▍        | 15/102 [00:09<00:56,  1.53it/s] 16%|█▌        | 16/102 [00:10<00:56,  1.53it/s] 17%|█▋        | 17/102 [00:11<00:55,  1.53it/s] 18%|█▊        | 18/102 [00:11<00:54,  1.53it/s] 19%|█▊        | 19/102 [00:12<00:54,  1.53it/s] 20%|█▉        | 20/102 [00:13<00:53,  1.53it/s] 21%|██        | 21/102 [00:13<00:53,  1.53it/s] 22%|██▏       | 22/102 [00:14<00:52,  1.53it/s] 23%|██▎       | 23/102 [00:15<00:51,  1.53it/s] 24%|██▎       | 24/102 [00:15<00:51,  1.52it/s] 25%|██▍       | 25/102 [00:16<00:50,  1.52it/s] 25%|██▌       | 26/102 [00:17<00:49,  1.52it/s] 26%|██▋       | 27/102 [00:17<00:49,  1.52it/s] 27%|██▋       | 28/102 [00:18<00:48,  1.52it/s] 28%|██▊       | 29/102 [00:18<00:47,  1.52it/s] 29%|██▉       | 30/102 [00:19<00:47,  1.52it/s] 30%|███       | 31/102 [00:20<00:46,  1.52it/s] 31%|███▏      | 32/102 [00:20<00:46,  1.52it/s] 32%|███▏      | 33/102 [00:21<00:45,  1.52it/s] 33%|███▎      | 34/102 [00:22<00:44,  1.52it/s] 34%|███▍      | 35/102 [00:22<00:44,  1.52it/s] 35%|███▌      | 36/102 [00:23<00:43,  1.52it/s] 36%|███▋      | 37/102 [00:24<00:42,  1.52it/s] 37%|███▋      | 38/102 [00:24<00:42,  1.52it/s] 38%|███▊      | 39/102 [00:25<00:41,  1.52it/s] 39%|███▉      | 40/102 [00:26<00:40,  1.52it/s] 40%|████      | 41/102 [00:26<00:40,  1.52it/s] 41%|████      | 42/102 [00:27<00:39,  1.52it/s] 42%|████▏     | 43/102 [00:28<00:38,  1.52it/s] 43%|████▎     | 44/102 [00:28<00:38,  1.52it/s] 44%|████▍     | 45/102 [00:29<00:37,  1.51it/s] 45%|████▌     | 46/102 [00:30<00:37,  1.51it/s] 46%|████▌     | 47/102 [00:30<00:36,  1.51it/s] 47%|████▋     | 48/102 [00:31<00:35,  1.51it/s] 48%|████▊     | 49/102 [00:32<00:35,  1.51it/s] 49%|████▉     | 50/102 [00:32<00:34,  1.51it/s] 50%|█████     | 51/102 [00:33<00:26,  1.94it/s][INFO|trainer.py:2340] 2023-11-01 17:40:37,007 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-51
[INFO|configuration_utils.py:446] 2023-11-01 17:40:37,022 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-51/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:40:38,841 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-51/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:40:38,852 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-51/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:40:38,861 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-51/special_tokens_map.json
 51%|█████     | 52/102 [00:39<01:57,  2.35s/it] 52%|█████▏    | 53/102 [00:40<01:30,  1.84s/it] 53%|█████▎    | 54/102 [00:40<01:11,  1.49s/it] 54%|█████▍    | 55/102 [00:41<00:58,  1.24s/it] 55%|█████▍    | 56/102 [00:42<00:49,  1.07s/it] 56%|█████▌    | 57/102 [00:42<00:42,  1.06it/s] 57%|█████▋    | 58/102 [00:43<00:37,  1.16it/s] 58%|█████▊    | 59/102 [00:44<00:34,  1.25it/s] 59%|█████▉    | 60/102 [00:44<00:31,  1.32it/s] 60%|█████▉    | 61/102 [00:45<00:29,  1.37it/s] 61%|██████    | 62/102 [00:46<00:28,  1.41it/s] 62%|██████▏   | 63/102 [00:46<00:27,  1.44it/s] 63%|██████▎   | 64/102 [00:47<00:26,  1.46it/s] 64%|██████▎   | 65/102 [00:48<00:25,  1.47it/s] 65%|██████▍   | 66/102 [00:48<00:24,  1.48it/s] 66%|██████▌   | 67/102 [00:49<00:23,  1.49it/s] 67%|██████▋   | 68/102 [00:50<00:22,  1.49it/s] 68%|██████▊   | 69/102 [00:50<00:22,  1.50it/s] 69%|██████▊   | 70/102 [00:51<00:21,  1.50it/s] 70%|██████▉   | 71/102 [00:52<00:20,  1.50it/s] 71%|███████   | 72/102 [00:52<00:19,  1.50it/s] 72%|███████▏  | 73/102 [00:53<00:19,  1.50it/s] 73%|███████▎  | 74/102 [00:54<00:18,  1.50it/s] 74%|███████▎  | 75/102 [00:54<00:17,  1.50it/s] 75%|███████▍  | 76/102 [00:55<00:17,  1.50it/s] 75%|███████▌  | 77/102 [00:56<00:16,  1.50it/s] 76%|███████▋  | 78/102 [00:56<00:16,  1.50it/s] 77%|███████▋  | 79/102 [00:57<00:15,  1.50it/s] 78%|███████▊  | 80/102 [00:58<00:14,  1.50it/s] 79%|███████▉  | 81/102 [00:58<00:14,  1.50it/s] 80%|████████  | 82/102 [00:59<00:13,  1.50it/s] 81%|████████▏ | 83/102 [01:00<00:12,  1.50it/s] 82%|████████▏ | 84/102 [01:00<00:12,  1.50it/s] 83%|████████▎ | 85/102 [01:01<00:11,  1.50it/s] 84%|████████▍ | 86/102 [01:02<00:10,  1.50it/s] 85%|████████▌ | 87/102 [01:02<00:09,  1.50it/s] 86%|████████▋ | 88/102 [01:03<00:09,  1.50it/s] 87%|████████▋ | 89/102 [01:04<00:08,  1.50it/s] 88%|████████▊ | 90/102 [01:04<00:07,  1.50it/s] 89%|████████▉ | 91/102 [01:05<00:07,  1.50it/s] 90%|█████████ | 92/102 [01:06<00:06,  1.50it/s] 91%|█████████ | 93/102 [01:06<00:06,  1.50it/s] 92%|█████████▏| 94/102 [01:07<00:05,  1.50it/s] 93%|█████████▎| 95/102 [01:08<00:04,  1.50it/s] 94%|█████████▍| 96/102 [01:08<00:04,  1.50it/s] 95%|█████████▌| 97/102 [01:09<00:03,  1.50it/s] 96%|█████████▌| 98/102 [01:10<00:02,  1.50it/s] 97%|█████████▋| 99/102 [01:10<00:02,  1.50it/s] 98%|█████████▊| 100/102 [01:11<00:01,  1.50it/s] 99%|█████████▉| 101/102 [01:12<00:00,  1.50it/s]100%|██████████| 102/102 [01:12<00:00,  1.93it/s][INFO|trainer.py:2340] 2023-11-01 17:41:16,424 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-102
[INFO|configuration_utils.py:446] 2023-11-01 17:41:16,437 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-102/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:41:18,244 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-102/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:41:18,255 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-102/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:41:18,262 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/checkpoint-102/special_tokens_map.json
[INFO|trainer.py:1662] 2023-11-01 17:41:22,353 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 102/102 [01:18<00:00,  1.93it/s]100%|██████████| 102/102 [01:18<00:00,  1.30it/s]
[INFO|trainer.py:2340] 2023-11-01 17:41:22,382 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05
[INFO|configuration_utils.py:446] 2023-11-01 17:41:22,391 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:41:24,211 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:41:24,220 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:41:24,227 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.05/special_tokens_map.json
{'train_runtime': 78.4682, 'train_samples_per_second': 81.842, 'train_steps_per_second': 1.3, 'train_loss': 0.6933035009047565, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.6933
  train_runtime            = 0:01:18.46
  train_samples            =       3211
  train_samples_per_second =     81.842
  train_steps_per_second   =        1.3
11/01/2023 17:41:24 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-11-01 17:41:24,412 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-11-01 17:41:24,414 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:41:24,414 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-11-01 17:41:24,414 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-11-01 17:41:24,414 >>   Num examples = 8341
[INFO|trainer.py:2595] 2023-11-01 17:41:24,414 >>   Batch size = 64
  0%|          | 0/131 [00:00<?, ?it/s]  2%|▏         | 2/131 [00:00<00:14,  8.65it/s]  2%|▏         | 3/131 [00:00<00:21,  6.07it/s]  3%|▎         | 4/131 [00:00<00:24,  5.26it/s]  4%|▍         | 5/131 [00:00<00:25,  4.87it/s]  5%|▍         | 6/131 [00:01<00:26,  4.66it/s]  5%|▌         | 7/131 [00:01<00:27,  4.53it/s]  6%|▌         | 8/131 [00:01<00:27,  4.45it/s]  7%|▋         | 9/131 [00:01<00:27,  4.40it/s]  8%|▊         | 10/131 [00:02<00:27,  4.36it/s]  8%|▊         | 11/131 [00:02<00:27,  4.33it/s]  9%|▉         | 12/131 [00:02<00:27,  4.32it/s] 10%|▉         | 13/131 [00:02<00:27,  4.31it/s] 11%|█         | 14/131 [00:03<00:27,  4.29it/s] 11%|█▏        | 15/131 [00:03<00:27,  4.28it/s] 12%|█▏        | 16/131 [00:03<00:26,  4.28it/s] 13%|█▎        | 17/131 [00:03<00:26,  4.27it/s] 14%|█▎        | 18/131 [00:03<00:26,  4.27it/s] 15%|█▍        | 19/131 [00:04<00:26,  4.27it/s] 15%|█▌        | 20/131 [00:04<00:25,  4.27it/s] 16%|█▌        | 21/131 [00:04<00:25,  4.27it/s] 17%|█▋        | 22/131 [00:04<00:25,  4.28it/s] 18%|█▊        | 23/131 [00:05<00:25,  4.27it/s] 18%|█▊        | 24/131 [00:05<00:25,  4.27it/s] 19%|█▉        | 25/131 [00:05<00:24,  4.27it/s] 20%|█▉        | 26/131 [00:05<00:24,  4.27it/s] 21%|██        | 27/131 [00:06<00:24,  4.27it/s] 21%|██▏       | 28/131 [00:06<00:24,  4.27it/s] 22%|██▏       | 29/131 [00:06<00:23,  4.27it/s] 23%|██▎       | 30/131 [00:06<00:23,  4.27it/s] 24%|██▎       | 31/131 [00:07<00:23,  4.27it/s] 24%|██▍       | 32/131 [00:07<00:23,  4.26it/s] 25%|██▌       | 33/131 [00:07<00:22,  4.26it/s] 26%|██▌       | 34/131 [00:07<00:22,  4.26it/s] 27%|██▋       | 35/131 [00:07<00:22,  4.27it/s] 27%|██▋       | 36/131 [00:08<00:22,  4.27it/s] 28%|██▊       | 37/131 [00:08<00:22,  4.27it/s] 29%|██▉       | 38/131 [00:08<00:21,  4.27it/s] 30%|██▉       | 39/131 [00:08<00:21,  4.27it/s] 31%|███       | 40/131 [00:09<00:21,  4.27it/s] 31%|███▏      | 41/131 [00:09<00:21,  4.27it/s] 32%|███▏      | 42/131 [00:09<00:20,  4.26it/s] 33%|███▎      | 43/131 [00:09<00:20,  4.26it/s] 34%|███▎      | 44/131 [00:10<00:20,  4.26it/s] 34%|███▍      | 45/131 [00:10<00:20,  4.27it/s] 35%|███▌      | 46/131 [00:10<00:19,  4.26it/s] 36%|███▌      | 47/131 [00:10<00:19,  4.27it/s] 37%|███▋      | 48/131 [00:11<00:19,  4.27it/s] 37%|███▋      | 49/131 [00:11<00:19,  4.27it/s] 38%|███▊      | 50/131 [00:11<00:18,  4.26it/s] 39%|███▉      | 51/131 [00:11<00:18,  4.27it/s] 40%|███▉      | 52/131 [00:11<00:18,  4.27it/s] 40%|████      | 53/131 [00:12<00:18,  4.27it/s] 41%|████      | 54/131 [00:12<00:18,  4.26it/s] 42%|████▏     | 55/131 [00:12<00:17,  4.26it/s] 43%|████▎     | 56/131 [00:12<00:17,  4.27it/s] 44%|████▎     | 57/131 [00:13<00:17,  4.26it/s] 44%|████▍     | 58/131 [00:13<00:17,  4.27it/s] 45%|████▌     | 59/131 [00:13<00:16,  4.26it/s] 46%|████▌     | 60/131 [00:13<00:16,  4.26it/s] 47%|████▋     | 61/131 [00:14<00:16,  4.26it/s] 47%|████▋     | 62/131 [00:14<00:16,  4.25it/s] 48%|████▊     | 63/131 [00:14<00:15,  4.25it/s] 49%|████▉     | 64/131 [00:14<00:15,  4.26it/s] 50%|████▉     | 65/131 [00:14<00:15,  4.26it/s] 50%|█████     | 66/131 [00:15<00:15,  4.26it/s] 51%|█████     | 67/131 [00:15<00:15,  4.26it/s] 52%|█████▏    | 68/131 [00:15<00:14,  4.26it/s] 53%|█████▎    | 69/131 [00:15<00:14,  4.26it/s] 53%|█████▎    | 70/131 [00:16<00:14,  4.25it/s] 54%|█████▍    | 71/131 [00:16<00:14,  4.26it/s] 55%|█████▍    | 72/131 [00:16<00:13,  4.25it/s] 56%|█████▌    | 73/131 [00:16<00:13,  4.25it/s] 56%|█████▋    | 74/131 [00:17<00:13,  4.25it/s] 57%|█████▋    | 75/131 [00:17<00:13,  4.24it/s] 58%|█████▊    | 76/131 [00:17<00:12,  4.25it/s] 59%|█████▉    | 77/131 [00:17<00:12,  4.25it/s] 60%|█████▉    | 78/131 [00:18<00:12,  4.25it/s] 60%|██████    | 79/131 [00:18<00:12,  4.25it/s] 61%|██████    | 80/131 [00:18<00:12,  4.25it/s] 62%|██████▏   | 81/131 [00:18<00:11,  4.25it/s] 63%|██████▎   | 82/131 [00:18<00:11,  4.25it/s] 63%|██████▎   | 83/131 [00:19<00:11,  4.25it/s] 64%|██████▍   | 84/131 [00:19<00:11,  4.25it/s] 65%|██████▍   | 85/131 [00:19<00:10,  4.26it/s] 66%|██████▌   | 86/131 [00:19<00:10,  4.26it/s] 66%|██████▋   | 87/131 [00:20<00:10,  4.26it/s] 67%|██████▋   | 88/131 [00:20<00:10,  4.26it/s] 68%|██████▊   | 89/131 [00:20<00:09,  4.26it/s] 69%|██████▊   | 90/131 [00:20<00:09,  4.25it/s] 69%|██████▉   | 91/131 [00:21<00:09,  4.25it/s] 70%|███████   | 92/131 [00:21<00:09,  4.25it/s] 71%|███████   | 93/131 [00:21<00:08,  4.24it/s] 72%|███████▏  | 94/131 [00:21<00:08,  4.24it/s] 73%|███████▎  | 95/131 [00:22<00:08,  4.24it/s] 73%|███████▎  | 96/131 [00:22<00:08,  4.24it/s] 74%|███████▍  | 97/131 [00:22<00:08,  4.25it/s] 75%|███████▍  | 98/131 [00:22<00:07,  4.25it/s] 76%|███████▌  | 99/131 [00:22<00:07,  4.25it/s] 76%|███████▋  | 100/131 [00:23<00:07,  4.25it/s] 77%|███████▋  | 101/131 [00:23<00:07,  4.25it/s] 78%|███████▊  | 102/131 [00:23<00:06,  4.24it/s] 79%|███████▊  | 103/131 [00:23<00:06,  4.25it/s] 79%|███████▉  | 104/131 [00:24<00:06,  4.25it/s] 80%|████████  | 105/131 [00:24<00:06,  4.24it/s] 81%|████████  | 106/131 [00:24<00:05,  4.24it/s] 82%|████████▏ | 107/131 [00:24<00:05,  4.24it/s] 82%|████████▏ | 108/131 [00:25<00:05,  4.24it/s] 83%|████████▎ | 109/131 [00:25<00:05,  4.24it/s] 84%|████████▍ | 110/131 [00:25<00:04,  4.24it/s] 85%|████████▍ | 111/131 [00:25<00:04,  4.24it/s] 85%|████████▌ | 112/131 [00:26<00:04,  4.24it/s] 86%|████████▋ | 113/131 [00:26<00:04,  4.24it/s] 87%|████████▋ | 114/131 [00:26<00:04,  4.25it/s] 88%|████████▊ | 115/131 [00:26<00:03,  4.24it/s] 89%|████████▊ | 116/131 [00:26<00:03,  4.25it/s] 89%|████████▉ | 117/131 [00:27<00:03,  4.25it/s] 90%|█████████ | 118/131 [00:27<00:03,  4.25it/s] 91%|█████████ | 119/131 [00:27<00:02,  4.23it/s] 92%|█████████▏| 120/131 [00:27<00:02,  4.24it/s] 92%|█████████▏| 121/131 [00:28<00:02,  4.24it/s] 93%|█████████▎| 122/131 [00:28<00:02,  4.23it/s] 94%|█████████▍| 123/131 [00:28<00:01,  4.22it/s] 95%|█████████▍| 124/131 [00:28<00:01,  4.22it/s] 95%|█████████▌| 125/131 [00:29<00:01,  4.23it/s] 96%|█████████▌| 126/131 [00:29<00:01,  4.23it/s] 97%|█████████▋| 127/131 [00:29<00:00,  4.22it/s] 98%|█████████▊| 128/131 [00:29<00:00,  4.24it/s] 98%|█████████▊| 129/131 [00:30<00:00,  4.24it/s] 99%|█████████▉| 130/131 [00:30<00:00,  4.23it/s][WARNING|training_args.py:1095] 2023-11-01 17:41:55,044 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 131/131 [00:30<00:00,  4.30it/s]
[WARNING|training_args.py:1095] 2023-11-01 17:41:55,097 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:41:55,098 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-11-01 17:41:55,385 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5157654881477356}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.5158
  eval_loss               =     0.6923
  eval_runtime            = 0:00:30.62
  eval_samples            =       8341
  eval_samples_per_second =    272.315
  eval_steps_per_second   =      4.277
Processing train_examples_0.1.csv
11/01/2023 17:42:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/01/2023 17:42:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/runs/Nov01_17-42-07_clip08.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/01/2023 17:42:07 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/experiments/pretraining-vs-finetuning/finetuning_data/social/train_examples_0.1.csv
11/01/2023 17:42:07 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/social/analysis_model_examples/dev_examples.csv
11/01/2023 17:42:07 - WARNING - datasets.builder - Using custom data configuration default-74d0d77f05066496
11/01/2023 17:42:07 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/01/2023 17:42:07 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-74d0d77f05066496/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
11/01/2023 17:42:08 - WARNING - datasets.builder - Reusing dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-74d0d77f05066496/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
11/01/2023 17:42:08 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-74d0d77f05066496/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 28.13it/s]
[INFO|configuration_utils.py:659] 2023-11-01 17:42:08,283 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:42:08,289 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-11-01 17:42:08,329 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-11-01 17:42:08,382 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:42:08,382 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:42:08,603 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:42:08,604 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:42:08,604 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:42:08,604 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:42:08,604 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-11-01 17:42:08,654 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:42:08,655 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-11-01 17:42:09,057 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-11-01 17:42:10,379 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-11-01 17:42:10,379 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
11/01/2023 17:42:11 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f52dc6311f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
11/01/2023 17:42:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-74d0d77f05066496/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
11/01/2023 17:42:12 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f52dc631040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
11/01/2023 17:42:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-74d0d77f05066496/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
hi
11/01/2023 17:42:12 - INFO - __main__ - Sample 2253 of the training set: {'sentence1': " It's understandable to not want to interfere in a married couple's relationship.", 'sentence2': 'The married couple are happy.', 'label': 1, 'input_ids': [0, 85, 18, 19717, 7, 45, 236, 7, 15365, 11, 10, 2997, 891, 18, 1291, 4, 2, 2, 133, 2997, 891, 32, 1372, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:42:12 - INFO - __main__ - Sample 2006 of the training set: {'sentence1': ' People are expected to train their pets to behave well.', 'sentence2': 'They want them to be good for their kids', 'label': 1, 'input_ids': [0, 1806, 32, 421, 7, 2341, 49, 10029, 7, 18871, 157, 4, 2, 2, 1213, 236, 106, 7, 28, 205, 13, 49, 1159, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:42:12 - INFO - __main__ - Sample 1828 of the training set: {'sentence1': ' You should always chip in to pay for utilities in your home.', 'sentence2': 'You have made other arrangements.', 'label': 0, 'input_ids': [0, 370, 197, 460, 6638, 11, 7, 582, 13, 9987, 11, 110, 184, 4, 2, 2, 1185, 33, 156, 97, 7863, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
[INFO|trainer.py:622] 2023-11-01 17:42:16,639 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-11-01 17:42:16,655 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-11-01 17:42:16,655 >>   Num examples = 6541
[INFO|trainer.py:1421] 2023-11-01 17:42:16,655 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-11-01 17:42:16,655 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-11-01 17:42:16,655 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-11-01 17:42:16,655 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-11-01 17:42:16,655 >>   Total optimization steps = 206
[WARNING|training_args.py:1095] 2023-11-01 17:42:16,669 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/206 [00:00<?, ?it/s]  0%|          | 1/206 [00:00<02:22,  1.44it/s]  1%|          | 2/206 [00:01<02:17,  1.48it/s]  1%|▏         | 3/206 [00:02<02:15,  1.50it/s]  2%|▏         | 4/206 [00:02<02:14,  1.50it/s]  2%|▏         | 5/206 [00:03<02:13,  1.50it/s]  3%|▎         | 6/206 [00:04<02:13,  1.50it/s]  3%|▎         | 7/206 [00:04<02:12,  1.50it/s]  4%|▍         | 8/206 [00:05<02:11,  1.50it/s]  4%|▍         | 9/206 [00:06<02:11,  1.50it/s]  5%|▍         | 10/206 [00:06<02:10,  1.50it/s]  5%|▌         | 11/206 [00:07<02:09,  1.50it/s]  6%|▌         | 12/206 [00:07<02:09,  1.50it/s]  6%|▋         | 13/206 [00:08<02:08,  1.50it/s]  7%|▋         | 14/206 [00:09<02:07,  1.50it/s]  7%|▋         | 15/206 [00:09<02:07,  1.50it/s]  8%|▊         | 16/206 [00:10<02:06,  1.50it/s]  8%|▊         | 17/206 [00:11<02:05,  1.50it/s]  9%|▊         | 18/206 [00:11<02:05,  1.50it/s]  9%|▉         | 19/206 [00:12<02:04,  1.50it/s] 10%|▉         | 20/206 [00:13<02:03,  1.50it/s] 10%|█         | 21/206 [00:13<02:03,  1.50it/s] 11%|█         | 22/206 [00:14<02:02,  1.50it/s] 11%|█         | 23/206 [00:15<02:01,  1.50it/s] 12%|█▏        | 24/206 [00:15<02:01,  1.50it/s] 12%|█▏        | 25/206 [00:16<02:00,  1.50it/s] 13%|█▎        | 26/206 [00:17<01:59,  1.50it/s] 13%|█▎        | 27/206 [00:17<01:59,  1.50it/s] 14%|█▎        | 28/206 [00:18<01:58,  1.50it/s] 14%|█▍        | 29/206 [00:19<01:57,  1.50it/s] 15%|█▍        | 30/206 [00:19<01:57,  1.50it/s] 15%|█▌        | 31/206 [00:20<01:57,  1.50it/s] 16%|█▌        | 32/206 [00:21<01:56,  1.49it/s] 16%|█▌        | 33/206 [00:22<01:55,  1.49it/s] 17%|█▋        | 34/206 [00:22<01:55,  1.49it/s] 17%|█▋        | 35/206 [00:23<01:54,  1.49it/s] 17%|█▋        | 36/206 [00:24<01:53,  1.49it/s] 18%|█▊        | 37/206 [00:24<01:53,  1.49it/s] 18%|█▊        | 38/206 [00:25<01:52,  1.49it/s] 19%|█▉        | 39/206 [00:26<01:51,  1.49it/s] 19%|█▉        | 40/206 [00:26<01:51,  1.49it/s] 20%|█▉        | 41/206 [00:27<01:50,  1.49it/s] 20%|██        | 42/206 [00:28<01:50,  1.49it/s] 21%|██        | 43/206 [00:28<01:49,  1.49it/s] 21%|██▏       | 44/206 [00:29<01:48,  1.49it/s] 22%|██▏       | 45/206 [00:30<01:48,  1.49it/s] 22%|██▏       | 46/206 [00:30<01:47,  1.49it/s] 23%|██▎       | 47/206 [00:31<01:46,  1.49it/s] 23%|██▎       | 48/206 [00:32<01:46,  1.49it/s] 24%|██▍       | 49/206 [00:32<01:45,  1.49it/s] 24%|██▍       | 50/206 [00:33<01:44,  1.49it/s] 25%|██▍       | 51/206 [00:34<01:44,  1.49it/s] 25%|██▌       | 52/206 [00:34<01:43,  1.49it/s] 26%|██▌       | 53/206 [00:35<01:42,  1.49it/s] 26%|██▌       | 54/206 [00:36<01:42,  1.49it/s] 27%|██▋       | 55/206 [00:36<01:41,  1.49it/s] 27%|██▋       | 56/206 [00:37<01:40,  1.49it/s] 28%|██▊       | 57/206 [00:38<01:40,  1.49it/s] 28%|██▊       | 58/206 [00:38<01:39,  1.49it/s] 29%|██▊       | 59/206 [00:39<01:38,  1.49it/s] 29%|██▉       | 60/206 [00:40<01:38,  1.49it/s] 30%|██▉       | 61/206 [00:40<01:37,  1.49it/s] 30%|███       | 62/206 [00:41<01:37,  1.48it/s] 31%|███       | 63/206 [00:42<01:36,  1.49it/s] 31%|███       | 64/206 [00:42<01:35,  1.48it/s] 32%|███▏      | 65/206 [00:43<01:35,  1.48it/s] 32%|███▏      | 66/206 [00:44<01:34,  1.48it/s] 33%|███▎      | 67/206 [00:44<01:34,  1.48it/s] 33%|███▎      | 68/206 [00:45<01:33,  1.48it/s] 33%|███▎      | 69/206 [00:46<01:33,  1.47it/s] 34%|███▍      | 70/206 [00:46<01:32,  1.47it/s] 34%|███▍      | 71/206 [00:47<01:32,  1.47it/s] 35%|███▍      | 72/206 [00:48<01:31,  1.46it/s] 35%|███▌      | 73/206 [00:48<01:30,  1.46it/s] 36%|███▌      | 74/206 [00:49<01:30,  1.46it/s] 36%|███▋      | 75/206 [00:50<01:29,  1.46it/s] 37%|███▋      | 76/206 [00:51<01:28,  1.46it/s] 37%|███▋      | 77/206 [00:51<01:28,  1.47it/s] 38%|███▊      | 78/206 [00:52<01:27,  1.47it/s] 38%|███▊      | 79/206 [00:53<01:26,  1.47it/s] 39%|███▉      | 80/206 [00:53<01:26,  1.46it/s] 39%|███▉      | 81/206 [00:54<01:25,  1.46it/s] 40%|███▉      | 82/206 [00:55<01:24,  1.46it/s] 40%|████      | 83/206 [00:55<01:24,  1.46it/s] 41%|████      | 84/206 [00:56<01:24,  1.44it/s] 41%|████▏     | 85/206 [00:57<01:23,  1.45it/s] 42%|████▏     | 86/206 [00:57<01:22,  1.45it/s] 42%|████▏     | 87/206 [00:58<01:21,  1.45it/s] 43%|████▎     | 88/206 [00:59<01:20,  1.46it/s] 43%|████▎     | 89/206 [00:59<01:20,  1.46it/s] 44%|████▎     | 90/206 [01:00<01:19,  1.45it/s] 44%|████▍     | 91/206 [01:01<01:18,  1.46it/s] 45%|████▍     | 92/206 [01:02<01:18,  1.45it/s] 45%|████▌     | 93/206 [01:02<01:17,  1.45it/s] 46%|████▌     | 94/206 [01:03<01:16,  1.46it/s] 46%|████▌     | 95/206 [01:04<01:17,  1.44it/s] 47%|████▋     | 96/206 [01:04<01:16,  1.44it/s] 47%|████▋     | 97/206 [01:05<01:15,  1.44it/s] 48%|████▊     | 98/206 [01:06<01:14,  1.44it/s] 48%|████▊     | 99/206 [01:06<01:13,  1.45it/s] 49%|████▊     | 100/206 [01:07<01:13,  1.43it/s] 49%|████▉     | 101/206 [01:08<01:12,  1.44it/s] 50%|████▉     | 102/206 [01:08<01:11,  1.45it/s] 50%|█████     | 103/206 [01:09<00:55,  1.85it/s][INFO|trainer.py:2340] 2023-11-01 17:43:25,845 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-103
[INFO|configuration_utils.py:446] 2023-11-01 17:43:25,855 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-103/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:43:27,728 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-103/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:43:27,734 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-103/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:43:27,740 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-103/special_tokens_map.json
 50%|█████     | 104/206 [01:15<03:54,  2.30s/it] 51%|█████     | 105/206 [01:16<03:02,  1.81s/it] 51%|█████▏    | 106/206 [01:16<02:26,  1.47s/it] 52%|█████▏    | 107/206 [01:17<02:01,  1.23s/it] 52%|█████▏    | 108/206 [01:18<01:43,  1.06s/it] 53%|█████▎    | 109/206 [01:18<01:31,  1.06it/s] 53%|█████▎    | 110/206 [01:19<01:22,  1.16it/s] 54%|█████▍    | 111/206 [01:20<01:16,  1.24it/s] 54%|█████▍    | 112/206 [01:20<01:11,  1.31it/s] 55%|█████▍    | 113/206 [01:21<01:08,  1.36it/s] 55%|█████▌    | 114/206 [01:22<01:05,  1.39it/s] 56%|█████▌    | 115/206 [01:22<01:05,  1.39it/s] 56%|█████▋    | 116/206 [01:23<01:05,  1.38it/s] 57%|█████▋    | 117/206 [01:24<01:04,  1.38it/s] 57%|█████▋    | 118/206 [01:25<01:03,  1.39it/s] 58%|█████▊    | 119/206 [01:25<01:01,  1.40it/s] 58%|█████▊    | 120/206 [01:26<01:00,  1.42it/s] 59%|█████▊    | 121/206 [01:27<00:59,  1.44it/s] 59%|█████▉    | 122/206 [01:27<00:57,  1.45it/s] 60%|█████▉    | 123/206 [01:28<00:56,  1.46it/s] 60%|██████    | 124/206 [01:29<00:55,  1.47it/s] 61%|██████    | 125/206 [01:29<00:55,  1.47it/s] 61%|██████    | 126/206 [01:30<00:54,  1.47it/s] 62%|██████▏   | 127/206 [01:31<00:53,  1.47it/s] 62%|██████▏   | 128/206 [01:31<00:52,  1.47it/s] 63%|██████▎   | 129/206 [01:32<00:52,  1.47it/s] 63%|██████▎   | 130/206 [01:33<00:51,  1.47it/s] 64%|██████▎   | 131/206 [01:33<00:50,  1.47it/s] 64%|██████▍   | 132/206 [01:34<00:50,  1.47it/s] 65%|██████▍   | 133/206 [01:35<00:49,  1.47it/s] 65%|██████▌   | 134/206 [01:35<00:49,  1.47it/s] 66%|██████▌   | 135/206 [01:36<00:48,  1.46it/s] 66%|██████▌   | 136/206 [01:37<00:47,  1.46it/s] 67%|██████▋   | 137/206 [01:38<00:47,  1.46it/s] 67%|██████▋   | 138/206 [01:38<00:46,  1.46it/s] 67%|██████▋   | 139/206 [01:39<00:45,  1.46it/s] 68%|██████▊   | 140/206 [01:40<00:45,  1.46it/s] 68%|██████▊   | 141/206 [01:40<00:44,  1.46it/s] 69%|██████▉   | 142/206 [01:41<00:43,  1.46it/s] 69%|██████▉   | 143/206 [01:42<00:43,  1.46it/s] 70%|██████▉   | 144/206 [01:42<00:42,  1.46it/s] 70%|███████   | 145/206 [01:43<00:41,  1.46it/s] 71%|███████   | 146/206 [01:44<00:41,  1.46it/s] 71%|███████▏  | 147/206 [01:44<00:40,  1.46it/s] 72%|███████▏  | 148/206 [01:45<00:39,  1.46it/s] 72%|███████▏  | 149/206 [01:46<00:39,  1.44it/s] 73%|███████▎  | 150/206 [01:47<00:38,  1.44it/s] 73%|███████▎  | 151/206 [01:47<00:37,  1.45it/s] 74%|███████▍  | 152/206 [01:48<00:37,  1.46it/s] 74%|███████▍  | 153/206 [01:49<00:36,  1.46it/s] 75%|███████▍  | 154/206 [01:49<00:36,  1.43it/s] 75%|███████▌  | 155/206 [01:50<00:35,  1.44it/s] 76%|███████▌  | 156/206 [01:51<00:34,  1.45it/s] 76%|███████▌  | 157/206 [01:51<00:33,  1.45it/s] 77%|███████▋  | 158/206 [01:52<00:33,  1.45it/s] 77%|███████▋  | 159/206 [01:53<00:32,  1.45it/s] 78%|███████▊  | 160/206 [01:53<00:31,  1.45it/s] 78%|███████▊  | 161/206 [01:54<00:30,  1.45it/s] 79%|███████▊  | 162/206 [01:55<00:30,  1.44it/s] 79%|███████▉  | 163/206 [01:56<00:30,  1.43it/s] 80%|███████▉  | 164/206 [01:56<00:29,  1.43it/s] 80%|████████  | 165/206 [01:57<00:28,  1.44it/s] 81%|████████  | 166/206 [01:58<00:27,  1.45it/s] 81%|████████  | 167/206 [01:58<00:27,  1.43it/s] 82%|████████▏ | 168/206 [01:59<00:26,  1.44it/s] 82%|████████▏ | 169/206 [02:00<00:25,  1.45it/s] 83%|████████▎ | 170/206 [02:00<00:24,  1.44it/s] 83%|████████▎ | 171/206 [02:01<00:24,  1.45it/s] 83%|████████▎ | 172/206 [02:02<00:23,  1.45it/s] 84%|████████▍ | 173/206 [02:02<00:22,  1.45it/s] 84%|████████▍ | 174/206 [02:03<00:21,  1.45it/s] 85%|████████▍ | 175/206 [02:04<00:21,  1.45it/s] 85%|████████▌ | 176/206 [02:04<00:20,  1.45it/s] 86%|████████▌ | 177/206 [02:05<00:19,  1.45it/s] 86%|████████▋ | 178/206 [02:06<00:19,  1.43it/s] 87%|████████▋ | 179/206 [02:07<00:18,  1.44it/s] 87%|████████▋ | 180/206 [02:07<00:18,  1.44it/s] 88%|████████▊ | 181/206 [02:08<00:17,  1.45it/s] 88%|████████▊ | 182/206 [02:09<00:16,  1.45it/s] 89%|████████▉ | 183/206 [02:09<00:16,  1.43it/s] 89%|████████▉ | 184/206 [02:10<00:15,  1.42it/s] 90%|████████▉ | 185/206 [02:11<00:14,  1.42it/s] 90%|█████████ | 186/206 [02:11<00:13,  1.43it/s] 91%|█████████ | 187/206 [02:12<00:13,  1.44it/s] 91%|█████████▏| 188/206 [02:13<00:12,  1.43it/s] 92%|█████████▏| 189/206 [02:14<00:11,  1.44it/s] 92%|█████████▏| 190/206 [02:14<00:11,  1.42it/s] 93%|█████████▎| 191/206 [02:15<00:10,  1.41it/s] 93%|█████████▎| 192/206 [02:16<00:09,  1.42it/s] 94%|█████████▎| 193/206 [02:16<00:09,  1.43it/s] 94%|█████████▍| 194/206 [02:17<00:08,  1.44it/s] 95%|█████████▍| 195/206 [02:18<00:07,  1.42it/s] 95%|█████████▌| 196/206 [02:18<00:06,  1.43it/s] 96%|█████████▌| 197/206 [02:19<00:06,  1.41it/s] 96%|█████████▌| 198/206 [02:20<00:05,  1.41it/s] 97%|█████████▋| 199/206 [02:21<00:04,  1.42it/s] 97%|█████████▋| 200/206 [02:21<00:04,  1.43it/s] 98%|█████████▊| 201/206 [02:22<00:03,  1.44it/s] 98%|█████████▊| 202/206 [02:23<00:02,  1.43it/s] 99%|█████████▊| 203/206 [02:23<00:02,  1.44it/s] 99%|█████████▉| 204/206 [02:24<00:01,  1.45it/s]100%|█████████▉| 205/206 [02:25<00:00,  1.46it/s]100%|██████████| 206/206 [02:25<00:00,  1.86it/s][INFO|trainer.py:2340] 2023-11-01 17:44:42,121 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-206
[INFO|configuration_utils.py:446] 2023-11-01 17:44:42,131 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-206/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:44:44,102 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-206/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:44:44,110 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-206/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:44:44,116 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/checkpoint-206/special_tokens_map.json
[INFO|trainer.py:1662] 2023-11-01 17:44:47,696 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 206/206 [02:31<00:00,  1.86it/s]100%|██████████| 206/206 [02:31<00:00,  1.36it/s]
[INFO|trainer.py:2340] 2023-11-01 17:44:47,741 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1
[INFO|configuration_utils.py:446] 2023-11-01 17:44:47,751 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:44:49,631 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:44:49,640 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:44:49,648 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.1/special_tokens_map.json
{'train_runtime': 151.0413, 'train_samples_per_second': 86.612, 'train_steps_per_second': 1.364, 'train_loss': 0.6932422674975349, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.6932
  train_runtime            = 0:02:31.04
  train_samples            =       6541
  train_samples_per_second =     86.612
  train_steps_per_second   =      1.364
11/01/2023 17:44:49 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-11-01 17:44:49,842 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-11-01 17:44:49,844 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:44:49,844 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-11-01 17:44:49,844 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-11-01 17:44:49,844 >>   Num examples = 8341
[INFO|trainer.py:2595] 2023-11-01 17:44:49,844 >>   Batch size = 64
  0%|          | 0/131 [00:00<?, ?it/s]  2%|▏         | 2/131 [00:00<00:14,  8.64it/s]  2%|▏         | 3/131 [00:00<00:21,  6.07it/s]  3%|▎         | 4/131 [00:00<00:24,  5.24it/s]  4%|▍         | 5/131 [00:00<00:25,  4.85it/s]  5%|▍         | 6/131 [00:01<00:26,  4.64it/s]  5%|▌         | 7/131 [00:01<00:27,  4.52it/s]  6%|▌         | 8/131 [00:01<00:27,  4.44it/s]  7%|▋         | 9/131 [00:01<00:27,  4.39it/s]  8%|▊         | 10/131 [00:02<00:27,  4.35it/s]  8%|▊         | 11/131 [00:02<00:27,  4.32it/s]  9%|▉         | 12/131 [00:02<00:27,  4.31it/s] 10%|▉         | 13/131 [00:02<00:27,  4.30it/s] 11%|█         | 14/131 [00:03<00:27,  4.29it/s] 11%|█▏        | 15/131 [00:03<00:27,  4.28it/s] 12%|█▏        | 16/131 [00:03<00:26,  4.28it/s] 13%|█▎        | 17/131 [00:03<00:26,  4.28it/s] 14%|█▎        | 18/131 [00:03<00:26,  4.28it/s] 15%|█▍        | 19/131 [00:04<00:26,  4.27it/s] 15%|█▌        | 20/131 [00:04<00:25,  4.28it/s] 16%|█▌        | 21/131 [00:04<00:25,  4.27it/s] 17%|█▋        | 22/131 [00:04<00:25,  4.27it/s] 18%|█▊        | 23/131 [00:05<00:25,  4.27it/s] 18%|█▊        | 24/131 [00:05<00:25,  4.27it/s] 19%|█▉        | 25/131 [00:05<00:24,  4.26it/s] 20%|█▉        | 26/131 [00:05<00:24,  4.26it/s] 21%|██        | 27/131 [00:06<00:24,  4.26it/s] 21%|██▏       | 28/131 [00:06<00:24,  4.26it/s] 22%|██▏       | 29/131 [00:06<00:23,  4.25it/s] 23%|██▎       | 30/131 [00:06<00:23,  4.26it/s] 24%|██▎       | 31/131 [00:07<00:23,  4.26it/s] 24%|██▍       | 32/131 [00:07<00:23,  4.26it/s] 25%|██▌       | 33/131 [00:07<00:23,  4.26it/s] 26%|██▌       | 34/131 [00:07<00:22,  4.25it/s] 27%|██▋       | 35/131 [00:07<00:22,  4.25it/s] 27%|██▋       | 36/131 [00:08<00:22,  4.26it/s] 28%|██▊       | 37/131 [00:08<00:22,  4.25it/s] 29%|██▉       | 38/131 [00:08<00:21,  4.25it/s] 30%|██▉       | 39/131 [00:08<00:21,  4.26it/s] 31%|███       | 40/131 [00:09<00:21,  4.26it/s] 31%|███▏      | 41/131 [00:09<00:21,  4.26it/s] 32%|███▏      | 42/131 [00:09<00:20,  4.26it/s] 33%|███▎      | 43/131 [00:09<00:20,  4.25it/s] 34%|███▎      | 44/131 [00:10<00:20,  4.24it/s] 34%|███▍      | 45/131 [00:10<00:20,  4.24it/s] 35%|███▌      | 46/131 [00:10<00:20,  4.25it/s] 36%|███▌      | 47/131 [00:10<00:19,  4.25it/s] 37%|███▋      | 48/131 [00:11<00:19,  4.22it/s] 37%|███▋      | 49/131 [00:11<00:19,  4.18it/s] 38%|███▊      | 50/131 [00:11<00:19,  4.17it/s] 39%|███▉      | 51/131 [00:11<00:19,  4.16it/s] 40%|███▉      | 52/131 [00:11<00:18,  4.17it/s] 40%|████      | 53/131 [00:12<00:18,  4.17it/s] 41%|████      | 54/131 [00:12<00:18,  4.19it/s] 42%|████▏     | 55/131 [00:12<00:18,  4.21it/s] 43%|████▎     | 56/131 [00:12<00:17,  4.22it/s] 44%|████▎     | 57/131 [00:13<00:17,  4.23it/s] 44%|████▍     | 58/131 [00:13<00:17,  4.24it/s] 45%|████▌     | 59/131 [00:13<00:17,  4.22it/s] 46%|████▌     | 60/131 [00:13<00:16,  4.18it/s] 47%|████▋     | 61/131 [00:14<00:16,  4.18it/s] 47%|████▋     | 62/131 [00:14<00:16,  4.19it/s] 48%|████▊     | 63/131 [00:14<00:16,  4.20it/s] 49%|████▉     | 64/131 [00:14<00:15,  4.21it/s] 50%|████▉     | 65/131 [00:15<00:15,  4.23it/s] 50%|█████     | 66/131 [00:15<00:15,  4.19it/s] 51%|█████     | 67/131 [00:15<00:15,  4.20it/s] 52%|█████▏    | 68/131 [00:15<00:14,  4.21it/s] 53%|█████▎    | 69/131 [00:16<00:14,  4.22it/s] 53%|█████▎    | 70/131 [00:16<00:14,  4.23it/s] 54%|█████▍    | 71/131 [00:16<00:14,  4.24it/s] 55%|█████▍    | 72/131 [00:16<00:13,  4.22it/s] 56%|█████▌    | 73/131 [00:16<00:13,  4.19it/s] 56%|█████▋    | 74/131 [00:17<00:13,  4.20it/s] 57%|█████▋    | 75/131 [00:17<00:13,  4.22it/s] 58%|█████▊    | 76/131 [00:17<00:13,  4.22it/s] 59%|█████▉    | 77/131 [00:17<00:12,  4.22it/s] 60%|█████▉    | 78/131 [00:18<00:12,  4.19it/s] 60%|██████    | 79/131 [00:18<00:12,  4.14it/s] 61%|██████    | 80/131 [00:18<00:12,  4.13it/s] 62%|██████▏   | 81/131 [00:18<00:12,  4.13it/s] 63%|██████▎   | 82/131 [00:19<00:11,  4.16it/s] 63%|██████▎   | 83/131 [00:19<00:11,  4.16it/s] 64%|██████▍   | 84/131 [00:19<00:11,  4.15it/s] 65%|██████▍   | 85/131 [00:19<00:11,  4.16it/s] 66%|██████▌   | 86/131 [00:20<00:10,  4.18it/s] 66%|██████▋   | 87/131 [00:20<00:10,  4.21it/s] 67%|██████▋   | 88/131 [00:20<00:10,  4.22it/s] 68%|██████▊   | 89/131 [00:20<00:09,  4.21it/s] 69%|██████▊   | 90/131 [00:21<00:09,  4.19it/s] 69%|██████▉   | 91/131 [00:21<00:09,  4.20it/s] 70%|███████   | 92/131 [00:21<00:09,  4.22it/s] 71%|███████   | 93/131 [00:21<00:09,  4.21it/s] 72%|███████▏  | 94/131 [00:21<00:08,  4.22it/s] 73%|███████▎  | 95/131 [00:22<00:08,  4.23it/s] 73%|███████▎  | 96/131 [00:22<00:08,  4.23it/s] 74%|███████▍  | 97/131 [00:22<00:08,  4.23it/s] 75%|███████▍  | 98/131 [00:22<00:07,  4.23it/s] 76%|███████▌  | 99/131 [00:23<00:07,  4.23it/s] 76%|███████▋  | 100/131 [00:23<00:07,  4.24it/s] 77%|███████▋  | 101/131 [00:23<00:07,  4.24it/s] 78%|███████▊  | 102/131 [00:23<00:06,  4.24it/s] 79%|███████▊  | 103/131 [00:24<00:06,  4.23it/s] 79%|███████▉  | 104/131 [00:24<00:06,  4.23it/s] 80%|████████  | 105/131 [00:24<00:06,  4.24it/s] 81%|████████  | 106/131 [00:24<00:05,  4.23it/s] 82%|████████▏ | 107/131 [00:25<00:05,  4.23it/s] 82%|████████▏ | 108/131 [00:25<00:05,  4.22it/s] 83%|████████▎ | 109/131 [00:25<00:05,  4.19it/s] 84%|████████▍ | 110/131 [00:25<00:05,  4.09it/s] 85%|████████▍ | 111/131 [00:26<00:04,  4.05it/s] 85%|████████▌ | 112/131 [00:26<00:04,  4.03it/s] 86%|████████▋ | 113/131 [00:26<00:04,  4.04it/s] 87%|████████▋ | 114/131 [00:26<00:04,  4.05it/s] 88%|████████▊ | 115/131 [00:27<00:03,  4.07it/s] 89%|████████▊ | 116/131 [00:27<00:03,  4.12it/s] 89%|████████▉ | 117/131 [00:27<00:03,  4.14it/s] 90%|█████████ | 118/131 [00:27<00:03,  4.16it/s] 91%|█████████ | 119/131 [00:27<00:02,  4.16it/s] 92%|█████████▏| 120/131 [00:28<00:02,  4.09it/s] 92%|█████████▏| 121/131 [00:28<00:02,  4.09it/s] 93%|█████████▎| 122/131 [00:28<00:02,  4.09it/s] 94%|█████████▍| 123/131 [00:28<00:01,  4.13it/s] 95%|█████████▍| 124/131 [00:29<00:01,  4.13it/s] 95%|█████████▌| 125/131 [00:29<00:01,  4.07it/s] 96%|█████████▌| 126/131 [00:29<00:01,  4.05it/s] 97%|█████████▋| 127/131 [00:29<00:00,  4.07it/s] 98%|█████████▊| 128/131 [00:30<00:00,  4.08it/s] 98%|█████████▊| 129/131 [00:30<00:00,  4.08it/s] 99%|█████████▉| 130/131 [00:30<00:00,  4.07it/s][WARNING|training_args.py:1095] 2023-11-01 17:45:20,884 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 131/131 [00:30<00:00,  4.25it/s]
[WARNING|training_args.py:1095] 2023-11-01 17:45:20,950 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:45:20,950 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-11-01 17:45:21,141 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5157654881477356}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.5158
  eval_loss               =      0.691
  eval_runtime            = 0:00:31.03
  eval_samples            =       8341
  eval_samples_per_second =    268.721
  eval_steps_per_second   =       4.22
Processing train_examples_0.5.csv
11/01/2023 17:45:37 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/01/2023 17:45:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/runs/Nov01_17-45-37_clip08.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/01/2023 17:45:37 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/experiments/pretraining-vs-finetuning/finetuning_data/social/train_examples_0.5.csv
11/01/2023 17:45:37 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/social/analysis_model_examples/dev_examples.csv
11/01/2023 17:45:37 - WARNING - datasets.builder - Using custom data configuration default-7b679c9ace78acaf
11/01/2023 17:45:37 - INFO - datasets.builder - Generating dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-7b679c9ace78acaf/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Downloading and preparing dataset csv/default to /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-7b679c9ace78acaf/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 9653.17it/s]11/01/2023 17:45:37 - INFO - datasets.download.download_manager - Downloading took 0.0 min
11/01/2023 17:45:37 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min

Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 20.86it/s]11/01/2023 17:45:37 - INFO - datasets.utils.info_utils - Unable to verify checksums.
11/01/2023 17:45:37 - INFO - datasets.builder - Generating train split

0 tables [00:00, ? tables/s]4 tables [00:00, 39.87 tables/s]                                11/01/2023 17:45:37 - INFO - datasets.builder - Generating validation split
0 tables [00:00, ? tables/s]                            11/01/2023 17:45:37 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-7b679c9ace78acaf/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 169.31it/s]
[INFO|configuration_utils.py:659] 2023-11-01 17:45:38,132 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:45:38,137 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-11-01 17:45:38,178 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-11-01 17:45:38,231 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:45:38,232 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:45:38,475 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:45:38,475 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:45:38,475 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:45:38,475 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:45:38,475 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-11-01 17:45:38,658 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:45:38,659 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-11-01 17:45:39,138 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-11-01 17:45:40,449 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-11-01 17:45:40,449 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
11/01/2023 17:45:41 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7efc7c85b1f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/33 [00:00<?, ?ba/s]11/01/2023 17:45:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-7b679c9ace78acaf/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:   3%|▎         | 1/33 [00:00<00:10,  2.97ba/s]Running tokenizer on dataset:   6%|▌         | 2/33 [00:00<00:09,  3.19ba/s]Running tokenizer on dataset:   9%|▉         | 3/33 [00:00<00:09,  3.30ba/s]Running tokenizer on dataset:  12%|█▏        | 4/33 [00:01<00:08,  3.38ba/s]Running tokenizer on dataset:  15%|█▌        | 5/33 [00:01<00:08,  3.45ba/s]Running tokenizer on dataset:  18%|█▊        | 6/33 [00:01<00:07,  3.53ba/s]Running tokenizer on dataset:  21%|██        | 7/33 [00:02<00:07,  3.34ba/s]Running tokenizer on dataset:  24%|██▍       | 8/33 [00:02<00:07,  3.43ba/s]Running tokenizer on dataset:  27%|██▋       | 9/33 [00:02<00:06,  3.50ba/s]Running tokenizer on dataset:  30%|███       | 10/33 [00:02<00:06,  3.55ba/s]Running tokenizer on dataset:  33%|███▎      | 11/33 [00:03<00:06,  3.57ba/s]Running tokenizer on dataset:  36%|███▋      | 12/33 [00:03<00:05,  3.62ba/s]Running tokenizer on dataset:  39%|███▉      | 13/33 [00:03<00:05,  3.66ba/s]Running tokenizer on dataset:  42%|████▏     | 14/33 [00:03<00:05,  3.66ba/s]Running tokenizer on dataset:  45%|████▌     | 15/33 [00:04<00:04,  3.68ba/s]Running tokenizer on dataset:  48%|████▊     | 16/33 [00:04<00:04,  3.70ba/s]Running tokenizer on dataset:  52%|█████▏    | 17/33 [00:04<00:04,  3.69ba/s]Running tokenizer on dataset:  55%|█████▍    | 18/33 [00:05<00:04,  3.71ba/s]Running tokenizer on dataset:  58%|█████▊    | 19/33 [00:05<00:03,  3.72ba/s]Running tokenizer on dataset:  61%|██████    | 20/33 [00:05<00:03,  3.71ba/s]Running tokenizer on dataset:  64%|██████▎   | 21/33 [00:05<00:03,  3.73ba/s]Running tokenizer on dataset:  67%|██████▋   | 22/33 [00:06<00:02,  3.74ba/s]Running tokenizer on dataset:  70%|██████▉   | 23/33 [00:06<00:02,  3.74ba/s]Running tokenizer on dataset:  73%|███████▎  | 24/33 [00:06<00:02,  3.71ba/s]Running tokenizer on dataset:  76%|███████▌  | 25/33 [00:06<00:02,  3.70ba/s]Running tokenizer on dataset:  79%|███████▉  | 26/33 [00:07<00:01,  3.73ba/s]Running tokenizer on dataset:  82%|████████▏ | 27/33 [00:07<00:01,  3.75ba/s]Running tokenizer on dataset:  85%|████████▍ | 28/33 [00:07<00:01,  3.77ba/s]Running tokenizer on dataset:  88%|████████▊ | 29/33 [00:08<00:01,  3.51ba/s]Running tokenizer on dataset:  91%|█████████ | 30/33 [00:08<00:00,  3.59ba/s]Running tokenizer on dataset:  94%|█████████▍| 31/33 [00:08<00:00,  3.63ba/s]Running tokenizer on dataset:  97%|█████████▋| 32/33 [00:08<00:00,  3.69ba/s]Running tokenizer on dataset: 100%|██████████| 33/33 [00:09<00:00,  4.25ba/s]Running tokenizer on dataset: 100%|██████████| 33/33 [00:09<00:00,  3.66ba/s]11/01/2023 17:45:51 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7efc7c88d820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/9 [00:00<?, ?ba/s]11/01/2023 17:45:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-7b679c9ace78acaf/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset:  11%|█         | 1/9 [00:00<00:02,  3.74ba/s]Running tokenizer on dataset:  22%|██▏       | 2/9 [00:00<00:01,  3.80ba/s]Running tokenizer on dataset:  33%|███▎      | 3/9 [00:00<00:01,  3.77ba/s]Running tokenizer on dataset:  44%|████▍     | 4/9 [00:01<00:01,  3.70ba/s]Running tokenizer on dataset:  56%|█████▌    | 5/9 [00:01<00:01,  3.71ba/s]Running tokenizer on dataset:  67%|██████▋   | 6/9 [00:01<00:00,  3.73ba/s]Running tokenizer on dataset:  78%|███████▊  | 7/9 [00:01<00:00,  3.76ba/s]Running tokenizer on dataset:  89%|████████▉ | 8/9 [00:02<00:00,  3.78ba/s]Running tokenizer on dataset: 100%|██████████| 9/9 [00:02<00:00,  4.06ba/s]hi
11/01/2023 17:45:53 - INFO - __main__ - Sample 9012 of the training set: {'sentence1': " It's bad to control what your partner does.", 'sentence2': 'they have a lot of bad habits that you need help controlling.', 'label': 0, 'input_ids': [0, 85, 18, 1099, 7, 797, 99, 110, 1784, 473, 4, 2, 2, 10010, 33, 10, 319, 9, 1099, 10095, 14, 47, 240, 244, 10568, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:45:53 - INFO - __main__ - Sample 8024 of the training set: {'sentence1': ' It is expected that your mother would know what kind of gifts you wants.', 'sentence2': 'You are estranged from your parents.', 'label': 0, 'input_ids': [0, 85, 16, 421, 14, 110, 985, 74, 216, 99, 761, 9, 7420, 47, 1072, 4, 2, 2, 1185, 32, 20599, 31, 110, 1041, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:45:53 - INFO - __main__ - Sample 7314 of the training set: {'sentence1': ' You are not supposed to tell other people how to treat their children.', 'sentence2': "You are the children's therapists.", 'label': 0, 'input_ids': [0, 370, 32, 45, 3518, 7, 1137, 97, 82, 141, 7, 3951, 49, 408, 4, 2, 2, 1185, 32, 5, 408, 18, 32464, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:622] 2023-11-01 17:45:58,172 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-11-01 17:45:58,197 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-11-01 17:45:58,197 >>   Num examples = 32554
[INFO|trainer.py:1421] 2023-11-01 17:45:58,197 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-11-01 17:45:58,197 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-11-01 17:45:58,197 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-11-01 17:45:58,197 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-11-01 17:45:58,197 >>   Total optimization steps = 1018
[WARNING|training_args.py:1095] 2023-11-01 17:45:58,211 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/1018 [00:00<?, ?it/s]  0%|          | 1/1018 [00:00<11:41,  1.45it/s]  0%|          | 2/1018 [00:01<11:20,  1.49it/s]  0%|          | 3/1018 [00:02<11:14,  1.51it/s]  0%|          | 4/1018 [00:02<11:11,  1.51it/s]  0%|          | 5/1018 [00:03<11:09,  1.51it/s]  1%|          | 6/1018 [00:03<11:08,  1.51it/s]  1%|          | 7/1018 [00:04<11:08,  1.51it/s]  1%|          | 8/1018 [00:05<11:07,  1.51it/s]  1%|          | 9/1018 [00:05<11:06,  1.51it/s]  1%|          | 10/1018 [00:06<11:04,  1.52it/s]  1%|          | 11/1018 [00:07<11:04,  1.52it/s]  1%|          | 12/1018 [00:07<11:03,  1.52it/s]  1%|▏         | 13/1018 [00:08<11:02,  1.52it/s]  1%|▏         | 14/1018 [00:09<11:01,  1.52it/s]  1%|▏         | 15/1018 [00:09<11:01,  1.52it/s]  2%|▏         | 16/1018 [00:10<11:01,  1.51it/s]  2%|▏         | 17/1018 [00:11<11:01,  1.51it/s]  2%|▏         | 18/1018 [00:11<11:01,  1.51it/s]  2%|▏         | 19/1018 [00:12<11:02,  1.51it/s]  2%|▏         | 20/1018 [00:13<11:02,  1.51it/s]  2%|▏         | 21/1018 [00:13<11:02,  1.51it/s]  2%|▏         | 22/1018 [00:14<11:02,  1.50it/s]  2%|▏         | 23/1018 [00:15<11:01,  1.50it/s]  2%|▏         | 24/1018 [00:15<11:01,  1.50it/s]  2%|▏         | 25/1018 [00:16<11:00,  1.50it/s]  3%|▎         | 26/1018 [00:17<10:59,  1.50it/s]  3%|▎         | 27/1018 [00:17<10:59,  1.50it/s]  3%|▎         | 28/1018 [00:18<10:58,  1.50it/s]  3%|▎         | 29/1018 [00:19<10:57,  1.50it/s]  3%|▎         | 30/1018 [00:19<10:57,  1.50it/s]  3%|▎         | 31/1018 [00:20<10:57,  1.50it/s]  3%|▎         | 32/1018 [00:21<10:56,  1.50it/s]  3%|▎         | 33/1018 [00:21<10:56,  1.50it/s]  3%|▎         | 34/1018 [00:22<10:54,  1.50it/s]  3%|▎         | 35/1018 [00:23<10:53,  1.50it/s]  4%|▎         | 36/1018 [00:23<10:53,  1.50it/s]  4%|▎         | 37/1018 [00:24<10:53,  1.50it/s]  4%|▎         | 38/1018 [00:25<10:54,  1.50it/s]  4%|▍         | 39/1018 [00:25<10:53,  1.50it/s]  4%|▍         | 40/1018 [00:26<10:52,  1.50it/s]  4%|▍         | 41/1018 [00:27<10:52,  1.50it/s]  4%|▍         | 42/1018 [00:27<10:51,  1.50it/s]  4%|▍         | 43/1018 [00:28<10:51,  1.50it/s]  4%|▍         | 44/1018 [00:29<10:51,  1.50it/s]  4%|▍         | 45/1018 [00:29<10:50,  1.50it/s]  5%|▍         | 46/1018 [00:30<10:50,  1.49it/s]  5%|▍         | 47/1018 [00:31<10:50,  1.49it/s]  5%|▍         | 48/1018 [00:31<10:49,  1.49it/s]  5%|▍         | 49/1018 [00:32<10:48,  1.49it/s]  5%|▍         | 50/1018 [00:33<10:48,  1.49it/s]  5%|▌         | 51/1018 [00:33<10:47,  1.49it/s]  5%|▌         | 52/1018 [00:34<10:47,  1.49it/s]  5%|▌         | 53/1018 [00:35<10:46,  1.49it/s]  5%|▌         | 54/1018 [00:35<10:45,  1.49it/s]  5%|▌         | 55/1018 [00:36<10:45,  1.49it/s]  6%|▌         | 56/1018 [00:37<10:45,  1.49it/s]  6%|▌         | 57/1018 [00:37<10:45,  1.49it/s]  6%|▌         | 58/1018 [00:38<10:44,  1.49it/s]  6%|▌         | 59/1018 [00:39<10:44,  1.49it/s]  6%|▌         | 60/1018 [00:39<10:43,  1.49it/s]  6%|▌         | 61/1018 [00:40<10:43,  1.49it/s]  6%|▌         | 62/1018 [00:41<10:42,  1.49it/s]  6%|▌         | 63/1018 [00:41<10:41,  1.49it/s]  6%|▋         | 64/1018 [00:42<10:41,  1.49it/s]  6%|▋         | 65/1018 [00:43<10:39,  1.49it/s]  6%|▋         | 66/1018 [00:43<10:39,  1.49it/s]  7%|▋         | 67/1018 [00:44<10:39,  1.49it/s]  7%|▋         | 68/1018 [00:45<10:37,  1.49it/s]  7%|▋         | 69/1018 [00:46<10:37,  1.49it/s]  7%|▋         | 70/1018 [00:46<10:38,  1.49it/s]  7%|▋         | 71/1018 [00:47<10:37,  1.49it/s]  7%|▋         | 72/1018 [00:48<10:36,  1.49it/s]  7%|▋         | 73/1018 [00:48<10:36,  1.49it/s]  7%|▋         | 74/1018 [00:49<10:45,  1.46it/s]  7%|▋         | 75/1018 [00:50<10:49,  1.45it/s]  7%|▋         | 76/1018 [00:50<10:48,  1.45it/s]  8%|▊         | 77/1018 [00:51<10:45,  1.46it/s]  8%|▊         | 78/1018 [00:52<10:40,  1.47it/s]  8%|▊         | 79/1018 [00:52<10:37,  1.47it/s]  8%|▊         | 80/1018 [00:53<10:36,  1.47it/s]  8%|▊         | 81/1018 [00:54<10:35,  1.47it/s]  8%|▊         | 82/1018 [00:54<10:34,  1.48it/s]  8%|▊         | 83/1018 [00:55<10:34,  1.47it/s]  8%|▊         | 84/1018 [00:56<10:35,  1.47it/s]  8%|▊         | 85/1018 [00:56<10:35,  1.47it/s]  8%|▊         | 86/1018 [00:57<10:36,  1.46it/s]  9%|▊         | 87/1018 [00:58<10:36,  1.46it/s]  9%|▊         | 88/1018 [00:58<10:37,  1.46it/s]  9%|▊         | 89/1018 [00:59<10:36,  1.46it/s]  9%|▉         | 90/1018 [01:00<10:36,  1.46it/s]  9%|▉         | 91/1018 [01:01<10:34,  1.46it/s]  9%|▉         | 92/1018 [01:01<10:33,  1.46it/s]  9%|▉         | 93/1018 [01:02<10:33,  1.46it/s]  9%|▉         | 94/1018 [01:03<10:32,  1.46it/s]  9%|▉         | 95/1018 [01:03<10:30,  1.46it/s]  9%|▉         | 96/1018 [01:04<10:30,  1.46it/s] 10%|▉         | 97/1018 [01:05<10:29,  1.46it/s] 10%|▉         | 98/1018 [01:05<10:29,  1.46it/s] 10%|▉         | 99/1018 [01:06<10:29,  1.46it/s] 10%|▉         | 100/1018 [01:07<10:26,  1.46it/s] 10%|▉         | 101/1018 [01:07<10:26,  1.46it/s] 10%|█         | 102/1018 [01:08<10:25,  1.46it/s] 10%|█         | 103/1018 [01:09<10:31,  1.45it/s] 10%|█         | 104/1018 [01:09<10:33,  1.44it/s] 10%|█         | 105/1018 [01:10<10:31,  1.45it/s] 10%|█         | 106/1018 [01:11<10:29,  1.45it/s] 11%|█         | 107/1018 [01:12<10:28,  1.45it/s] 11%|█         | 108/1018 [01:12<10:34,  1.43it/s] 11%|█         | 109/1018 [01:13<10:30,  1.44it/s] 11%|█         | 110/1018 [01:14<10:26,  1.45it/s] 11%|█         | 111/1018 [01:14<10:30,  1.44it/s] 11%|█         | 112/1018 [01:15<10:26,  1.44it/s] 11%|█         | 113/1018 [01:16<10:32,  1.43it/s] 11%|█         | 114/1018 [01:16<10:31,  1.43it/s] 11%|█▏        | 115/1018 [01:17<10:27,  1.44it/s] 11%|█▏        | 116/1018 [01:18<10:33,  1.42it/s] 11%|█▏        | 117/1018 [01:18<10:28,  1.43it/s] 12%|█▏        | 118/1018 [01:19<10:29,  1.43it/s] 12%|█▏        | 119/1018 [01:20<10:24,  1.44it/s] 12%|█▏        | 120/1018 [01:21<10:20,  1.45it/s] 12%|█▏        | 121/1018 [01:21<10:23,  1.44it/s] 12%|█▏        | 122/1018 [01:22<10:20,  1.44it/s] 12%|█▏        | 123/1018 [01:23<10:24,  1.43it/s] 12%|█▏        | 124/1018 [01:23<10:26,  1.43it/s] 12%|█▏        | 125/1018 [01:24<10:23,  1.43it/s] 12%|█▏        | 126/1018 [01:25<10:19,  1.44it/s] 12%|█▏        | 127/1018 [01:25<10:15,  1.45it/s] 13%|█▎        | 128/1018 [01:26<10:17,  1.44it/s] 13%|█▎        | 129/1018 [01:27<10:15,  1.44it/s] 13%|█▎        | 130/1018 [01:28<10:12,  1.45it/s] 13%|█▎        | 131/1018 [01:28<10:17,  1.44it/s] 13%|█▎        | 132/1018 [01:29<10:19,  1.43it/s] 13%|█▎        | 133/1018 [01:30<10:16,  1.44it/s] 13%|█▎        | 134/1018 [01:30<10:11,  1.45it/s] 13%|█▎        | 135/1018 [01:31<10:08,  1.45it/s] 13%|█▎        | 136/1018 [01:32<10:19,  1.42it/s] 13%|█▎        | 137/1018 [01:32<10:22,  1.41it/s] 14%|█▎        | 138/1018 [01:33<10:17,  1.43it/s] 14%|█▎        | 139/1018 [01:34<10:12,  1.43it/s] 14%|█▍        | 140/1018 [01:34<10:09,  1.44it/s] 14%|█▍        | 141/1018 [01:35<10:14,  1.43it/s] 14%|█▍        | 142/1018 [01:36<10:16,  1.42it/s] 14%|█▍        | 143/1018 [01:37<10:12,  1.43it/s] 14%|█▍        | 144/1018 [01:37<10:07,  1.44it/s] 14%|█▍        | 145/1018 [01:38<10:04,  1.44it/s] 14%|█▍        | 146/1018 [01:39<10:05,  1.44it/s] 14%|█▍        | 147/1018 [01:39<10:02,  1.45it/s] 15%|█▍        | 148/1018 [01:40<09:59,  1.45it/s] 15%|█▍        | 149/1018 [01:41<09:58,  1.45it/s] 15%|█▍        | 150/1018 [01:41<09:56,  1.46it/s] 15%|█▍        | 151/1018 [01:42<10:06,  1.43it/s] 15%|█▍        | 152/1018 [01:43<10:11,  1.42it/s] 15%|█▌        | 153/1018 [01:44<10:06,  1.43it/s] 15%|█▌        | 154/1018 [01:44<10:01,  1.44it/s] 15%|█▌        | 155/1018 [01:45<09:57,  1.45it/s] 15%|█▌        | 156/1018 [01:46<10:00,  1.44it/s] 15%|█▌        | 157/1018 [01:46<09:58,  1.44it/s] 16%|█▌        | 158/1018 [01:47<10:01,  1.43it/s] 16%|█▌        | 159/1018 [01:48<10:02,  1.42it/s] 16%|█▌        | 160/1018 [01:48<10:00,  1.43it/s] 16%|█▌        | 161/1018 [01:49<09:56,  1.44it/s] 16%|█▌        | 162/1018 [01:50<09:52,  1.45it/s] 16%|█▌        | 163/1018 [01:50<09:53,  1.44it/s] 16%|█▌        | 164/1018 [01:51<09:50,  1.45it/s] 16%|█▌        | 165/1018 [01:52<09:47,  1.45it/s] 16%|█▋        | 166/1018 [01:53<09:51,  1.44it/s] 16%|█▋        | 167/1018 [01:53<09:48,  1.45it/s] 17%|█▋        | 168/1018 [01:54<09:45,  1.45it/s] 17%|█▋        | 169/1018 [01:55<09:50,  1.44it/s] 17%|█▋        | 170/1018 [01:55<09:52,  1.43it/s] 17%|█▋        | 171/1018 [01:56<09:59,  1.41it/s] 17%|█▋        | 172/1018 [01:57<09:57,  1.42it/s] 17%|█▋        | 173/1018 [01:58<10:01,  1.41it/s] 17%|█▋        | 174/1018 [01:58<09:58,  1.41it/s] 17%|█▋        | 175/1018 [01:59<09:52,  1.42it/s] 17%|█▋        | 176/1018 [02:00<09:47,  1.43it/s] 17%|█▋        | 177/1018 [02:00<09:44,  1.44it/s] 17%|█▋        | 178/1018 [02:01<09:45,  1.43it/s] 18%|█▊        | 179/1018 [02:02<09:41,  1.44it/s] 18%|█▊        | 180/1018 [02:02<09:37,  1.45it/s] 18%|█▊        | 181/1018 [02:03<09:40,  1.44it/s] 18%|█▊        | 182/1018 [02:04<09:38,  1.44it/s] 18%|█▊        | 183/1018 [02:04<09:44,  1.43it/s] 18%|█▊        | 184/1018 [02:05<09:45,  1.42it/s] 18%|█▊        | 185/1018 [02:06<09:42,  1.43it/s] 18%|█▊        | 186/1018 [02:07<09:38,  1.44it/s] 18%|█▊        | 187/1018 [02:07<09:34,  1.45it/s] 18%|█▊        | 188/1018 [02:08<09:39,  1.43it/s] 19%|█▊        | 189/1018 [02:09<09:37,  1.44it/s] 19%|█▊        | 190/1018 [02:09<09:33,  1.44it/s] 19%|█▉        | 191/1018 [02:10<09:34,  1.44it/s] 19%|█▉        | 192/1018 [02:11<09:33,  1.44it/s] 19%|█▉        | 193/1018 [02:11<09:41,  1.42it/s] 19%|█▉        | 194/1018 [02:12<09:39,  1.42it/s] 19%|█▉        | 195/1018 [02:13<09:41,  1.41it/s] 19%|█▉        | 196/1018 [02:14<09:40,  1.42it/s] 19%|█▉        | 197/1018 [02:14<09:35,  1.43it/s] 19%|█▉        | 198/1018 [02:15<09:30,  1.44it/s] 20%|█▉        | 199/1018 [02:16<09:27,  1.44it/s] 20%|█▉        | 200/1018 [02:16<09:25,  1.45it/s] 20%|█▉        | 201/1018 [02:17<09:22,  1.45it/s] 20%|█▉        | 202/1018 [02:18<09:21,  1.45it/s] 20%|█▉        | 203/1018 [02:18<09:27,  1.44it/s] 20%|██        | 204/1018 [02:19<09:27,  1.43it/s] 20%|██        | 205/1018 [02:20<09:23,  1.44it/s] 20%|██        | 206/1018 [02:20<09:21,  1.45it/s] 20%|██        | 207/1018 [02:21<09:19,  1.45it/s] 20%|██        | 208/1018 [02:22<09:17,  1.45it/s] 21%|██        | 209/1018 [02:23<09:15,  1.46it/s] 21%|██        | 210/1018 [02:23<09:13,  1.46it/s] 21%|██        | 211/1018 [02:24<09:13,  1.46it/s] 21%|██        | 212/1018 [02:25<09:12,  1.46it/s] 21%|██        | 213/1018 [02:25<09:12,  1.46it/s] 21%|██        | 214/1018 [02:26<09:18,  1.44it/s] 21%|██        | 215/1018 [02:27<09:17,  1.44it/s] 21%|██        | 216/1018 [02:27<09:15,  1.44it/s] 21%|██▏       | 217/1018 [02:28<09:12,  1.45it/s] 21%|██▏       | 218/1018 [02:29<09:10,  1.45it/s] 22%|██▏       | 219/1018 [02:29<09:13,  1.44it/s] 22%|██▏       | 220/1018 [02:30<09:10,  1.45it/s] 22%|██▏       | 221/1018 [02:31<09:08,  1.45it/s] 22%|██▏       | 222/1018 [02:32<09:15,  1.43it/s] 22%|██▏       | 223/1018 [02:32<09:22,  1.41it/s] 22%|██▏       | 224/1018 [02:33<09:23,  1.41it/s] 22%|██▏       | 225/1018 [02:34<09:17,  1.42it/s] 22%|██▏       | 226/1018 [02:34<09:20,  1.41it/s] 22%|██▏       | 227/1018 [02:35<09:17,  1.42it/s] 22%|██▏       | 228/1018 [02:36<09:13,  1.43it/s] 22%|██▏       | 229/1018 [02:36<09:09,  1.44it/s] 23%|██▎       | 230/1018 [02:37<09:05,  1.45it/s] 23%|██▎       | 231/1018 [02:38<09:08,  1.44it/s] 23%|██▎       | 232/1018 [02:39<09:06,  1.44it/s] 23%|██▎       | 233/1018 [02:39<09:04,  1.44it/s] 23%|██▎       | 234/1018 [02:40<09:12,  1.42it/s] 23%|██▎       | 235/1018 [02:41<09:15,  1.41it/s] 23%|██▎       | 236/1018 [02:41<09:08,  1.42it/s] 23%|██▎       | 237/1018 [02:42<09:03,  1.44it/s] 23%|██▎       | 238/1018 [02:43<09:00,  1.44it/s] 23%|██▎       | 239/1018 [02:43<08:57,  1.45it/s] 24%|██▎       | 240/1018 [02:44<08:55,  1.45it/s] 24%|██▎       | 241/1018 [02:45<09:01,  1.44it/s] 24%|██▍       | 242/1018 [02:45<09:02,  1.43it/s] 24%|██▍       | 243/1018 [02:46<08:58,  1.44it/s] 24%|██▍       | 244/1018 [02:47<08:58,  1.44it/s] 24%|██▍       | 245/1018 [02:48<08:55,  1.44it/s] 24%|██▍       | 246/1018 [02:48<09:02,  1.42it/s] 24%|██▍       | 247/1018 [02:49<09:06,  1.41it/s] 24%|██▍       | 248/1018 [02:50<09:01,  1.42it/s] 24%|██▍       | 249/1018 [02:50<08:57,  1.43it/s] 25%|██▍       | 250/1018 [02:51<08:52,  1.44it/s] 25%|██▍       | 251/1018 [02:52<08:56,  1.43it/s] 25%|██▍       | 252/1018 [02:52<08:52,  1.44it/s] 25%|██▍       | 253/1018 [02:53<08:49,  1.44it/s] 25%|██▍       | 254/1018 [02:54<08:47,  1.45it/s] 25%|██▌       | 255/1018 [02:55<08:45,  1.45it/s] 25%|██▌       | 256/1018 [02:55<08:48,  1.44it/s] 25%|██▌       | 257/1018 [02:56<08:48,  1.44it/s] 25%|██▌       | 258/1018 [02:57<08:45,  1.44it/s] 25%|██▌       | 259/1018 [02:57<08:50,  1.43it/s] 26%|██▌       | 260/1018 [02:58<08:52,  1.42it/s] 26%|██▌       | 261/1018 [02:59<08:48,  1.43it/s] 26%|██▌       | 262/1018 [02:59<08:46,  1.44it/s] 26%|██▌       | 263/1018 [03:00<08:43,  1.44it/s] 26%|██▌       | 264/1018 [03:01<08:44,  1.44it/s] 26%|██▌       | 265/1018 [03:02<08:44,  1.43it/s] 26%|██▌       | 266/1018 [03:02<08:52,  1.41it/s] 26%|██▌       | 267/1018 [03:03<08:49,  1.42it/s] 26%|██▋       | 268/1018 [03:04<08:43,  1.43it/s] 26%|██▋       | 269/1018 [03:04<08:39,  1.44it/s] 27%|██▋       | 270/1018 [03:05<08:36,  1.45it/s] 27%|██▋       | 271/1018 [03:06<08:42,  1.43it/s] 27%|██▋       | 272/1018 [03:06<08:44,  1.42it/s] 27%|██▋       | 273/1018 [03:07<08:40,  1.43it/s] 27%|██▋       | 274/1018 [03:08<08:36,  1.44it/s] 27%|██▋       | 275/1018 [03:08<08:33,  1.45it/s] 27%|██▋       | 276/1018 [03:09<08:36,  1.44it/s] 27%|██▋       | 277/1018 [03:10<08:33,  1.44it/s] 27%|██▋       | 278/1018 [03:11<08:29,  1.45it/s] 27%|██▋       | 279/1018 [03:11<08:28,  1.45it/s] 28%|██▊       | 280/1018 [03:12<08:27,  1.46it/s] 28%|██▊       | 281/1018 [03:13<08:26,  1.46it/s] 28%|██▊       | 282/1018 [03:13<08:25,  1.46it/s] 28%|██▊       | 283/1018 [03:14<08:24,  1.46it/s] 28%|██▊       | 284/1018 [03:15<08:23,  1.46it/s] 28%|██▊       | 285/1018 [03:15<08:22,  1.46it/s] 28%|██▊       | 286/1018 [03:16<08:22,  1.46it/s] 28%|██▊       | 287/1018 [03:17<08:28,  1.44it/s] 28%|██▊       | 288/1018 [03:17<08:25,  1.44it/s] 28%|██▊       | 289/1018 [03:18<08:23,  1.45it/s] 28%|██▊       | 290/1018 [03:19<08:28,  1.43it/s] 29%|██▊       | 291/1018 [03:20<08:30,  1.42it/s] 29%|██▊       | 292/1018 [03:20<08:30,  1.42it/s] 29%|██▉       | 293/1018 [03:21<08:27,  1.43it/s] 29%|██▉       | 294/1018 [03:22<08:23,  1.44it/s] 29%|██▉       | 295/1018 [03:22<08:20,  1.44it/s] 29%|██▉       | 296/1018 [03:23<08:18,  1.45it/s] 29%|██▉       | 297/1018 [03:24<08:20,  1.44it/s] 29%|██▉       | 298/1018 [03:24<08:17,  1.45it/s] 29%|██▉       | 299/1018 [03:25<08:16,  1.45it/s] 29%|██▉       | 300/1018 [03:26<08:19,  1.44it/s] 30%|██▉       | 301/1018 [03:27<08:22,  1.43it/s] 30%|██▉       | 302/1018 [03:27<08:19,  1.43it/s] 30%|██▉       | 303/1018 [03:28<08:15,  1.44it/s] 30%|██▉       | 304/1018 [03:29<08:12,  1.45it/s] 30%|██▉       | 305/1018 [03:29<08:16,  1.44it/s] 30%|███       | 306/1018 [03:30<08:15,  1.44it/s] 30%|███       | 307/1018 [03:31<08:13,  1.44it/s] 30%|███       | 308/1018 [03:31<08:10,  1.45it/s] 30%|███       | 309/1018 [03:32<08:07,  1.45it/s] 30%|███       | 310/1018 [03:33<08:11,  1.44it/s] 31%|███       | 311/1018 [03:33<08:08,  1.45it/s] 31%|███       | 312/1018 [03:34<08:07,  1.45it/s] 31%|███       | 313/1018 [03:35<08:11,  1.43it/s] 31%|███       | 314/1018 [03:36<08:08,  1.44it/s] 31%|███       | 315/1018 [03:36<08:14,  1.42it/s] 31%|███       | 316/1018 [03:37<08:12,  1.43it/s] 31%|███       | 317/1018 [03:38<08:08,  1.44it/s] 31%|███       | 318/1018 [03:38<08:04,  1.44it/s] 31%|███▏      | 319/1018 [03:39<08:01,  1.45it/s] 31%|███▏      | 320/1018 [03:40<08:06,  1.44it/s] 32%|███▏      | 321/1018 [03:40<08:04,  1.44it/s] 32%|███▏      | 322/1018 [03:41<08:02,  1.44it/s] 32%|███▏      | 323/1018 [03:42<08:04,  1.44it/s] 32%|███▏      | 324/1018 [03:42<08:00,  1.44it/s] 32%|███▏      | 325/1018 [03:43<07:58,  1.45it/s] 32%|███▏      | 326/1018 [03:44<07:57,  1.45it/s] 32%|███▏      | 327/1018 [03:45<07:55,  1.45it/s] 32%|███▏      | 328/1018 [03:45<08:00,  1.44it/s] 32%|███▏      | 329/1018 [03:46<07:59,  1.44it/s] 32%|███▏      | 330/1018 [03:47<07:56,  1.44it/s] 33%|███▎      | 331/1018 [03:47<07:53,  1.45it/s] 33%|███▎      | 332/1018 [03:48<07:52,  1.45it/s] 33%|███▎      | 333/1018 [03:49<07:50,  1.46it/s] 33%|███▎      | 334/1018 [03:49<07:49,  1.46it/s] 33%|███▎      | 335/1018 [03:50<07:48,  1.46it/s] 33%|███▎      | 336/1018 [03:51<07:54,  1.44it/s] 33%|███▎      | 337/1018 [03:51<07:51,  1.44it/s] 33%|███▎      | 338/1018 [03:52<07:49,  1.45it/s] 33%|███▎      | 339/1018 [03:53<07:50,  1.44it/s] 33%|███▎      | 340/1018 [03:54<07:47,  1.45it/s] 33%|███▎      | 341/1018 [03:54<07:45,  1.45it/s] 34%|███▎      | 342/1018 [03:55<07:49,  1.44it/s] 34%|███▎      | 343/1018 [03:56<07:47,  1.44it/s] 34%|███▍      | 344/1018 [03:56<07:45,  1.45it/s] 34%|███▍      | 345/1018 [03:57<07:43,  1.45it/s] 34%|███▍      | 346/1018 [03:58<07:42,  1.45it/s] 34%|███▍      | 347/1018 [03:58<07:46,  1.44it/s] 34%|███▍      | 348/1018 [03:59<07:43,  1.44it/s] 34%|███▍      | 349/1018 [04:00<07:41,  1.45it/s] 34%|███▍      | 350/1018 [04:00<07:40,  1.45it/s] 34%|███▍      | 351/1018 [04:01<07:38,  1.46it/s] 35%|███▍      | 352/1018 [04:02<07:37,  1.46it/s] 35%|███▍      | 353/1018 [04:02<07:36,  1.46it/s] 35%|███▍      | 354/1018 [04:03<07:35,  1.46it/s] 35%|███▍      | 355/1018 [04:04<07:39,  1.44it/s] 35%|███▍      | 356/1018 [04:05<07:39,  1.44it/s] 35%|███▌      | 357/1018 [04:05<07:37,  1.45it/s] 35%|███▌      | 358/1018 [04:06<07:37,  1.44it/s] 35%|███▌      | 359/1018 [04:07<07:34,  1.45it/s] 35%|███▌      | 360/1018 [04:07<07:32,  1.45it/s] 35%|███▌      | 361/1018 [04:08<07:31,  1.46it/s] 36%|███▌      | 362/1018 [04:09<07:29,  1.46it/s] 36%|███▌      | 363/1018 [04:09<07:33,  1.44it/s] 36%|███▌      | 364/1018 [04:10<07:31,  1.45it/s] 36%|███▌      | 365/1018 [04:11<07:29,  1.45it/s] 36%|███▌      | 366/1018 [04:11<07:32,  1.44it/s] 36%|███▌      | 367/1018 [04:12<07:30,  1.44it/s] 36%|███▌      | 368/1018 [04:13<07:28,  1.45it/s] 36%|███▌      | 369/1018 [04:14<07:26,  1.45it/s] 36%|███▋      | 370/1018 [04:14<07:25,  1.45it/s] 36%|███▋      | 371/1018 [04:15<07:28,  1.44it/s] 37%|███▋      | 372/1018 [04:16<07:27,  1.44it/s] 37%|███▋      | 373/1018 [04:16<07:25,  1.45it/s] 37%|███▋      | 374/1018 [04:17<07:28,  1.44it/s] 37%|███▋      | 375/1018 [04:18<07:25,  1.44it/s] 37%|███▋      | 376/1018 [04:18<07:22,  1.45it/s] 37%|███▋      | 377/1018 [04:19<07:21,  1.45it/s] 37%|███▋      | 378/1018 [04:20<07:19,  1.45it/s] 37%|███▋      | 379/1018 [04:20<07:18,  1.46it/s] 37%|███▋      | 380/1018 [04:21<07:17,  1.46it/s] 37%|███▋      | 381/1018 [04:22<07:16,  1.46it/s] 38%|███▊      | 382/1018 [04:23<07:23,  1.43it/s] 38%|███▊      | 383/1018 [04:23<07:22,  1.44it/s] 38%|███▊      | 384/1018 [04:24<07:19,  1.44it/s] 38%|███▊      | 385/1018 [04:25<07:18,  1.44it/s] 38%|███▊      | 386/1018 [04:25<07:15,  1.45it/s] 38%|███▊      | 387/1018 [04:26<07:19,  1.43it/s] 38%|███▊      | 388/1018 [04:27<07:16,  1.44it/s] 38%|███▊      | 389/1018 [04:27<07:13,  1.45it/s] 38%|███▊      | 390/1018 [04:28<07:16,  1.44it/s] 38%|███▊      | 391/1018 [04:29<07:14,  1.44it/s] 39%|███▊      | 392/1018 [04:29<07:12,  1.45it/s] 39%|███▊      | 393/1018 [04:30<07:10,  1.45it/s] 39%|███▊      | 394/1018 [04:31<07:09,  1.45it/s] 39%|███▉      | 395/1018 [04:31<07:08,  1.45it/s] 39%|███▉      | 396/1018 [04:32<07:07,  1.46it/s] 39%|███▉      | 397/1018 [04:33<07:06,  1.46it/s] 39%|███▉      | 398/1018 [04:34<07:08,  1.45it/s] 39%|███▉      | 399/1018 [04:34<07:07,  1.45it/s] 39%|███▉      | 400/1018 [04:35<07:04,  1.46it/s] 39%|███▉      | 401/1018 [04:36<07:05,  1.45it/s] 39%|███▉      | 402/1018 [04:36<07:04,  1.45it/s] 40%|███▉      | 403/1018 [04:37<07:02,  1.46it/s] 40%|███▉      | 404/1018 [04:38<07:01,  1.46it/s] 40%|███▉      | 405/1018 [04:38<07:00,  1.46it/s] 40%|███▉      | 406/1018 [04:39<06:59,  1.46it/s] 40%|███▉      | 407/1018 [04:40<06:58,  1.46it/s] 40%|████      | 408/1018 [04:40<06:57,  1.46it/s] 40%|████      | 409/1018 [04:41<06:56,  1.46it/s] 40%|████      | 410/1018 [04:42<06:56,  1.46it/s] 40%|████      | 411/1018 [04:42<06:55,  1.46it/s] 40%|████      | 412/1018 [04:43<06:58,  1.45it/s] 41%|████      | 413/1018 [04:44<06:56,  1.45it/s] 41%|████      | 414/1018 [04:45<06:54,  1.46it/s] 41%|████      | 415/1018 [04:45<06:57,  1.44it/s] 41%|████      | 416/1018 [04:46<06:55,  1.45it/s] 41%|████      | 417/1018 [04:47<06:52,  1.46it/s] 41%|████      | 418/1018 [04:47<06:55,  1.44it/s] 41%|████      | 419/1018 [04:48<06:58,  1.43it/s] 41%|████▏     | 420/1018 [04:49<06:56,  1.44it/s] 41%|████▏     | 421/1018 [04:49<06:53,  1.44it/s] 41%|████▏     | 422/1018 [04:50<06:51,  1.45it/s] 42%|████▏     | 423/1018 [04:51<06:53,  1.44it/s] 42%|████▏     | 424/1018 [04:51<06:50,  1.45it/s] 42%|████▏     | 425/1018 [04:52<06:52,  1.44it/s] 42%|████▏     | 426/1018 [04:53<06:49,  1.45it/s] 42%|████▏     | 427/1018 [04:54<06:47,  1.45it/s] 42%|████▏     | 428/1018 [04:54<06:50,  1.44it/s] 42%|████▏     | 429/1018 [04:55<06:48,  1.44it/s] 42%|████▏     | 430/1018 [04:56<06:46,  1.45it/s] 42%|████▏     | 431/1018 [04:56<06:44,  1.45it/s] 42%|████▏     | 432/1018 [04:57<06:43,  1.45it/s] 43%|████▎     | 433/1018 [04:58<06:42,  1.45it/s] 43%|████▎     | 434/1018 [04:58<06:41,  1.46it/s] 43%|████▎     | 435/1018 [04:59<06:40,  1.46it/s] 43%|████▎     | 436/1018 [05:00<06:44,  1.44it/s] 43%|████▎     | 437/1018 [05:00<06:41,  1.45it/s] 43%|████▎     | 438/1018 [05:01<06:40,  1.45it/s] 43%|████▎     | 439/1018 [05:02<06:40,  1.44it/s] 43%|████▎     | 440/1018 [05:03<06:38,  1.45it/s] 43%|████▎     | 441/1018 [05:03<06:36,  1.46it/s] 43%|████▎     | 442/1018 [05:04<06:35,  1.46it/s] 44%|████▎     | 443/1018 [05:05<06:35,  1.45it/s] 44%|████▎     | 444/1018 [05:05<06:35,  1.45it/s] 44%|████▎     | 445/1018 [05:06<06:33,  1.45it/s] 44%|████▍     | 446/1018 [05:07<06:33,  1.46it/s] 44%|████▍     | 447/1018 [05:07<06:38,  1.43it/s] 44%|████▍     | 448/1018 [05:08<06:36,  1.44it/s] 44%|████▍     | 449/1018 [05:09<06:33,  1.45it/s] 44%|████▍     | 450/1018 [05:09<06:33,  1.44it/s] 44%|████▍     | 451/1018 [05:10<06:33,  1.44it/s] 44%|████▍     | 452/1018 [05:11<06:31,  1.45it/s] 44%|████▍     | 453/1018 [05:11<06:28,  1.45it/s] 45%|████▍     | 454/1018 [05:12<06:27,  1.46it/s] 45%|████▍     | 455/1018 [05:13<06:31,  1.44it/s] 45%|████▍     | 456/1018 [05:14<06:29,  1.44it/s] 45%|████▍     | 457/1018 [05:14<06:27,  1.45it/s] 45%|████▍     | 458/1018 [05:15<06:27,  1.44it/s] 45%|████▌     | 459/1018 [05:16<06:25,  1.45it/s] 45%|████▌     | 460/1018 [05:16<06:23,  1.45it/s] 45%|████▌     | 461/1018 [05:17<06:23,  1.45it/s] 45%|████▌     | 462/1018 [05:18<06:22,  1.45it/s] 45%|████▌     | 463/1018 [05:18<06:21,  1.46it/s] 46%|████▌     | 464/1018 [05:19<06:20,  1.46it/s] 46%|████▌     | 465/1018 [05:20<06:20,  1.45it/s] 46%|████▌     | 466/1018 [05:20<06:19,  1.45it/s] 46%|████▌     | 467/1018 [05:21<06:18,  1.45it/s] 46%|████▌     | 468/1018 [05:22<06:17,  1.46it/s] 46%|████▌     | 469/1018 [05:23<06:19,  1.45it/s] 46%|████▌     | 470/1018 [05:23<06:17,  1.45it/s] 46%|████▋     | 471/1018 [05:24<06:16,  1.45it/s] 46%|████▋     | 472/1018 [05:25<06:14,  1.46it/s] 46%|████▋     | 473/1018 [05:25<06:13,  1.46it/s] 47%|████▋     | 474/1018 [05:26<06:15,  1.45it/s] 47%|████▋     | 475/1018 [05:27<06:14,  1.45it/s] 47%|████▋     | 476/1018 [05:27<06:13,  1.45it/s] 47%|████▋     | 477/1018 [05:28<06:12,  1.45it/s] 47%|████▋     | 478/1018 [05:29<06:11,  1.46it/s] 47%|████▋     | 479/1018 [05:29<06:09,  1.46it/s] 47%|████▋     | 480/1018 [05:30<06:12,  1.45it/s] 47%|████▋     | 481/1018 [05:31<06:10,  1.45it/s] 47%|████▋     | 482/1018 [05:31<06:09,  1.45it/s] 47%|████▋     | 483/1018 [05:32<06:07,  1.46it/s] 48%|████▊     | 484/1018 [05:33<06:06,  1.46it/s] 48%|████▊     | 485/1018 [05:34<06:07,  1.45it/s] 48%|████▊     | 486/1018 [05:34<06:06,  1.45it/s] 48%|████▊     | 487/1018 [05:35<06:04,  1.46it/s] 48%|████▊     | 488/1018 [05:36<06:07,  1.44it/s] 48%|████▊     | 489/1018 [05:36<06:04,  1.45it/s] 48%|████▊     | 490/1018 [05:37<06:02,  1.46it/s] 48%|████▊     | 491/1018 [05:38<06:05,  1.44it/s] 48%|████▊     | 492/1018 [05:38<06:03,  1.45it/s] 48%|████▊     | 493/1018 [05:39<06:02,  1.45it/s] 49%|████▊     | 494/1018 [05:40<06:00,  1.45it/s] 49%|████▊     | 495/1018 [05:40<05:59,  1.46it/s] 49%|████▊     | 496/1018 [05:41<06:01,  1.45it/s] 49%|████▉     | 497/1018 [05:42<05:59,  1.45it/s] 49%|████▉     | 498/1018 [05:42<05:57,  1.45it/s] 49%|████▉     | 499/1018 [05:43<05:58,  1.45it/s] 49%|████▉     | 500/1018 [05:44<05:57,  1.45it/s]                                                   49%|████▉     | 500/1018 [05:44<05:57,  1.45it/s] 49%|████▉     | 501/1018 [05:45<05:58,  1.44it/s] 49%|████▉     | 502/1018 [05:45<05:56,  1.45it/s] 49%|████▉     | 503/1018 [05:46<05:54,  1.45it/s] 50%|████▉     | 504/1018 [05:47<05:56,  1.44it/s] 50%|████▉     | 505/1018 [05:47<05:54,  1.45it/s] 50%|████▉     | 506/1018 [05:48<05:53,  1.45it/s] 50%|████▉     | 507/1018 [05:49<05:58,  1.42it/s] 50%|████▉     | 508/1018 [05:49<05:55,  1.43it/s] 50%|█████     | 509/1018 [05:50<05:21,  1.59it/s][INFO|trainer.py:2340] 2023-11-01 17:51:48,704 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-509
[INFO|configuration_utils.py:446] 2023-11-01 17:51:48,715 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-509/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:51:50,556 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-509/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:51:50,563 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-509/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:51:50,568 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-509/special_tokens_map.json
 50%|█████     | 510/1018 [05:56<20:01,  2.37s/it] 50%|█████     | 511/1018 [05:57<15:41,  1.86s/it] 50%|█████     | 512/1018 [05:58<12:39,  1.50s/it] 50%|█████     | 513/1018 [05:58<10:31,  1.25s/it] 50%|█████     | 514/1018 [05:59<09:02,  1.08s/it] 51%|█████     | 515/1018 [06:00<08:00,  1.05it/s] 51%|█████     | 516/1018 [06:00<07:16,  1.15it/s] 51%|█████     | 517/1018 [06:01<06:45,  1.23it/s] 51%|█████     | 518/1018 [06:02<06:24,  1.30it/s] 51%|█████     | 519/1018 [06:02<06:09,  1.35it/s] 51%|█████     | 520/1018 [06:03<05:58,  1.39it/s] 51%|█████     | 521/1018 [06:04<05:50,  1.42it/s] 51%|█████▏    | 522/1018 [06:04<05:45,  1.44it/s] 51%|█████▏    | 523/1018 [06:05<05:40,  1.45it/s] 51%|█████▏    | 524/1018 [06:06<05:37,  1.46it/s] 52%|█████▏    | 525/1018 [06:06<05:35,  1.47it/s] 52%|█████▏    | 526/1018 [06:07<05:33,  1.47it/s] 52%|█████▏    | 527/1018 [06:08<05:32,  1.48it/s] 52%|█████▏    | 528/1018 [06:08<05:31,  1.48it/s] 52%|█████▏    | 529/1018 [06:09<05:30,  1.48it/s] 52%|█████▏    | 530/1018 [06:10<05:29,  1.48it/s] 52%|█████▏    | 531/1018 [06:10<05:28,  1.48it/s] 52%|█████▏    | 532/1018 [06:11<05:28,  1.48it/s] 52%|█████▏    | 533/1018 [06:12<05:27,  1.48it/s] 52%|█████▏    | 534/1018 [06:13<05:29,  1.47it/s] 53%|█████▎    | 535/1018 [06:13<05:30,  1.46it/s] 53%|█████▎    | 536/1018 [06:14<05:28,  1.47it/s] 53%|█████▎    | 537/1018 [06:15<05:26,  1.47it/s] 53%|█████▎    | 538/1018 [06:15<05:25,  1.47it/s] 53%|█████▎    | 539/1018 [06:16<05:24,  1.47it/s] 53%|█████▎    | 540/1018 [06:17<05:26,  1.47it/s] 53%|█████▎    | 541/1018 [06:17<05:24,  1.47it/s] 53%|█████▎    | 542/1018 [06:18<05:24,  1.47it/s] 53%|█████▎    | 543/1018 [06:19<05:23,  1.47it/s] 53%|█████▎    | 544/1018 [06:19<05:21,  1.47it/s] 54%|█████▎    | 545/1018 [06:20<05:21,  1.47it/s] 54%|█████▎    | 546/1018 [06:21<05:27,  1.44it/s] 54%|█████▎    | 547/1018 [06:21<05:32,  1.42it/s] 54%|█████▍    | 548/1018 [06:22<05:31,  1.42it/s] 54%|█████▍    | 549/1018 [06:23<05:28,  1.43it/s] 54%|█████▍    | 550/1018 [06:24<05:25,  1.44it/s] 54%|█████▍    | 551/1018 [06:24<05:21,  1.45it/s] 54%|█████▍    | 552/1018 [06:25<05:18,  1.46it/s] 54%|█████▍    | 553/1018 [06:26<05:17,  1.46it/s] 54%|█████▍    | 554/1018 [06:26<05:17,  1.46it/s] 55%|█████▍    | 555/1018 [06:27<05:16,  1.46it/s] 55%|█████▍    | 556/1018 [06:28<05:15,  1.47it/s] 55%|█████▍    | 557/1018 [06:28<05:15,  1.46it/s] 55%|█████▍    | 558/1018 [06:29<05:15,  1.46it/s] 55%|█████▍    | 559/1018 [06:30<05:14,  1.46it/s] 55%|█████▌    | 560/1018 [06:30<05:13,  1.46it/s] 55%|█████▌    | 561/1018 [06:31<05:11,  1.47it/s] 55%|█████▌    | 562/1018 [06:32<05:11,  1.46it/s] 55%|█████▌    | 563/1018 [06:32<05:11,  1.46it/s] 55%|█████▌    | 564/1018 [06:33<05:10,  1.46it/s] 56%|█████▌    | 565/1018 [06:34<05:09,  1.46it/s] 56%|█████▌    | 566/1018 [06:34<05:09,  1.46it/s] 56%|█████▌    | 567/1018 [06:35<05:09,  1.46it/s] 56%|█████▌    | 568/1018 [06:36<05:07,  1.46it/s] 56%|█████▌    | 569/1018 [06:36<05:07,  1.46it/s] 56%|█████▌    | 570/1018 [06:37<05:07,  1.46it/s] 56%|█████▌    | 571/1018 [06:38<05:06,  1.46it/s] 56%|█████▌    | 572/1018 [06:39<05:05,  1.46it/s] 56%|█████▋    | 573/1018 [06:39<05:04,  1.46it/s] 56%|█████▋    | 574/1018 [06:40<05:03,  1.46it/s] 56%|█████▋    | 575/1018 [06:41<05:02,  1.46it/s] 57%|█████▋    | 576/1018 [06:41<05:02,  1.46it/s] 57%|█████▋    | 577/1018 [06:42<05:01,  1.46it/s] 57%|█████▋    | 578/1018 [06:43<05:01,  1.46it/s] 57%|█████▋    | 579/1018 [06:43<05:00,  1.46it/s] 57%|█████▋    | 580/1018 [06:44<05:00,  1.46it/s] 57%|█████▋    | 581/1018 [06:45<04:59,  1.46it/s] 57%|█████▋    | 582/1018 [06:45<04:58,  1.46it/s] 57%|█████▋    | 583/1018 [06:46<05:01,  1.44it/s] 57%|█████▋    | 584/1018 [06:47<05:00,  1.45it/s] 57%|█████▋    | 585/1018 [06:47<04:58,  1.45it/s] 58%|█████▊    | 586/1018 [06:48<04:56,  1.46it/s] 58%|█████▊    | 587/1018 [06:49<04:56,  1.45it/s] 58%|█████▊    | 588/1018 [06:50<04:55,  1.46it/s] 58%|█████▊    | 589/1018 [06:50<04:53,  1.46it/s] 58%|█████▊    | 590/1018 [06:51<04:53,  1.46it/s] 58%|█████▊    | 591/1018 [06:52<04:52,  1.46it/s] 58%|█████▊    | 592/1018 [06:52<04:52,  1.46it/s] 58%|█████▊    | 593/1018 [06:53<04:51,  1.46it/s] 58%|█████▊    | 594/1018 [06:54<04:52,  1.45it/s] 58%|█████▊    | 595/1018 [06:54<04:51,  1.45it/s] 59%|█████▊    | 596/1018 [06:55<04:50,  1.45it/s] 59%|█████▊    | 597/1018 [06:56<04:51,  1.45it/s] 59%|█████▊    | 598/1018 [06:56<04:49,  1.45it/s] 59%|█████▉    | 599/1018 [06:57<04:50,  1.44it/s] 59%|█████▉    | 600/1018 [06:58<04:49,  1.45it/s] 59%|█████▉    | 601/1018 [06:58<04:47,  1.45it/s] 59%|█████▉    | 602/1018 [06:59<04:50,  1.43it/s] 59%|█████▉    | 603/1018 [07:00<04:48,  1.44it/s] 59%|█████▉    | 604/1018 [07:01<04:46,  1.44it/s] 59%|█████▉    | 605/1018 [07:01<04:45,  1.45it/s] 60%|█████▉    | 606/1018 [07:02<04:45,  1.44it/s] 60%|█████▉    | 607/1018 [07:03<04:43,  1.45it/s] 60%|█████▉    | 608/1018 [07:03<04:41,  1.45it/s] 60%|█████▉    | 609/1018 [07:04<04:40,  1.46it/s] 60%|█████▉    | 610/1018 [07:05<04:42,  1.44it/s] 60%|██████    | 611/1018 [07:05<04:41,  1.45it/s] 60%|██████    | 612/1018 [07:06<04:39,  1.45it/s] 60%|██████    | 613/1018 [07:07<04:38,  1.46it/s] 60%|██████    | 614/1018 [07:07<04:37,  1.46it/s] 60%|██████    | 615/1018 [07:08<04:36,  1.46it/s] 61%|██████    | 616/1018 [07:09<04:35,  1.46it/s] 61%|██████    | 617/1018 [07:10<04:34,  1.46it/s] 61%|██████    | 618/1018 [07:10<04:33,  1.46it/s] 61%|██████    | 619/1018 [07:11<04:33,  1.46it/s] 61%|██████    | 620/1018 [07:12<04:32,  1.46it/s] 61%|██████    | 621/1018 [07:12<04:33,  1.45it/s] 61%|██████    | 622/1018 [07:13<04:32,  1.45it/s] 61%|██████    | 623/1018 [07:14<04:31,  1.45it/s] 61%|██████▏   | 624/1018 [07:14<04:33,  1.44it/s] 61%|██████▏   | 625/1018 [07:15<04:32,  1.44it/s] 61%|██████▏   | 626/1018 [07:16<04:30,  1.45it/s] 62%|██████▏   | 627/1018 [07:16<04:28,  1.45it/s] 62%|██████▏   | 628/1018 [07:17<04:27,  1.46it/s] 62%|██████▏   | 629/1018 [07:18<04:26,  1.46it/s] 62%|██████▏   | 630/1018 [07:18<04:25,  1.46it/s] 62%|██████▏   | 631/1018 [07:19<04:25,  1.46it/s] 62%|██████▏   | 632/1018 [07:20<04:25,  1.46it/s] 62%|██████▏   | 633/1018 [07:21<04:24,  1.46it/s] 62%|██████▏   | 634/1018 [07:21<04:24,  1.45it/s] 62%|██████▏   | 635/1018 [07:22<04:23,  1.46it/s] 62%|██████▏   | 636/1018 [07:23<04:22,  1.46it/s] 63%|██████▎   | 637/1018 [07:23<04:21,  1.46it/s] 63%|██████▎   | 638/1018 [07:24<04:22,  1.44it/s] 63%|██████▎   | 639/1018 [07:25<04:21,  1.45it/s] 63%|██████▎   | 640/1018 [07:25<04:20,  1.45it/s] 63%|██████▎   | 641/1018 [07:26<04:19,  1.45it/s] 63%|██████▎   | 642/1018 [07:27<04:18,  1.45it/s] 63%|██████▎   | 643/1018 [07:27<04:21,  1.44it/s] 63%|██████▎   | 644/1018 [07:28<04:19,  1.44it/s] 63%|██████▎   | 645/1018 [07:29<04:17,  1.45it/s] 63%|██████▎   | 646/1018 [07:30<04:19,  1.43it/s] 64%|██████▎   | 647/1018 [07:30<04:17,  1.44it/s] 64%|██████▎   | 648/1018 [07:31<04:15,  1.45it/s] 64%|██████▍   | 649/1018 [07:32<04:14,  1.45it/s] 64%|██████▍   | 650/1018 [07:32<04:13,  1.45it/s] 64%|██████▍   | 651/1018 [07:33<04:11,  1.46it/s] 64%|██████▍   | 652/1018 [07:34<04:11,  1.46it/s] 64%|██████▍   | 653/1018 [07:34<04:10,  1.46it/s] 64%|██████▍   | 654/1018 [07:35<04:13,  1.44it/s] 64%|██████▍   | 655/1018 [07:36<04:12,  1.44it/s] 64%|██████▍   | 656/1018 [07:36<04:10,  1.45it/s] 65%|██████▍   | 657/1018 [07:37<04:09,  1.45it/s] 65%|██████▍   | 658/1018 [07:38<04:07,  1.45it/s] 65%|██████▍   | 659/1018 [07:38<04:08,  1.45it/s] 65%|██████▍   | 660/1018 [07:39<04:06,  1.45it/s] 65%|██████▍   | 661/1018 [07:40<04:05,  1.45it/s] 65%|██████▌   | 662/1018 [07:41<04:06,  1.45it/s] 65%|██████▌   | 663/1018 [07:41<04:04,  1.45it/s] 65%|██████▌   | 664/1018 [07:42<04:03,  1.45it/s] 65%|██████▌   | 665/1018 [07:43<04:05,  1.44it/s] 65%|██████▌   | 666/1018 [07:43<04:03,  1.44it/s] 66%|██████▌   | 667/1018 [07:44<04:02,  1.45it/s] 66%|██████▌   | 668/1018 [07:45<04:00,  1.45it/s] 66%|██████▌   | 669/1018 [07:45<03:59,  1.46it/s] 66%|██████▌   | 670/1018 [07:46<04:00,  1.45it/s] 66%|██████▌   | 671/1018 [07:47<03:59,  1.45it/s] 66%|██████▌   | 672/1018 [07:47<03:58,  1.45it/s] 66%|██████▌   | 673/1018 [07:48<03:59,  1.44it/s] 66%|██████▌   | 674/1018 [07:49<03:57,  1.45it/s] 66%|██████▋   | 675/1018 [07:50<03:56,  1.45it/s] 66%|██████▋   | 676/1018 [07:50<03:55,  1.46it/s] 67%|██████▋   | 677/1018 [07:51<03:54,  1.46it/s] 67%|██████▋   | 678/1018 [07:52<03:54,  1.45it/s] 67%|██████▋   | 679/1018 [07:52<03:55,  1.44it/s] 67%|██████▋   | 680/1018 [07:53<03:54,  1.44it/s] 67%|██████▋   | 681/1018 [07:54<03:57,  1.42it/s] 67%|██████▋   | 682/1018 [07:54<03:55,  1.43it/s] 67%|██████▋   | 683/1018 [07:55<03:52,  1.44it/s] 67%|██████▋   | 684/1018 [07:56<03:51,  1.44it/s] 67%|██████▋   | 685/1018 [07:56<03:50,  1.44it/s] 67%|██████▋   | 686/1018 [07:57<03:49,  1.45it/s] 67%|██████▋   | 687/1018 [07:58<03:47,  1.45it/s] 68%|██████▊   | 688/1018 [07:59<03:46,  1.46it/s] 68%|██████▊   | 689/1018 [07:59<03:47,  1.45it/s] 68%|██████▊   | 690/1018 [08:00<03:46,  1.45it/s] 68%|██████▊   | 691/1018 [08:01<03:44,  1.45it/s] 68%|██████▊   | 692/1018 [08:01<03:44,  1.46it/s] 68%|██████▊   | 693/1018 [08:02<03:43,  1.45it/s] 68%|██████▊   | 694/1018 [08:03<03:45,  1.44it/s] 68%|██████▊   | 695/1018 [08:03<03:43,  1.45it/s] 68%|██████▊   | 696/1018 [08:04<03:42,  1.45it/s] 68%|██████▊   | 697/1018 [08:05<03:43,  1.44it/s] 69%|██████▊   | 698/1018 [08:05<03:42,  1.44it/s] 69%|██████▊   | 699/1018 [08:06<03:40,  1.45it/s] 69%|██████▉   | 700/1018 [08:07<03:39,  1.45it/s] 69%|██████▉   | 701/1018 [08:07<03:38,  1.45it/s] 69%|██████▉   | 702/1018 [08:08<03:38,  1.45it/s] 69%|██████▉   | 703/1018 [08:09<03:37,  1.45it/s] 69%|██████▉   | 704/1018 [08:10<03:36,  1.45it/s] 69%|██████▉   | 705/1018 [08:10<03:38,  1.43it/s] 69%|██████▉   | 706/1018 [08:11<03:36,  1.44it/s] 69%|██████▉   | 707/1018 [08:12<03:34,  1.45it/s] 70%|██████▉   | 708/1018 [08:12<03:33,  1.45it/s] 70%|██████▉   | 709/1018 [08:13<03:32,  1.46it/s] 70%|██████▉   | 710/1018 [08:14<03:34,  1.44it/s] 70%|██████▉   | 711/1018 [08:14<03:33,  1.44it/s] 70%|██████▉   | 712/1018 [08:15<03:31,  1.44it/s] 70%|███████   | 713/1018 [08:16<03:30,  1.45it/s] 70%|███████   | 714/1018 [08:16<03:29,  1.45it/s] 70%|███████   | 715/1018 [08:17<03:28,  1.45it/s] 70%|███████   | 716/1018 [08:18<03:27,  1.45it/s] 70%|███████   | 717/1018 [08:19<03:26,  1.46it/s] 71%|███████   | 718/1018 [08:19<03:25,  1.46it/s] 71%|███████   | 719/1018 [08:20<03:25,  1.46it/s] 71%|███████   | 720/1018 [08:21<03:24,  1.46it/s] 71%|███████   | 721/1018 [08:21<03:26,  1.44it/s] 71%|███████   | 722/1018 [08:22<03:25,  1.44it/s] 71%|███████   | 723/1018 [08:23<03:24,  1.45it/s] 71%|███████   | 724/1018 [08:23<03:23,  1.45it/s] 71%|███████   | 725/1018 [08:24<03:22,  1.45it/s] 71%|███████▏  | 726/1018 [08:25<03:21,  1.45it/s] 71%|███████▏  | 727/1018 [08:25<03:20,  1.45it/s] 72%|███████▏  | 728/1018 [08:26<03:19,  1.46it/s] 72%|███████▏  | 729/1018 [08:27<03:20,  1.44it/s] 72%|███████▏  | 730/1018 [08:28<03:18,  1.45it/s] 72%|███████▏  | 731/1018 [08:28<03:17,  1.45it/s] 72%|███████▏  | 732/1018 [08:29<03:20,  1.43it/s] 72%|███████▏  | 733/1018 [08:30<03:23,  1.40it/s] 72%|███████▏  | 734/1018 [08:30<03:22,  1.40it/s] 72%|███████▏  | 735/1018 [08:31<03:19,  1.42it/s] 72%|███████▏  | 736/1018 [08:32<03:16,  1.44it/s] 72%|███████▏  | 737/1018 [08:32<03:14,  1.44it/s] 72%|███████▏  | 738/1018 [08:33<03:13,  1.45it/s] 73%|███████▎  | 739/1018 [08:34<03:11,  1.45it/s] 73%|███████▎  | 740/1018 [08:34<03:10,  1.46it/s] 73%|███████▎  | 741/1018 [08:35<03:09,  1.46it/s] 73%|███████▎  | 742/1018 [08:36<03:10,  1.45it/s] 73%|███████▎  | 743/1018 [08:37<03:09,  1.45it/s] 73%|███████▎  | 744/1018 [08:37<03:08,  1.45it/s] 73%|███████▎  | 745/1018 [08:38<03:07,  1.45it/s] 73%|███████▎  | 746/1018 [08:39<03:06,  1.45it/s] 73%|███████▎  | 747/1018 [08:39<03:06,  1.46it/s] 73%|███████▎  | 748/1018 [08:40<03:05,  1.46it/s] 74%|███████▎  | 749/1018 [08:41<03:04,  1.46it/s] 74%|███████▎  | 750/1018 [08:41<03:03,  1.46it/s] 74%|███████▍  | 751/1018 [08:42<03:02,  1.46it/s] 74%|███████▍  | 752/1018 [08:43<03:02,  1.45it/s] 74%|███████▍  | 753/1018 [08:43<03:02,  1.46it/s] 74%|███████▍  | 754/1018 [08:44<03:01,  1.46it/s] 74%|███████▍  | 755/1018 [08:45<03:00,  1.46it/s] 74%|███████▍  | 756/1018 [08:46<03:01,  1.44it/s] 74%|███████▍  | 757/1018 [08:46<03:00,  1.45it/s] 74%|███████▍  | 758/1018 [08:47<02:59,  1.45it/s] 75%|███████▍  | 759/1018 [08:48<02:58,  1.45it/s] 75%|███████▍  | 760/1018 [08:48<02:56,  1.46it/s] 75%|███████▍  | 761/1018 [08:49<02:56,  1.46it/s] 75%|███████▍  | 762/1018 [08:50<02:55,  1.46it/s] 75%|███████▍  | 763/1018 [08:50<02:55,  1.46it/s] 75%|███████▌  | 764/1018 [08:51<02:54,  1.46it/s] 75%|███████▌  | 765/1018 [08:52<02:53,  1.46it/s] 75%|███████▌  | 766/1018 [08:52<02:52,  1.46it/s] 75%|███████▌  | 767/1018 [08:53<02:53,  1.44it/s] 75%|███████▌  | 768/1018 [08:54<02:52,  1.45it/s] 76%|███████▌  | 769/1018 [08:54<02:51,  1.45it/s] 76%|███████▌  | 770/1018 [08:55<02:50,  1.45it/s] 76%|███████▌  | 771/1018 [08:56<02:49,  1.45it/s] 76%|███████▌  | 772/1018 [08:56<02:49,  1.45it/s] 76%|███████▌  | 773/1018 [08:57<02:48,  1.45it/s] 76%|███████▌  | 774/1018 [08:58<02:47,  1.46it/s] 76%|███████▌  | 775/1018 [08:59<02:46,  1.46it/s] 76%|███████▌  | 776/1018 [08:59<02:45,  1.46it/s] 76%|███████▋  | 777/1018 [09:00<02:45,  1.46it/s] 76%|███████▋  | 778/1018 [09:01<02:45,  1.45it/s] 77%|███████▋  | 779/1018 [09:01<02:45,  1.44it/s] 77%|███████▋  | 780/1018 [09:02<02:45,  1.44it/s] 77%|███████▋  | 781/1018 [09:03<02:44,  1.44it/s] 77%|███████▋  | 782/1018 [09:03<02:43,  1.44it/s] 77%|███████▋  | 783/1018 [09:04<02:42,  1.45it/s] 77%|███████▋  | 784/1018 [09:05<02:41,  1.45it/s] 77%|███████▋  | 785/1018 [09:05<02:40,  1.45it/s] 77%|███████▋  | 786/1018 [09:06<02:39,  1.45it/s] 77%|███████▋  | 787/1018 [09:07<02:38,  1.45it/s] 77%|███████▋  | 788/1018 [09:08<02:37,  1.46it/s] 78%|███████▊  | 789/1018 [09:08<02:38,  1.44it/s] 78%|███████▊  | 790/1018 [09:09<02:37,  1.45it/s] 78%|███████▊  | 791/1018 [09:10<02:36,  1.45it/s] 78%|███████▊  | 792/1018 [09:10<02:35,  1.45it/s] 78%|███████▊  | 793/1018 [09:11<02:34,  1.45it/s] 78%|███████▊  | 794/1018 [09:12<02:33,  1.46it/s] 78%|███████▊  | 795/1018 [09:12<02:33,  1.46it/s] 78%|███████▊  | 796/1018 [09:13<02:32,  1.46it/s] 78%|███████▊  | 797/1018 [09:14<02:31,  1.46it/s] 78%|███████▊  | 798/1018 [09:14<02:30,  1.46it/s] 78%|███████▊  | 799/1018 [09:15<02:29,  1.46it/s] 79%|███████▊  | 800/1018 [09:16<02:30,  1.45it/s] 79%|███████▊  | 801/1018 [09:16<02:29,  1.45it/s] 79%|███████▉  | 802/1018 [09:17<02:28,  1.45it/s] 79%|███████▉  | 803/1018 [09:18<02:28,  1.44it/s] 79%|███████▉  | 804/1018 [09:19<02:27,  1.45it/s] 79%|███████▉  | 805/1018 [09:19<02:26,  1.45it/s] 79%|███████▉  | 806/1018 [09:20<02:26,  1.45it/s] 79%|███████▉  | 807/1018 [09:21<02:25,  1.45it/s] 79%|███████▉  | 808/1018 [09:21<02:24,  1.45it/s] 79%|███████▉  | 809/1018 [09:22<02:23,  1.45it/s] 80%|███████▉  | 810/1018 [09:23<02:23,  1.45it/s] 80%|███████▉  | 811/1018 [09:23<02:23,  1.44it/s] 80%|███████▉  | 812/1018 [09:24<02:22,  1.45it/s] 80%|███████▉  | 813/1018 [09:25<02:21,  1.45it/s] 80%|███████▉  | 814/1018 [09:25<02:20,  1.45it/s] 80%|████████  | 815/1018 [09:26<02:20,  1.45it/s] 80%|████████  | 816/1018 [09:27<02:20,  1.44it/s] 80%|████████  | 817/1018 [09:28<02:19,  1.44it/s] 80%|████████  | 818/1018 [09:28<02:18,  1.44it/s] 80%|████████  | 819/1018 [09:29<02:17,  1.45it/s] 81%|████████  | 820/1018 [09:30<02:16,  1.45it/s] 81%|████████  | 821/1018 [09:30<02:15,  1.45it/s] 81%|████████  | 822/1018 [09:31<02:14,  1.45it/s] 81%|████████  | 823/1018 [09:32<02:13,  1.46it/s] 81%|████████  | 824/1018 [09:32<02:13,  1.46it/s] 81%|████████  | 825/1018 [09:33<02:13,  1.45it/s] 81%|████████  | 826/1018 [09:34<02:12,  1.45it/s] 81%|████████  | 827/1018 [09:34<02:11,  1.45it/s] 81%|████████▏ | 828/1018 [09:35<02:10,  1.45it/s] 81%|████████▏ | 829/1018 [09:36<02:10,  1.45it/s] 82%|████████▏ | 830/1018 [09:36<02:11,  1.43it/s] 82%|████████▏ | 831/1018 [09:37<02:09,  1.44it/s] 82%|████████▏ | 832/1018 [09:38<02:08,  1.45it/s] 82%|████████▏ | 833/1018 [09:39<02:07,  1.45it/s] 82%|████████▏ | 834/1018 [09:39<02:06,  1.46it/s] 82%|████████▏ | 835/1018 [09:40<02:05,  1.46it/s] 82%|████████▏ | 836/1018 [09:41<02:04,  1.46it/s] 82%|████████▏ | 837/1018 [09:41<02:04,  1.46it/s] 82%|████████▏ | 838/1018 [09:42<02:04,  1.44it/s] 82%|████████▏ | 839/1018 [09:43<02:03,  1.45it/s] 83%|████████▎ | 840/1018 [09:43<02:02,  1.45it/s] 83%|████████▎ | 841/1018 [09:44<02:02,  1.44it/s] 83%|████████▎ | 842/1018 [09:45<02:01,  1.45it/s] 83%|████████▎ | 843/1018 [09:45<02:00,  1.45it/s] 83%|████████▎ | 844/1018 [09:46<01:59,  1.45it/s] 83%|████████▎ | 845/1018 [09:47<01:59,  1.45it/s] 83%|████████▎ | 846/1018 [09:48<01:58,  1.45it/s] 83%|████████▎ | 847/1018 [09:48<01:57,  1.46it/s] 83%|████████▎ | 848/1018 [09:49<01:56,  1.45it/s] 83%|████████▎ | 849/1018 [09:50<01:56,  1.46it/s] 83%|████████▎ | 850/1018 [09:50<01:55,  1.46it/s] 84%|████████▎ | 851/1018 [09:51<01:54,  1.46it/s] 84%|████████▎ | 852/1018 [09:52<01:54,  1.45it/s] 84%|████████▍ | 853/1018 [09:52<01:53,  1.45it/s] 84%|████████▍ | 854/1018 [09:53<01:52,  1.46it/s] 84%|████████▍ | 855/1018 [09:54<01:52,  1.45it/s] 84%|████████▍ | 856/1018 [09:54<01:51,  1.45it/s] 84%|████████▍ | 857/1018 [09:55<01:50,  1.46it/s] 84%|████████▍ | 858/1018 [09:56<01:49,  1.46it/s] 84%|████████▍ | 859/1018 [09:56<01:49,  1.46it/s] 84%|████████▍ | 860/1018 [09:57<01:48,  1.46it/s] 85%|████████▍ | 861/1018 [09:58<01:47,  1.46it/s] 85%|████████▍ | 862/1018 [09:58<01:46,  1.46it/s] 85%|████████▍ | 863/1018 [09:59<01:46,  1.46it/s] 85%|████████▍ | 864/1018 [10:00<01:45,  1.46it/s] 85%|████████▍ | 865/1018 [10:01<01:45,  1.46it/s] 85%|████████▌ | 866/1018 [10:01<01:44,  1.46it/s] 85%|████████▌ | 867/1018 [10:02<01:43,  1.46it/s] 85%|████████▌ | 868/1018 [10:03<01:43,  1.45it/s] 85%|████████▌ | 869/1018 [10:03<01:42,  1.45it/s] 85%|████████▌ | 870/1018 [10:04<01:42,  1.45it/s] 86%|████████▌ | 871/1018 [10:05<01:41,  1.45it/s] 86%|████████▌ | 872/1018 [10:05<01:40,  1.45it/s] 86%|████████▌ | 873/1018 [10:06<01:39,  1.46it/s] 86%|████████▌ | 874/1018 [10:07<01:38,  1.46it/s] 86%|████████▌ | 875/1018 [10:07<01:38,  1.46it/s] 86%|████████▌ | 876/1018 [10:08<01:37,  1.46it/s] 86%|████████▌ | 877/1018 [10:09<01:37,  1.44it/s] 86%|████████▌ | 878/1018 [10:10<01:36,  1.45it/s] 86%|████████▋ | 879/1018 [10:10<01:35,  1.45it/s] 86%|████████▋ | 880/1018 [10:11<01:34,  1.46it/s] 87%|████████▋ | 881/1018 [10:12<01:34,  1.46it/s] 87%|████████▋ | 882/1018 [10:12<01:33,  1.46it/s] 87%|████████▋ | 883/1018 [10:13<01:33,  1.44it/s] 87%|████████▋ | 884/1018 [10:14<01:32,  1.45it/s] 87%|████████▋ | 885/1018 [10:14<01:31,  1.45it/s] 87%|████████▋ | 886/1018 [10:15<01:30,  1.45it/s] 87%|████████▋ | 887/1018 [10:16<01:30,  1.45it/s] 87%|████████▋ | 888/1018 [10:16<01:29,  1.46it/s] 87%|████████▋ | 889/1018 [10:17<01:28,  1.46it/s] 87%|████████▋ | 890/1018 [10:18<01:27,  1.46it/s] 88%|████████▊ | 891/1018 [10:18<01:27,  1.46it/s] 88%|████████▊ | 892/1018 [10:19<01:26,  1.46it/s] 88%|████████▊ | 893/1018 [10:20<01:25,  1.46it/s] 88%|████████▊ | 894/1018 [10:21<01:25,  1.45it/s] 88%|████████▊ | 895/1018 [10:21<01:24,  1.45it/s] 88%|████████▊ | 896/1018 [10:22<01:23,  1.45it/s] 88%|████████▊ | 897/1018 [10:23<01:24,  1.44it/s] 88%|████████▊ | 898/1018 [10:23<01:23,  1.44it/s] 88%|████████▊ | 899/1018 [10:24<01:22,  1.45it/s] 88%|████████▊ | 900/1018 [10:25<01:21,  1.45it/s] 89%|████████▊ | 901/1018 [10:25<01:20,  1.45it/s] 89%|████████▊ | 902/1018 [10:26<01:19,  1.45it/s] 89%|████████▊ | 903/1018 [10:27<01:19,  1.45it/s] 89%|████████▉ | 904/1018 [10:27<01:18,  1.45it/s] 89%|████████▉ | 905/1018 [10:28<01:17,  1.46it/s] 89%|████████▉ | 906/1018 [10:29<01:16,  1.46it/s] 89%|████████▉ | 907/1018 [10:29<01:16,  1.46it/s] 89%|████████▉ | 908/1018 [10:30<01:16,  1.45it/s] 89%|████████▉ | 909/1018 [10:31<01:15,  1.45it/s] 89%|████████▉ | 910/1018 [10:32<01:14,  1.46it/s] 89%|████████▉ | 911/1018 [10:32<01:13,  1.46it/s] 90%|████████▉ | 912/1018 [10:33<01:12,  1.46it/s] 90%|████████▉ | 913/1018 [10:34<01:11,  1.46it/s] 90%|████████▉ | 914/1018 [10:34<01:11,  1.46it/s] 90%|████████▉ | 915/1018 [10:35<01:10,  1.46it/s] 90%|████████▉ | 916/1018 [10:36<01:09,  1.46it/s] 90%|█████████ | 917/1018 [10:36<01:09,  1.46it/s] 90%|█████████ | 918/1018 [10:37<01:08,  1.46it/s] 90%|█████████ | 919/1018 [10:38<01:08,  1.44it/s] 90%|█████████ | 920/1018 [10:38<01:07,  1.45it/s] 90%|█████████ | 921/1018 [10:39<01:06,  1.45it/s] 91%|█████████ | 922/1018 [10:40<01:06,  1.45it/s] 91%|█████████ | 923/1018 [10:40<01:05,  1.46it/s] 91%|█████████ | 924/1018 [10:41<01:04,  1.46it/s] 91%|█████████ | 925/1018 [10:42<01:04,  1.45it/s] 91%|█████████ | 926/1018 [10:43<01:03,  1.45it/s] 91%|█████████ | 927/1018 [10:43<01:02,  1.45it/s] 91%|█████████ | 928/1018 [10:44<01:01,  1.46it/s] 91%|█████████▏| 929/1018 [10:45<01:01,  1.46it/s] 91%|█████████▏| 930/1018 [10:45<01:00,  1.46it/s] 91%|█████████▏| 931/1018 [10:46<00:59,  1.46it/s] 92%|█████████▏| 932/1018 [10:47<00:59,  1.45it/s] 92%|█████████▏| 933/1018 [10:47<00:58,  1.46it/s] 92%|█████████▏| 934/1018 [10:48<00:57,  1.46it/s] 92%|█████████▏| 935/1018 [10:49<00:56,  1.46it/s] 92%|█████████▏| 936/1018 [10:49<00:56,  1.46it/s] 92%|█████████▏| 937/1018 [10:50<00:55,  1.46it/s] 92%|█████████▏| 938/1018 [10:51<00:54,  1.46it/s] 92%|█████████▏| 939/1018 [10:51<00:54,  1.44it/s] 92%|█████████▏| 940/1018 [10:52<00:53,  1.45it/s] 92%|█████████▏| 941/1018 [10:53<00:53,  1.45it/s] 93%|█████████▎| 942/1018 [10:54<00:52,  1.46it/s] 93%|█████████▎| 943/1018 [10:54<00:51,  1.45it/s] 93%|█████████▎| 944/1018 [10:55<00:50,  1.46it/s] 93%|█████████▎| 945/1018 [10:56<00:50,  1.46it/s] 93%|█████████▎| 946/1018 [10:56<00:49,  1.46it/s] 93%|█████████▎| 947/1018 [10:57<00:49,  1.45it/s] 93%|█████████▎| 948/1018 [10:58<00:48,  1.45it/s] 93%|█████████▎| 949/1018 [10:58<00:47,  1.45it/s] 93%|█████████▎| 950/1018 [10:59<00:47,  1.43it/s] 93%|█████████▎| 951/1018 [11:00<00:47,  1.40it/s] 94%|█████████▎| 952/1018 [11:01<00:46,  1.41it/s] 94%|█████████▎| 953/1018 [11:01<00:45,  1.42it/s] 94%|█████████▎| 954/1018 [11:02<00:44,  1.43it/s] 94%|█████████▍| 955/1018 [11:03<00:43,  1.45it/s] 94%|█████████▍| 956/1018 [11:03<00:42,  1.45it/s] 94%|█████████▍| 957/1018 [11:04<00:41,  1.46it/s] 94%|█████████▍| 958/1018 [11:05<00:41,  1.46it/s] 94%|█████████▍| 959/1018 [11:05<00:40,  1.46it/s] 94%|█████████▍| 960/1018 [11:06<00:39,  1.46it/s] 94%|█████████▍| 961/1018 [11:07<00:39,  1.46it/s] 94%|█████████▍| 962/1018 [11:07<00:38,  1.46it/s] 95%|█████████▍| 963/1018 [11:08<00:37,  1.45it/s] 95%|█████████▍| 964/1018 [11:09<00:37,  1.46it/s] 95%|█████████▍| 965/1018 [11:09<00:36,  1.46it/s] 95%|█████████▍| 966/1018 [11:10<00:35,  1.45it/s] 95%|█████████▍| 967/1018 [11:11<00:35,  1.45it/s] 95%|█████████▌| 968/1018 [11:11<00:34,  1.46it/s] 95%|█████████▌| 969/1018 [11:12<00:33,  1.46it/s] 95%|█████████▌| 970/1018 [11:13<00:32,  1.46it/s] 95%|█████████▌| 971/1018 [11:14<00:32,  1.46it/s] 95%|█████████▌| 972/1018 [11:14<00:31,  1.46it/s] 96%|█████████▌| 973/1018 [11:15<00:30,  1.46it/s] 96%|█████████▌| 974/1018 [11:16<00:30,  1.46it/s] 96%|█████████▌| 975/1018 [11:16<00:29,  1.46it/s] 96%|█████████▌| 976/1018 [11:17<00:28,  1.46it/s] 96%|█████████▌| 977/1018 [11:18<00:28,  1.46it/s] 96%|█████████▌| 978/1018 [11:18<00:27,  1.46it/s] 96%|█████████▌| 979/1018 [11:19<00:26,  1.46it/s] 96%|█████████▋| 980/1018 [11:20<00:26,  1.46it/s] 96%|█████████▋| 981/1018 [11:20<00:25,  1.46it/s] 96%|█████████▋| 982/1018 [11:21<00:24,  1.46it/s] 97%|█████████▋| 983/1018 [11:22<00:24,  1.46it/s] 97%|█████████▋| 984/1018 [11:22<00:23,  1.46it/s] 97%|█████████▋| 985/1018 [11:23<00:22,  1.46it/s] 97%|█████████▋| 986/1018 [11:24<00:22,  1.45it/s] 97%|█████████▋| 987/1018 [11:25<00:21,  1.45it/s] 97%|█████████▋| 988/1018 [11:25<00:20,  1.46it/s] 97%|█████████▋| 989/1018 [11:26<00:19,  1.46it/s] 97%|█████████▋| 990/1018 [11:27<00:19,  1.46it/s] 97%|█████████▋| 991/1018 [11:27<00:18,  1.46it/s] 97%|█████████▋| 992/1018 [11:28<00:17,  1.46it/s] 98%|█████████▊| 993/1018 [11:29<00:17,  1.46it/s] 98%|█████████▊| 994/1018 [11:29<00:16,  1.46it/s] 98%|█████████▊| 995/1018 [11:30<00:15,  1.46it/s] 98%|█████████▊| 996/1018 [11:31<00:15,  1.46it/s] 98%|█████████▊| 997/1018 [11:31<00:14,  1.46it/s] 98%|█████████▊| 998/1018 [11:32<00:13,  1.46it/s] 98%|█████████▊| 999/1018 [11:33<00:13,  1.45it/s] 98%|█████████▊| 1000/1018 [11:33<00:12,  1.46it/s]                                                    98%|█████████▊| 1000/1018 [11:33<00:12,  1.46it/s] 98%|█████████▊| 1001/1018 [11:34<00:11,  1.44it/s] 98%|█████████▊| 1002/1018 [11:35<00:11,  1.44it/s] 99%|█████████▊| 1003/1018 [11:36<00:10,  1.45it/s] 99%|█████████▊| 1004/1018 [11:36<00:09,  1.45it/s] 99%|█████████▊| 1005/1018 [11:37<00:08,  1.45it/s] 99%|█████████▉| 1006/1018 [11:38<00:08,  1.45it/s] 99%|█████████▉| 1007/1018 [11:38<00:07,  1.45it/s] 99%|█████████▉| 1008/1018 [11:39<00:06,  1.46it/s] 99%|█████████▉| 1009/1018 [11:40<00:06,  1.46it/s] 99%|█████████▉| 1010/1018 [11:40<00:05,  1.46it/s] 99%|█████████▉| 1011/1018 [11:41<00:04,  1.45it/s] 99%|█████████▉| 1012/1018 [11:42<00:04,  1.45it/s]100%|█████████▉| 1013/1018 [11:42<00:03,  1.45it/s]100%|█████████▉| 1014/1018 [11:43<00:02,  1.45it/s]100%|█████████▉| 1015/1018 [11:44<00:02,  1.46it/s]100%|█████████▉| 1016/1018 [11:44<00:01,  1.46it/s]100%|█████████▉| 1017/1018 [11:45<00:00,  1.46it/s]100%|██████████| 1018/1018 [11:46<00:00,  1.61it/s][INFO|trainer.py:2340] 2023-11-01 17:57:44,375 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-1018
[INFO|configuration_utils.py:446] 2023-11-01 17:57:44,385 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-1018/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:57:46,228 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-1018/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:57:46,234 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-1018/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:57:46,243 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/checkpoint-1018/special_tokens_map.json
[INFO|trainer.py:1662] 2023-11-01 17:57:50,103 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1018/1018 [11:51<00:00,  1.61it/s]100%|██████████| 1018/1018 [11:51<00:00,  1.43it/s]
[INFO|trainer.py:2340] 2023-11-01 17:57:50,139 >> Saving model checkpoint to chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5
[INFO|configuration_utils.py:446] 2023-11-01 17:57:50,156 >> Configuration saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:57:52,006 >> Model weights saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:57:52,013 >> tokenizer config file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:57:52,026 >> Special tokens file saved in chkpts/analysis_models/pretraining-vs-finetuning/varying-finetuning/social/d-social-roberta-base-0.5/special_tokens_map.json
{'loss': 0.5951, 'learning_rate': 2.544204322200393e-06, 'epoch': 0.98}
{'loss': 0.4694, 'learning_rate': 8.840864440078585e-08, 'epoch': 1.96}
{'train_runtime': 711.906, 'train_samples_per_second': 91.456, 'train_steps_per_second': 1.43, 'train_loss': 0.5309032288422051, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.5309
  train_runtime            = 0:11:51.90
  train_samples            =      32554
  train_samples_per_second =     91.456
  train_steps_per_second   =       1.43
11/01/2023 17:57:52 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-11-01 17:57:52,179 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-11-01 17:57:52,181 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:57:52,181 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-11-01 17:57:52,181 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-11-01 17:57:52,181 >>   Num examples = 8341
[INFO|trainer.py:2595] 2023-11-01 17:57:52,181 >>   Batch size = 64
  0%|          | 0/131 [00:00<?, ?it/s]  2%|▏         | 2/131 [00:00<00:15,  8.50it/s]  2%|▏         | 3/131 [00:00<00:21,  6.04it/s]  3%|▎         | 4/131 [00:00<00:24,  5.23it/s]  4%|▍         | 5/131 [00:00<00:25,  4.86it/s]  5%|▍         | 6/131 [00:01<00:26,  4.65it/s]  5%|▌         | 7/131 [00:01<00:27,  4.52it/s]  6%|▌         | 8/131 [00:01<00:27,  4.44it/s]  7%|▋         | 9/131 [00:01<00:27,  4.38it/s]  8%|▊         | 10/131 [00:02<00:27,  4.35it/s]  8%|▊         | 11/131 [00:02<00:27,  4.32it/s]  9%|▉         | 12/131 [00:02<00:27,  4.31it/s] 10%|▉         | 13/131 [00:02<00:27,  4.29it/s] 11%|█         | 14/131 [00:03<00:27,  4.28it/s] 11%|█▏        | 15/131 [00:03<00:27,  4.27it/s] 12%|█▏        | 16/131 [00:03<00:26,  4.27it/s] 13%|█▎        | 17/131 [00:03<00:26,  4.26it/s] 14%|█▎        | 18/131 [00:03<00:26,  4.27it/s] 15%|█▍        | 19/131 [00:04<00:26,  4.27it/s] 15%|█▌        | 20/131 [00:04<00:26,  4.26it/s] 16%|█▌        | 21/131 [00:04<00:25,  4.27it/s] 17%|█▋        | 22/131 [00:04<00:25,  4.26it/s] 18%|█▊        | 23/131 [00:05<00:25,  4.26it/s] 18%|█▊        | 24/131 [00:05<00:25,  4.27it/s] 19%|█▉        | 25/131 [00:05<00:24,  4.26it/s] 20%|█▉        | 26/131 [00:05<00:24,  4.26it/s] 21%|██        | 27/131 [00:06<00:24,  4.26it/s] 21%|██▏       | 28/131 [00:06<00:24,  4.26it/s] 22%|██▏       | 29/131 [00:06<00:23,  4.26it/s] 23%|██▎       | 30/131 [00:06<00:23,  4.26it/s] 24%|██▎       | 31/131 [00:07<00:23,  4.26it/s] 24%|██▍       | 32/131 [00:07<00:23,  4.25it/s] 25%|██▌       | 33/131 [00:07<00:23,  4.26it/s] 26%|██▌       | 34/131 [00:07<00:22,  4.26it/s] 27%|██▋       | 35/131 [00:07<00:22,  4.26it/s] 27%|██▋       | 36/131 [00:08<00:22,  4.26it/s] 28%|██▊       | 37/131 [00:08<00:22,  4.26it/s] 29%|██▉       | 38/131 [00:08<00:21,  4.26it/s] 30%|██▉       | 39/131 [00:08<00:21,  4.26it/s] 31%|███       | 40/131 [00:09<00:21,  4.26it/s] 31%|███▏      | 41/131 [00:09<00:21,  4.26it/s] 32%|███▏      | 42/131 [00:09<00:20,  4.25it/s] 33%|███▎      | 43/131 [00:09<00:20,  4.25it/s] 34%|███▎      | 44/131 [00:10<00:20,  4.25it/s] 34%|███▍      | 45/131 [00:10<00:20,  4.25it/s] 35%|███▌      | 46/131 [00:10<00:19,  4.25it/s] 36%|███▌      | 47/131 [00:10<00:19,  4.25it/s] 37%|███▋      | 48/131 [00:11<00:19,  4.25it/s] 37%|███▋      | 49/131 [00:11<00:19,  4.25it/s] 38%|███▊      | 50/131 [00:11<00:19,  4.26it/s] 39%|███▉      | 51/131 [00:11<00:18,  4.26it/s] 40%|███▉      | 52/131 [00:11<00:18,  4.25it/s] 40%|████      | 53/131 [00:12<00:18,  4.25it/s] 41%|████      | 54/131 [00:12<00:18,  4.25it/s] 42%|████▏     | 55/131 [00:12<00:17,  4.25it/s] 43%|████▎     | 56/131 [00:12<00:17,  4.25it/s] 44%|████▎     | 57/131 [00:13<00:17,  4.25it/s] 44%|████▍     | 58/131 [00:13<00:17,  4.25it/s] 45%|████▌     | 59/131 [00:13<00:16,  4.25it/s] 46%|████▌     | 60/131 [00:13<00:16,  4.25it/s] 47%|████▋     | 61/131 [00:14<00:16,  4.24it/s] 47%|████▋     | 62/131 [00:14<00:16,  4.24it/s] 48%|████▊     | 63/131 [00:14<00:16,  4.24it/s] 49%|████▉     | 64/131 [00:14<00:15,  4.25it/s] 50%|████▉     | 65/131 [00:15<00:15,  4.24it/s] 50%|█████     | 66/131 [00:15<00:15,  4.25it/s] 51%|█████     | 67/131 [00:15<00:15,  4.25it/s] 52%|█████▏    | 68/131 [00:15<00:14,  4.24it/s] 53%|█████▎    | 69/131 [00:15<00:14,  4.25it/s] 53%|█████▎    | 70/131 [00:16<00:14,  4.25it/s] 54%|█████▍    | 71/131 [00:16<00:14,  4.26it/s] 55%|█████▍    | 72/131 [00:16<00:13,  4.25it/s] 56%|█████▌    | 73/131 [00:16<00:13,  4.25it/s] 56%|█████▋    | 74/131 [00:17<00:13,  4.25it/s] 57%|█████▋    | 75/131 [00:17<00:13,  4.24it/s] 58%|█████▊    | 76/131 [00:17<00:12,  4.24it/s] 59%|█████▉    | 77/131 [00:17<00:12,  4.24it/s] 60%|█████▉    | 78/131 [00:18<00:12,  4.24it/s] 60%|██████    | 79/131 [00:18<00:12,  4.24it/s] 61%|██████    | 80/131 [00:18<00:12,  4.24it/s] 62%|██████▏   | 81/131 [00:18<00:11,  4.24it/s] 63%|██████▎   | 82/131 [00:19<00:11,  4.25it/s] 63%|██████▎   | 83/131 [00:19<00:11,  4.24it/s] 64%|██████▍   | 84/131 [00:19<00:11,  4.24it/s] 65%|██████▍   | 85/131 [00:19<00:10,  4.25it/s] 66%|██████▌   | 86/131 [00:19<00:10,  4.25it/s] 66%|██████▋   | 87/131 [00:20<00:10,  4.24it/s] 67%|██████▋   | 88/131 [00:20<00:10,  4.24it/s] 68%|██████▊   | 89/131 [00:20<00:09,  4.24it/s] 69%|██████▊   | 90/131 [00:20<00:09,  4.24it/s] 69%|██████▉   | 91/131 [00:21<00:09,  4.24it/s] 70%|███████   | 92/131 [00:21<00:09,  4.23it/s] 71%|███████   | 93/131 [00:21<00:08,  4.23it/s] 72%|███████▏  | 94/131 [00:21<00:08,  4.23it/s] 73%|███████▎  | 95/131 [00:22<00:08,  4.24it/s] 73%|███████▎  | 96/131 [00:22<00:08,  4.24it/s] 74%|███████▍  | 97/131 [00:22<00:08,  4.24it/s] 75%|███████▍  | 98/131 [00:22<00:07,  4.24it/s] 76%|███████▌  | 99/131 [00:23<00:07,  4.24it/s] 76%|███████▋  | 100/131 [00:23<00:07,  4.24it/s] 77%|███████▋  | 101/131 [00:23<00:07,  4.24it/s] 78%|███████▊  | 102/131 [00:23<00:06,  4.20it/s] 79%|███████▊  | 103/131 [00:24<00:06,  4.16it/s] 79%|███████▉  | 104/131 [00:24<00:06,  4.12it/s] 80%|████████  | 105/131 [00:24<00:06,  4.13it/s] 81%|████████  | 106/131 [00:24<00:06,  4.13it/s] 82%|████████▏ | 107/131 [00:24<00:05,  4.15it/s] 82%|████████▏ | 108/131 [00:25<00:05,  4.17it/s] 83%|████████▎ | 109/131 [00:25<00:05,  4.18it/s] 84%|████████▍ | 110/131 [00:25<00:04,  4.21it/s] 85%|████████▍ | 111/131 [00:25<00:04,  4.22it/s] 85%|████████▌ | 112/131 [00:26<00:04,  4.22it/s] 86%|████████▋ | 113/131 [00:26<00:04,  4.23it/s] 87%|████████▋ | 114/131 [00:26<00:04,  4.22it/s] 88%|████████▊ | 115/131 [00:26<00:03,  4.23it/s] 89%|████████▊ | 116/131 [00:27<00:03,  4.23it/s] 89%|████████▉ | 117/131 [00:27<00:03,  4.23it/s] 90%|█████████ | 118/131 [00:27<00:03,  4.23it/s] 91%|█████████ | 119/131 [00:27<00:02,  4.22it/s] 92%|█████████▏| 120/131 [00:28<00:02,  4.23it/s] 92%|█████████▏| 121/131 [00:28<00:02,  4.23it/s] 93%|█████████▎| 122/131 [00:28<00:02,  4.23it/s] 94%|█████████▍| 123/131 [00:28<00:01,  4.23it/s] 95%|█████████▍| 124/131 [00:28<00:01,  4.22it/s] 95%|█████████▌| 125/131 [00:29<00:01,  4.23it/s] 96%|█████████▌| 126/131 [00:29<00:01,  4.22it/s] 97%|█████████▋| 127/131 [00:29<00:00,  4.22it/s] 98%|█████████▊| 128/131 [00:29<00:00,  4.23it/s] 98%|█████████▊| 129/131 [00:30<00:00,  4.23it/s] 99%|█████████▉| 130/131 [00:30<00:00,  4.23it/s][WARNING|training_args.py:1095] 2023-11-01 17:58:22,941 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 131/131 [00:30<00:00,  4.29it/s]
[WARNING|training_args.py:1095] 2023-11-01 17:58:22,990 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:58:22,991 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-11-01 17:58:23,162 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7702913284301758}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.7703
  eval_loss               =     0.4819
  eval_runtime            = 0:00:30.75
  eval_samples            =       8341
  eval_samples_per_second =    271.167
  eval_steps_per_second   =      4.259
