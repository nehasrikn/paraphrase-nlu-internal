Processing train_examples_0.01.csv
11/01/2023 17:03:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/01/2023 17:03:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/runs/Nov01_17-03-25_clip02.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/01/2023 17:03:26 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/experiments/pretraining-vs-finetuning/finetuning_data/snli/train_examples_0.01.csv
11/01/2023 17:03:26 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/snli/analysis_model_examples/dev_examples.csv
11/01/2023 17:03:26 - WARNING - datasets.builder - Using custom data configuration default-957ebd6d7280e6b1
11/01/2023 17:03:26 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/01/2023 17:03:26 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-957ebd6d7280e6b1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
11/01/2023 17:03:26 - WARNING - datasets.builder - Reusing dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-957ebd6d7280e6b1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
11/01/2023 17:03:26 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-957ebd6d7280e6b1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 557.94it/s]
[INFO|configuration_utils.py:659] 2023-11-01 17:03:26,258 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:03:26,261 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-11-01 17:03:26,303 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-11-01 17:03:26,348 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:03:26,348 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:03:26,565 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:03:26,566 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:03:26,566 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:03:26,566 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:03:26,566 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-11-01 17:03:26,608 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:03:26,609 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-11-01 17:03:26,968 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-11-01 17:03:28,194 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-11-01 17:03:28,194 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
11/01/2023 17:03:28 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f68655e51f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
11/01/2023 17:03:28 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-957ebd6d7280e6b1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
11/01/2023 17:03:29 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f68655e5040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
11/01/2023 17:03:29 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-957ebd6d7280e6b1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
hi
11/01/2023 17:03:29 - INFO - __main__ - Sample 281 of the training set: {'sentence1': 'The sunshine behind a group of trees on a long rode. This is the first time the sun has shown today.', 'sentence2': 'The ground is wet.', 'label': 1, 'input_ids': [0, 133, 15049, 639, 10, 333, 9, 3980, 15, 10, 251, 12783, 4, 152, 16, 5, 78, 86, 5, 3778, 34, 2343, 452, 4, 2, 2, 133, 1255, 16, 7727, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:03:29 - INFO - __main__ - Sample 250 of the training set: {'sentence1': 'A man walks into a store in the shopping district. A man walks into a store to buy a present.', 'sentence2': 'The man needs groceries', 'label': 0, 'input_ids': [0, 250, 313, 5792, 88, 10, 1400, 11, 5, 3482, 1418, 4, 83, 313, 5792, 88, 10, 1400, 7, 907, 10, 1455, 4, 2, 2, 133, 313, 782, 20279, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:03:29 - INFO - __main__ - Sample 228 of the training set: {'sentence1': 'Lady taking children on a hike through the woods. A family is on a camping trip.', 'sentence2': 'The lady is a school teacher.', 'label': 0, 'input_ids': [0, 35425, 602, 408, 15, 10, 5960, 149, 5, 14193, 4, 83, 284, 16, 15, 10, 16724, 1805, 4, 2, 2, 133, 6429, 16, 10, 334, 3254, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
[INFO|trainer.py:622] 2023-11-01 17:03:33,126 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-11-01 17:03:33,140 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-11-01 17:03:33,140 >>   Num examples = 788
[INFO|trainer.py:1421] 2023-11-01 17:03:33,140 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-11-01 17:03:33,140 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-11-01 17:03:33,140 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-11-01 17:03:33,140 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-11-01 17:03:33,140 >>   Total optimization steps = 26
[WARNING|training_args.py:1095] 2023-11-01 17:03:33,155 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/26 [00:00<?, ?it/s]  4%|▍         | 1/26 [00:00<00:16,  1.49it/s]  8%|▊         | 2/26 [00:01<00:15,  1.55it/s] 12%|█▏        | 3/26 [00:01<00:14,  1.57it/s] 15%|█▌        | 4/26 [00:02<00:13,  1.58it/s] 19%|█▉        | 5/26 [00:03<00:13,  1.58it/s] 23%|██▎       | 6/26 [00:03<00:12,  1.59it/s] 27%|██▋       | 7/26 [00:04<00:11,  1.59it/s] 31%|███       | 8/26 [00:05<00:11,  1.59it/s] 35%|███▍      | 9/26 [00:05<00:10,  1.59it/s] 38%|███▊      | 10/26 [00:06<00:10,  1.59it/s] 42%|████▏     | 11/26 [00:06<00:09,  1.59it/s] 46%|████▌     | 12/26 [00:07<00:08,  1.59it/s] 50%|█████     | 13/26 [00:07<00:06,  1.95it/s][INFO|trainer.py:2340] 2023-11-01 17:03:41,012 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-13
[INFO|configuration_utils.py:446] 2023-11-01 17:03:41,019 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-13/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:03:42,828 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-13/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:03:42,834 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-13/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:03:42,837 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-13/special_tokens_map.json
 54%|█████▍    | 14/26 [00:14<00:26,  2.23s/it] 58%|█████▊    | 15/26 [00:14<00:19,  1.74s/it] 62%|██████▏   | 16/26 [00:15<00:14,  1.41s/it] 65%|██████▌   | 17/26 [00:15<00:10,  1.18s/it] 69%|██████▉   | 18/26 [00:16<00:08,  1.01s/it] 73%|███████▎  | 19/26 [00:17<00:06,  1.12it/s] 77%|███████▋  | 20/26 [00:17<00:04,  1.22it/s] 81%|████████  | 21/26 [00:18<00:03,  1.31it/s] 85%|████████▍ | 22/26 [00:19<00:02,  1.38it/s] 88%|████████▊ | 23/26 [00:19<00:02,  1.44it/s] 92%|█████████▏| 24/26 [00:20<00:01,  1.48it/s] 96%|█████████▌| 25/26 [00:20<00:00,  1.50it/s]100%|██████████| 26/26 [00:21<00:00,  1.86it/s][INFO|trainer.py:2340] 2023-11-01 17:03:54,393 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-26
[INFO|configuration_utils.py:446] 2023-11-01 17:03:54,403 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-26/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:03:56,270 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-26/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:03:56,277 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-26/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:03:56,281 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/checkpoint-26/special_tokens_map.json
[INFO|trainer.py:1662] 2023-11-01 17:03:59,885 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 26/26 [00:26<00:00,  1.86it/s]100%|██████████| 26/26 [00:26<00:00,  1.03s/it]
[INFO|trainer.py:2340] 2023-11-01 17:03:59,903 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01
[INFO|configuration_utils.py:446] 2023-11-01 17:03:59,909 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:04:01,680 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:04:01,686 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:04:01,691 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.01/special_tokens_map.json
{'train_runtime': 26.745, 'train_samples_per_second': 58.927, 'train_steps_per_second': 0.972, 'train_loss': 0.6988327319805439, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.6988
  train_runtime            = 0:00:26.74
  train_samples            =        788
  train_samples_per_second =     58.927
  train_steps_per_second   =      0.972
11/01/2023 17:04:01 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-11-01 17:04:01,859 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-11-01 17:04:01,861 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:04:01,861 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-11-01 17:04:01,861 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-11-01 17:04:01,861 >>   Num examples = 780
[INFO|trainer.py:2595] 2023-11-01 17:04:01,861 >>   Batch size = 64
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:01,  9.12it/s] 23%|██▎       | 3/13 [00:00<00:01,  6.42it/s] 31%|███       | 4/13 [00:00<00:01,  5.56it/s] 38%|███▊      | 5/13 [00:00<00:01,  5.15it/s] 46%|████▌     | 6/13 [00:01<00:01,  4.93it/s] 54%|█████▍    | 7/13 [00:01<00:01,  4.79it/s] 62%|██████▏   | 8/13 [00:01<00:01,  4.70it/s] 69%|██████▉   | 9/13 [00:01<00:00,  4.65it/s] 77%|███████▋  | 10/13 [00:01<00:00,  4.60it/s] 85%|████████▍ | 11/13 [00:02<00:00,  4.58it/s] 92%|█████████▏| 12/13 [00:02<00:00,  4.56it/s][WARNING|training_args.py:1095] 2023-11-01 17:04:04,571 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 13/13 [00:02<00:00,  5.20it/s]
[WARNING|training_args.py:1095] 2023-11-01 17:04:04,606 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:04:04,606 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-11-01 17:04:04,776 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5051282048225403}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.5051
  eval_loss               =      0.694
  eval_runtime            = 0:00:02.71
  eval_samples            =        780
  eval_samples_per_second =    287.799
  eval_steps_per_second   =      4.797
Processing train_examples_0.05.csv
11/01/2023 17:04:11 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/01/2023 17:04:11 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/runs/Nov01_17-04-11_clip02.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/01/2023 17:04:11 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/experiments/pretraining-vs-finetuning/finetuning_data/snli/train_examples_0.05.csv
11/01/2023 17:04:11 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/snli/analysis_model_examples/dev_examples.csv
11/01/2023 17:04:11 - WARNING - datasets.builder - Using custom data configuration default-5eb3dec7b3ef7167
11/01/2023 17:04:11 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/01/2023 17:04:11 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-5eb3dec7b3ef7167/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
11/01/2023 17:04:11 - WARNING - datasets.builder - Reusing dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-5eb3dec7b3ef7167/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
11/01/2023 17:04:11 - INFO - datasets.info - Loading Dataset info from /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-5eb3dec7b3ef7167/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 502.31it/s]
[INFO|configuration_utils.py:659] 2023-11-01 17:04:11,677 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:04:11,682 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-11-01 17:04:11,723 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-11-01 17:04:11,777 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:04:11,777 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:04:12,005 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:04:12,005 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:04:12,005 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:04:12,005 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:04:12,005 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-11-01 17:04:12,057 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:04:12,057 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-11-01 17:04:12,302 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-11-01 17:04:13,523 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-11-01 17:04:13,523 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
11/01/2023 17:04:14 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f60780b91f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
11/01/2023 17:04:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-5eb3dec7b3ef7167/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
11/01/2023 17:04:15 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f60780b9040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.
Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]11/01/2023 17:04:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-5eb3dec7b3ef7167/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  3.15ba/s]Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  3.15ba/s]hi
11/01/2023 17:04:15 - INFO - __main__ - Sample 1126 of the training set: {'sentence1': 'Two people sit on a well kept stone path that overlooks a large body of water. Two people are taking pictures on the well.', 'sentence2': "There's a bucket in their photos.", 'label': 1, 'input_ids': [0, 9058, 82, 2662, 15, 10, 157, 1682, 7326, 2718, 14, 20718, 29, 10, 739, 809, 9, 514, 4, 1596, 82, 32, 602, 3493, 15, 5, 157, 4, 2, 2, 970, 18, 10, 14792, 11, 49, 2356, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:04:15 - INFO - __main__ - Sample 1003 of the training set: {'sentence1': 'A group of people eating at an outside cafe with a bike showing in the foreground. The people eating outside of the cafe are at small tables.', 'sentence2': 'The meal was served family-style for ten people.', 'label': 0, 'input_ids': [0, 250, 333, 9, 82, 4441, 23, 41, 751, 16381, 19, 10, 4806, 2018, 11, 5, 39299, 4, 20, 82, 4441, 751, 9, 5, 16381, 32, 23, 650, 9248, 4, 2, 2, 133, 5820, 21, 1665, 284, 12, 5827, 13, 2724, 82, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:04:15 - INFO - __main__ - Sample 914 of the training set: {'sentence1': 'A man in a blue shirt is speaking to a group of people in front of a building. The man is giving a speech about an important topic.', 'sentence2': 'The man is asking the other people when the store doors will open and the big sale will start.', 'label': 0, 'input_ids': [0, 250, 313, 11, 10, 2440, 6399, 16, 2686, 7, 10, 333, 9, 82, 11, 760, 9, 10, 745, 4, 20, 313, 16, 1311, 10, 1901, 59, 41, 505, 5674, 4, 2, 2, 133, 313, 16, 1996, 5, 97, 82, 77, 5, 1400, 4259, 40, 490, 8, 5, 380, 1392, 40, 386, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:622] 2023-11-01 17:04:18,815 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-11-01 17:04:18,829 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-11-01 17:04:18,829 >>   Num examples = 3755
[INFO|trainer.py:1421] 2023-11-01 17:04:18,829 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-11-01 17:04:18,829 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-11-01 17:04:18,829 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-11-01 17:04:18,829 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-11-01 17:04:18,829 >>   Total optimization steps = 118
[WARNING|training_args.py:1095] 2023-11-01 17:04:18,847 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/118 [00:00<?, ?it/s]  1%|          | 1/118 [00:00<01:14,  1.58it/s]  2%|▏         | 2/118 [00:01<01:13,  1.58it/s]  3%|▎         | 3/118 [00:01<01:12,  1.58it/s]  3%|▎         | 4/118 [00:02<01:12,  1.58it/s]  4%|▍         | 5/118 [00:03<01:11,  1.58it/s]  5%|▌         | 6/118 [00:03<01:11,  1.58it/s]  6%|▌         | 7/118 [00:04<01:10,  1.57it/s]  7%|▋         | 8/118 [00:05<01:09,  1.57it/s]  8%|▊         | 9/118 [00:05<01:09,  1.57it/s]  8%|▊         | 10/118 [00:06<01:08,  1.57it/s]  9%|▉         | 11/118 [00:06<01:08,  1.57it/s] 10%|█         | 12/118 [00:07<01:07,  1.57it/s] 11%|█         | 13/118 [00:08<01:06,  1.57it/s] 12%|█▏        | 14/118 [00:08<01:06,  1.57it/s] 13%|█▎        | 15/118 [00:09<01:05,  1.57it/s] 14%|█▎        | 16/118 [00:10<01:04,  1.57it/s] 14%|█▍        | 17/118 [00:10<01:04,  1.57it/s] 15%|█▌        | 18/118 [00:11<01:03,  1.57it/s] 16%|█▌        | 19/118 [00:12<01:03,  1.57it/s] 17%|█▋        | 20/118 [00:12<01:02,  1.57it/s] 18%|█▊        | 21/118 [00:13<01:01,  1.57it/s] 19%|█▊        | 22/118 [00:13<01:01,  1.57it/s] 19%|█▉        | 23/118 [00:14<01:00,  1.57it/s] 20%|██        | 24/118 [00:15<00:59,  1.57it/s] 21%|██        | 25/118 [00:15<00:59,  1.57it/s] 22%|██▏       | 26/118 [00:16<00:58,  1.57it/s] 23%|██▎       | 27/118 [00:17<00:58,  1.57it/s] 24%|██▎       | 28/118 [00:17<00:57,  1.57it/s] 25%|██▍       | 29/118 [00:18<00:56,  1.57it/s] 25%|██▌       | 30/118 [00:19<00:56,  1.56it/s] 26%|██▋       | 31/118 [00:19<00:55,  1.56it/s] 27%|██▋       | 32/118 [00:20<00:54,  1.56it/s] 28%|██▊       | 33/118 [00:21<00:54,  1.56it/s] 29%|██▉       | 34/118 [00:21<00:53,  1.56it/s] 30%|██▉       | 35/118 [00:22<00:53,  1.56it/s] 31%|███       | 36/118 [00:22<00:52,  1.56it/s] 31%|███▏      | 37/118 [00:23<00:51,  1.56it/s] 32%|███▏      | 38/118 [00:24<00:51,  1.56it/s] 33%|███▎      | 39/118 [00:24<00:50,  1.56it/s] 34%|███▍      | 40/118 [00:25<00:50,  1.56it/s] 35%|███▍      | 41/118 [00:26<00:49,  1.56it/s] 36%|███▌      | 42/118 [00:26<00:48,  1.56it/s] 36%|███▋      | 43/118 [00:27<00:48,  1.56it/s] 37%|███▋      | 44/118 [00:28<00:47,  1.56it/s] 38%|███▊      | 45/118 [00:28<00:46,  1.55it/s] 39%|███▉      | 46/118 [00:29<00:46,  1.55it/s] 40%|███▉      | 47/118 [00:30<00:45,  1.55it/s] 41%|████      | 48/118 [00:30<00:45,  1.55it/s] 42%|████▏     | 49/118 [00:31<00:44,  1.55it/s] 42%|████▏     | 50/118 [00:31<00:43,  1.55it/s] 43%|████▎     | 51/118 [00:32<00:43,  1.55it/s] 44%|████▍     | 52/118 [00:33<00:42,  1.55it/s] 45%|████▍     | 53/118 [00:33<00:41,  1.55it/s] 46%|████▌     | 54/118 [00:34<00:41,  1.55it/s] 47%|████▋     | 55/118 [00:35<00:40,  1.55it/s] 47%|████▋     | 56/118 [00:35<00:39,  1.55it/s] 48%|████▊     | 57/118 [00:36<00:39,  1.55it/s] 49%|████▉     | 58/118 [00:37<00:38,  1.55it/s] 50%|█████     | 59/118 [00:37<00:34,  1.69it/s][INFO|trainer.py:2340] 2023-11-01 17:04:56,465 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-59
[INFO|configuration_utils.py:446] 2023-11-01 17:04:56,472 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-59/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:04:58,303 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-59/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:04:58,310 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-59/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:04:58,314 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-59/special_tokens_map.json
 51%|█████     | 60/118 [00:43<02:10,  2.25s/it] 52%|█████▏    | 61/118 [00:44<01:40,  1.77s/it] 53%|█████▎    | 62/118 [00:44<01:20,  1.43s/it] 53%|█████▎    | 63/118 [00:45<01:05,  1.19s/it] 54%|█████▍    | 64/118 [00:46<00:55,  1.03s/it] 55%|█████▌    | 65/118 [00:46<00:48,  1.10it/s] 56%|█████▌    | 66/118 [00:47<00:43,  1.20it/s] 57%|█████▋    | 67/118 [00:48<00:39,  1.29it/s] 58%|█████▊    | 68/118 [00:48<00:36,  1.36it/s] 58%|█████▊    | 69/118 [00:49<00:34,  1.41it/s] 59%|█████▉    | 70/118 [00:50<00:33,  1.45it/s] 60%|██████    | 71/118 [00:50<00:31,  1.48it/s] 61%|██████    | 72/118 [00:51<00:30,  1.50it/s] 62%|██████▏   | 73/118 [00:52<00:29,  1.51it/s] 63%|██████▎   | 74/118 [00:52<00:28,  1.52it/s] 64%|██████▎   | 75/118 [00:53<00:28,  1.53it/s] 64%|██████▍   | 76/118 [00:54<00:27,  1.54it/s] 65%|██████▌   | 77/118 [00:54<00:26,  1.54it/s] 66%|██████▌   | 78/118 [00:55<00:25,  1.54it/s] 67%|██████▋   | 79/118 [00:55<00:25,  1.54it/s] 68%|██████▊   | 80/118 [00:56<00:24,  1.55it/s] 69%|██████▊   | 81/118 [00:57<00:23,  1.55it/s] 69%|██████▉   | 82/118 [00:57<00:23,  1.55it/s] 70%|███████   | 83/118 [00:58<00:22,  1.55it/s] 71%|███████   | 84/118 [00:59<00:22,  1.54it/s] 72%|███████▏  | 85/118 [00:59<00:21,  1.54it/s] 73%|███████▎  | 86/118 [01:00<00:20,  1.54it/s] 74%|███████▎  | 87/118 [01:01<00:20,  1.54it/s] 75%|███████▍  | 88/118 [01:01<00:19,  1.54it/s] 75%|███████▌  | 89/118 [01:02<00:18,  1.54it/s] 76%|███████▋  | 90/118 [01:03<00:18,  1.54it/s] 77%|███████▋  | 91/118 [01:03<00:17,  1.54it/s] 78%|███████▊  | 92/118 [01:04<00:16,  1.54it/s] 79%|███████▉  | 93/118 [01:05<00:16,  1.54it/s] 80%|███████▉  | 94/118 [01:05<00:15,  1.53it/s] 81%|████████  | 95/118 [01:06<00:15,  1.53it/s] 81%|████████▏ | 96/118 [01:07<00:14,  1.53it/s] 82%|████████▏ | 97/118 [01:07<00:13,  1.52it/s] 83%|████████▎ | 98/118 [01:08<00:13,  1.52it/s] 84%|████████▍ | 99/118 [01:08<00:12,  1.52it/s] 85%|████████▍ | 100/118 [01:09<00:11,  1.52it/s] 86%|████████▌ | 101/118 [01:10<00:11,  1.53it/s] 86%|████████▋ | 102/118 [01:10<00:10,  1.51it/s] 87%|████████▋ | 103/118 [01:11<00:09,  1.52it/s] 88%|████████▊ | 104/118 [01:12<00:09,  1.51it/s] 89%|████████▉ | 105/118 [01:12<00:08,  1.52it/s] 90%|████████▉ | 106/118 [01:13<00:07,  1.52it/s] 91%|█████████ | 107/118 [01:14<00:07,  1.50it/s] 92%|█████████▏| 108/118 [01:14<00:06,  1.51it/s] 92%|█████████▏| 109/118 [01:15<00:05,  1.51it/s] 93%|█████████▎| 110/118 [01:16<00:05,  1.52it/s] 94%|█████████▍| 111/118 [01:16<00:04,  1.51it/s] 95%|█████████▍| 112/118 [01:17<00:03,  1.51it/s] 96%|█████████▌| 113/118 [01:18<00:03,  1.51it/s] 97%|█████████▋| 114/118 [01:18<00:02,  1.52it/s] 97%|█████████▋| 115/118 [01:19<00:01,  1.50it/s] 98%|█████████▊| 116/118 [01:20<00:01,  1.51it/s] 99%|█████████▉| 117/118 [01:20<00:00,  1.51it/s]100%|██████████| 118/118 [01:21<00:00,  1.65it/s][INFO|trainer.py:2340] 2023-11-01 17:05:40,243 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-118
[INFO|configuration_utils.py:446] 2023-11-01 17:05:40,250 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-118/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:05:42,058 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:05:42,066 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:05:42,072 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/checkpoint-118/special_tokens_map.json
[INFO|trainer.py:1662] 2023-11-01 17:05:45,587 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 118/118 [01:26<00:00,  1.65it/s]100%|██████████| 118/118 [01:26<00:00,  1.36it/s]
[INFO|trainer.py:2340] 2023-11-01 17:05:45,610 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05
[INFO|configuration_utils.py:446] 2023-11-01 17:05:45,616 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:05:47,407 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:05:47,415 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:05:47,422 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.05/special_tokens_map.json
{'train_runtime': 86.7573, 'train_samples_per_second': 86.563, 'train_steps_per_second': 1.36, 'train_loss': 0.6946614152294094, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.6947
  train_runtime            = 0:01:26.75
  train_samples            =       3755
  train_samples_per_second =     86.563
  train_steps_per_second   =       1.36
11/01/2023 17:05:47 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-11-01 17:05:47,508 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-11-01 17:05:47,510 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:05:47,510 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-11-01 17:05:47,510 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-11-01 17:05:47,510 >>   Num examples = 780
[INFO|trainer.py:2595] 2023-11-01 17:05:47,510 >>   Batch size = 64
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:01,  8.84it/s] 23%|██▎       | 3/13 [00:00<00:01,  6.27it/s] 31%|███       | 4/13 [00:00<00:01,  5.45it/s] 38%|███▊      | 5/13 [00:00<00:01,  5.06it/s] 46%|████▌     | 6/13 [00:01<00:01,  4.84it/s] 54%|█████▍    | 7/13 [00:01<00:01,  4.71it/s] 62%|██████▏   | 8/13 [00:01<00:01,  4.62it/s] 69%|██████▉   | 9/13 [00:01<00:00,  4.57it/s] 77%|███████▋  | 10/13 [00:02<00:00,  4.53it/s] 85%|████████▍ | 11/13 [00:02<00:00,  4.51it/s] 92%|█████████▏| 12/13 [00:02<00:00,  4.48it/s][WARNING|training_args.py:1095] 2023-11-01 17:05:50,279 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 13/13 [00:02<00:00,  5.11it/s]
[WARNING|training_args.py:1095] 2023-11-01 17:05:50,313 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:05:50,313 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-11-01 17:05:50,800 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5051282048225403}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.5051
  eval_loss               =     0.6931
  eval_runtime            = 0:00:02.76
  eval_samples            =        780
  eval_samples_per_second =    281.639
  eval_steps_per_second   =      4.694
Processing train_examples_0.1.csv
11/01/2023 17:05:58 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/01/2023 17:05:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/runs/Nov01_17-05-58_clip02.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/01/2023 17:05:58 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/experiments/pretraining-vs-finetuning/finetuning_data/snli/train_examples_0.1.csv
11/01/2023 17:05:58 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/snli/analysis_model_examples/dev_examples.csv
11/01/2023 17:05:58 - WARNING - datasets.builder - Using custom data configuration default-19a4199f61cf1131
11/01/2023 17:05:58 - INFO - datasets.builder - Generating dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-19a4199f61cf1131/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Downloading and preparing dataset csv/default to /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-19a4199f61cf1131/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 10565.00it/s]11/01/2023 17:05:58 - INFO - datasets.download.download_manager - Downloading took 0.0 min
11/01/2023 17:05:58 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min

Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 63.45it/s]11/01/2023 17:05:58 - INFO - datasets.utils.info_utils - Unable to verify checksums.
11/01/2023 17:05:58 - INFO - datasets.builder - Generating train split

0 tables [00:00, ? tables/s]                            11/01/2023 17:05:58 - INFO - datasets.builder - Generating validation split
0 tables [00:00, ? tables/s]                            11/01/2023 17:05:58 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-19a4199f61cf1131/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 440.39it/s]
[INFO|configuration_utils.py:659] 2023-11-01 17:05:58,438 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:05:58,441 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-11-01 17:05:58,478 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-11-01 17:05:58,519 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:05:58,519 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:05:58,725 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:05:58,726 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:05:58,726 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:05:58,726 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:05:58,726 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-11-01 17:05:58,770 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:05:58,771 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-11-01 17:05:59,012 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-11-01 17:06:00,218 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-11-01 17:06:00,219 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
11/01/2023 17:06:00 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7ff051a0f0d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/8 [00:00<?, ?ba/s]11/01/2023 17:06:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-19a4199f61cf1131/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:  12%|█▎        | 1/8 [00:00<00:02,  2.51ba/s]Running tokenizer on dataset:  25%|██▌       | 2/8 [00:00<00:02,  2.70ba/s]Running tokenizer on dataset:  38%|███▊      | 3/8 [00:01<00:01,  2.78ba/s]Running tokenizer on dataset:  50%|█████     | 4/8 [00:01<00:01,  2.88ba/s]Running tokenizer on dataset:  62%|██████▎   | 5/8 [00:01<00:01,  2.94ba/s]Running tokenizer on dataset:  75%|███████▌  | 6/8 [00:02<00:00,  3.01ba/s]Running tokenizer on dataset:  88%|████████▊ | 7/8 [00:02<00:00,  2.85ba/s]Running tokenizer on dataset: 100%|██████████| 8/8 [00:02<00:00,  3.44ba/s]Running tokenizer on dataset: 100%|██████████| 8/8 [00:02<00:00,  3.06ba/s]11/01/2023 17:06:04 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7ff051a611f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]11/01/2023 17:06:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-19a4199f61cf1131/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  3.84ba/s]Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  3.83ba/s]hi
11/01/2023 17:06:04 - INFO - __main__ - Sample 2253 of the training set: {'sentence1': 'A man in a brown coat is wearing a helmet and riding a bicycle while carrying two plastic bags. A man carries groceries.', 'sentence2': 'A man carries plastic bags inside plastic bags to bin to be recycled.', 'label': 0, 'input_ids': [0, 250, 313, 11, 10, 6219, 9540, 16, 2498, 10, 13722, 8, 5793, 10, 14678, 150, 3406, 80, 4136, 5565, 4, 83, 313, 5723, 20279, 4, 2, 2, 250, 313, 5723, 4136, 5565, 1025, 4136, 5565, 7, 6870, 7, 28, 18505, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:06:04 - INFO - __main__ - Sample 2006 of the training set: {'sentence1': 'a young man jumping on a red railing A teenager is grinding on a railing with his skateboard.', 'sentence2': 'He is wearing knee and elbow pads.', 'label': 1, 'input_ids': [0, 102, 664, 313, 9755, 15, 10, 1275, 35376, 83, 7044, 16, 25440, 15, 10, 35376, 19, 39, 12800, 4929, 4, 2, 2, 894, 16, 2498, 4117, 8, 13789, 18569, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:06:04 - INFO - __main__ - Sample 1828 of the training set: {'sentence1': 'Juggler on beach in blue swimsuit. A juggler is trying to earn money.', 'sentence2': 'There is a crowd gathered', 'label': 1, 'input_ids': [0, 863, 15106, 1371, 15, 4105, 11, 2440, 6966, 18605, 4, 83, 23840, 1371, 16, 667, 7, 4073, 418, 4, 2, 2, 970, 16, 10, 2180, 4366, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:622] 2023-11-01 17:06:08,112 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-11-01 17:06:08,127 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-11-01 17:06:08,127 >>   Num examples = 7508
[INFO|trainer.py:1421] 2023-11-01 17:06:08,127 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-11-01 17:06:08,127 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-11-01 17:06:08,127 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-11-01 17:06:08,127 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-11-01 17:06:08,127 >>   Total optimization steps = 236
[WARNING|training_args.py:1095] 2023-11-01 17:06:08,143 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/236 [00:00<?, ?it/s]  0%|          | 1/236 [00:00<02:30,  1.56it/s]  1%|          | 2/236 [00:01<02:29,  1.56it/s]  1%|▏         | 3/236 [00:01<02:29,  1.56it/s]  2%|▏         | 4/236 [00:02<02:28,  1.56it/s]  2%|▏         | 5/236 [00:03<02:28,  1.56it/s]  3%|▎         | 6/236 [00:03<02:27,  1.56it/s]  3%|▎         | 7/236 [00:04<02:26,  1.56it/s]  3%|▎         | 8/236 [00:05<02:26,  1.56it/s]  4%|▍         | 9/236 [00:05<02:25,  1.56it/s]  4%|▍         | 10/236 [00:06<02:25,  1.56it/s]  5%|▍         | 11/236 [00:07<02:24,  1.56it/s]  5%|▌         | 12/236 [00:07<02:23,  1.56it/s]  6%|▌         | 13/236 [00:08<02:23,  1.56it/s]  6%|▌         | 14/236 [00:08<02:22,  1.56it/s]  6%|▋         | 15/236 [00:09<02:22,  1.55it/s]  7%|▋         | 16/236 [00:10<02:22,  1.55it/s]  7%|▋         | 17/236 [00:10<02:21,  1.55it/s]  8%|▊         | 18/236 [00:11<02:20,  1.55it/s]  8%|▊         | 19/236 [00:12<02:20,  1.55it/s]  8%|▊         | 20/236 [00:12<02:19,  1.55it/s]  9%|▉         | 21/236 [00:13<02:18,  1.55it/s]  9%|▉         | 22/236 [00:14<02:18,  1.55it/s] 10%|▉         | 23/236 [00:14<02:17,  1.55it/s] 10%|█         | 24/236 [00:15<02:16,  1.55it/s] 11%|█         | 25/236 [00:16<02:16,  1.55it/s] 11%|█         | 26/236 [00:16<02:15,  1.55it/s] 11%|█▏        | 27/236 [00:17<02:15,  1.55it/s] 12%|█▏        | 28/236 [00:18<02:14,  1.54it/s] 12%|█▏        | 29/236 [00:18<02:14,  1.54it/s] 13%|█▎        | 30/236 [00:19<02:13,  1.54it/s] 13%|█▎        | 31/236 [00:19<02:12,  1.54it/s] 14%|█▎        | 32/236 [00:20<02:12,  1.54it/s] 14%|█▍        | 33/236 [00:21<02:11,  1.54it/s] 14%|█▍        | 34/236 [00:21<02:11,  1.54it/s] 15%|█▍        | 35/236 [00:22<02:10,  1.54it/s] 15%|█▌        | 36/236 [00:23<02:09,  1.54it/s] 16%|█▌        | 37/236 [00:23<02:09,  1.54it/s] 16%|█▌        | 38/236 [00:24<02:08,  1.54it/s] 17%|█▋        | 39/236 [00:25<02:07,  1.54it/s] 17%|█▋        | 40/236 [00:25<02:07,  1.54it/s] 17%|█▋        | 41/236 [00:26<02:06,  1.54it/s] 18%|█▊        | 42/236 [00:27<02:06,  1.54it/s] 18%|█▊        | 43/236 [00:27<02:05,  1.53it/s] 19%|█▊        | 44/236 [00:28<02:05,  1.52it/s] 19%|█▉        | 45/236 [00:29<02:05,  1.52it/s] 19%|█▉        | 46/236 [00:29<02:05,  1.52it/s] 20%|█▉        | 47/236 [00:30<02:04,  1.52it/s] 20%|██        | 48/236 [00:31<02:04,  1.51it/s] 21%|██        | 49/236 [00:31<02:03,  1.52it/s] 21%|██        | 50/236 [00:32<02:02,  1.52it/s] 22%|██▏       | 51/236 [00:33<02:01,  1.52it/s] 22%|██▏       | 52/236 [00:33<02:02,  1.50it/s] 22%|██▏       | 53/236 [00:34<02:01,  1.51it/s] 23%|██▎       | 54/236 [00:35<02:00,  1.51it/s] 23%|██▎       | 55/236 [00:35<01:59,  1.51it/s] 24%|██▎       | 56/236 [00:36<02:00,  1.49it/s] 24%|██▍       | 57/236 [00:37<01:59,  1.50it/s] 25%|██▍       | 58/236 [00:37<01:57,  1.51it/s] 25%|██▌       | 59/236 [00:38<01:56,  1.52it/s] 25%|██▌       | 60/236 [00:39<01:56,  1.52it/s] 26%|██▌       | 61/236 [00:39<01:56,  1.51it/s] 26%|██▋       | 62/236 [00:40<01:55,  1.51it/s] 27%|██▋       | 63/236 [00:41<01:54,  1.52it/s] 27%|██▋       | 64/236 [00:41<01:53,  1.52it/s] 28%|██▊       | 65/236 [00:42<01:54,  1.49it/s] 28%|██▊       | 66/236 [00:43<01:53,  1.49it/s] 28%|██▊       | 67/236 [00:43<01:52,  1.50it/s] 29%|██▉       | 68/236 [00:44<01:53,  1.48it/s] 29%|██▉       | 69/236 [00:45<01:52,  1.48it/s] 30%|██▉       | 70/236 [00:45<01:51,  1.49it/s] 30%|███       | 71/236 [00:46<01:49,  1.50it/s] 31%|███       | 72/236 [00:47<01:50,  1.48it/s] 31%|███       | 73/236 [00:47<01:49,  1.48it/s] 31%|███▏      | 74/236 [00:48<01:48,  1.50it/s] 32%|███▏      | 75/236 [00:49<01:49,  1.48it/s] 32%|███▏      | 76/236 [00:49<01:48,  1.48it/s] 33%|███▎      | 77/236 [00:50<01:46,  1.49it/s] 33%|███▎      | 78/236 [00:51<01:45,  1.50it/s] 33%|███▎      | 79/236 [00:51<01:45,  1.49it/s] 34%|███▍      | 80/236 [00:52<01:44,  1.50it/s] 34%|███▍      | 81/236 [00:53<01:42,  1.51it/s] 35%|███▍      | 82/236 [00:53<01:42,  1.51it/s] 35%|███▌      | 83/236 [00:54<01:43,  1.48it/s] 36%|███▌      | 84/236 [00:55<01:43,  1.47it/s] 36%|███▌      | 85/236 [00:55<01:42,  1.47it/s] 36%|███▋      | 86/236 [00:56<01:42,  1.46it/s] 37%|███▋      | 87/236 [00:57<01:42,  1.46it/s] 37%|███▋      | 88/236 [00:57<01:40,  1.47it/s] 38%|███▊      | 89/236 [00:58<01:39,  1.48it/s] 38%|███▊      | 90/236 [00:59<01:38,  1.49it/s] 39%|███▊      | 91/236 [00:59<01:36,  1.50it/s] 39%|███▉      | 92/236 [01:00<01:35,  1.51it/s] 39%|███▉      | 93/236 [01:01<01:36,  1.48it/s] 40%|███▉      | 94/236 [01:01<01:35,  1.48it/s] 40%|████      | 95/236 [01:02<01:34,  1.49it/s] 41%|████      | 96/236 [01:03<01:34,  1.48it/s] 41%|████      | 97/236 [01:03<01:33,  1.49it/s] 42%|████▏     | 98/236 [01:04<01:32,  1.50it/s] 42%|████▏     | 99/236 [01:05<01:31,  1.50it/s] 42%|████▏     | 100/236 [01:05<01:31,  1.49it/s] 43%|████▎     | 101/236 [01:06<01:29,  1.50it/s] 43%|████▎     | 102/236 [01:07<01:29,  1.50it/s] 44%|████▎     | 103/236 [01:07<01:28,  1.51it/s] 44%|████▍     | 104/236 [01:08<01:28,  1.50it/s] 44%|████▍     | 105/236 [01:09<01:27,  1.50it/s] 45%|████▍     | 106/236 [01:09<01:26,  1.51it/s] 45%|████▌     | 107/236 [01:10<01:25,  1.51it/s] 46%|████▌     | 108/236 [01:11<01:26,  1.49it/s] 46%|████▌     | 109/236 [01:11<01:25,  1.49it/s] 47%|████▋     | 110/236 [01:12<01:24,  1.50it/s] 47%|████▋     | 111/236 [01:13<01:23,  1.50it/s] 47%|████▋     | 112/236 [01:13<01:23,  1.48it/s] 48%|████▊     | 113/236 [01:14<01:22,  1.49it/s] 48%|████▊     | 114/236 [01:15<01:21,  1.49it/s] 49%|████▊     | 115/236 [01:15<01:21,  1.48it/s] 49%|████▉     | 116/236 [01:16<01:20,  1.49it/s] 50%|████▉     | 117/236 [01:17<01:19,  1.50it/s] 50%|█████     | 118/236 [01:17<01:04,  1.84it/s][INFO|trainer.py:2340] 2023-11-01 17:07:25,726 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-118
[INFO|configuration_utils.py:446] 2023-11-01 17:07:25,734 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-118/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:07:27,540 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:07:27,549 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:07:27,554 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-118/special_tokens_map.json
 50%|█████     | 119/236 [01:23<04:29,  2.31s/it] 51%|█████     | 120/236 [01:24<03:29,  1.81s/it] 51%|█████▏    | 121/236 [01:25<02:47,  1.46s/it] 52%|█████▏    | 122/236 [01:25<02:18,  1.22s/it] 52%|█████▏    | 123/236 [01:26<01:58,  1.05s/it] 53%|█████▎    | 124/236 [01:27<01:43,  1.08it/s] 53%|█████▎    | 125/236 [01:27<01:33,  1.18it/s] 53%|█████▎    | 126/236 [01:28<01:26,  1.27it/s] 54%|█████▍    | 127/236 [01:29<01:21,  1.34it/s] 54%|█████▍    | 128/236 [01:29<01:17,  1.40it/s] 55%|█████▍    | 129/236 [01:30<01:14,  1.44it/s] 55%|█████▌    | 130/236 [01:31<01:12,  1.46it/s] 56%|█████▌    | 131/236 [01:31<01:10,  1.48it/s] 56%|█████▌    | 132/236 [01:32<01:09,  1.50it/s] 56%|█████▋    | 133/236 [01:33<01:08,  1.51it/s] 57%|█████▋    | 134/236 [01:33<01:07,  1.51it/s] 57%|█████▋    | 135/236 [01:34<01:06,  1.52it/s] 58%|█████▊    | 136/236 [01:35<01:05,  1.52it/s] 58%|█████▊    | 137/236 [01:35<01:05,  1.52it/s] 58%|█████▊    | 138/236 [01:36<01:04,  1.51it/s] 59%|█████▉    | 139/236 [01:37<01:04,  1.51it/s] 59%|█████▉    | 140/236 [01:37<01:03,  1.51it/s] 60%|█████▉    | 141/236 [01:38<01:03,  1.50it/s] 60%|██████    | 142/236 [01:39<01:02,  1.51it/s] 61%|██████    | 143/236 [01:39<01:01,  1.51it/s] 61%|██████    | 144/236 [01:40<01:01,  1.51it/s] 61%|██████▏   | 145/236 [01:40<01:00,  1.51it/s] 62%|██████▏   | 146/236 [01:41<00:59,  1.51it/s] 62%|██████▏   | 147/236 [01:42<00:58,  1.52it/s] 63%|██████▎   | 148/236 [01:42<00:58,  1.50it/s] 63%|██████▎   | 149/236 [01:43<00:57,  1.51it/s] 64%|██████▎   | 150/236 [01:44<00:57,  1.51it/s] 64%|██████▍   | 151/236 [01:44<00:56,  1.51it/s] 64%|██████▍   | 152/236 [01:45<00:56,  1.49it/s] 65%|██████▍   | 153/236 [01:46<00:55,  1.50it/s] 65%|██████▌   | 154/236 [01:46<00:54,  1.51it/s] 66%|██████▌   | 155/236 [01:47<00:53,  1.51it/s] 66%|██████▌   | 156/236 [01:48<00:53,  1.49it/s] 67%|██████▋   | 157/236 [01:48<00:52,  1.49it/s] 67%|██████▋   | 158/236 [01:49<00:51,  1.51it/s] 67%|██████▋   | 159/236 [01:50<00:50,  1.51it/s] 68%|██████▊   | 160/236 [01:50<00:50,  1.49it/s] 68%|██████▊   | 161/236 [01:51<00:50,  1.49it/s] 69%|██████▊   | 162/236 [01:52<00:49,  1.50it/s] 69%|██████▉   | 163/236 [01:52<00:48,  1.50it/s] 69%|██████▉   | 164/236 [01:53<00:47,  1.50it/s] 70%|██████▉   | 165/236 [01:54<00:47,  1.51it/s] 70%|███████   | 166/236 [01:54<00:46,  1.51it/s] 71%|███████   | 167/236 [01:55<00:45,  1.51it/s] 71%|███████   | 168/236 [01:56<00:45,  1.50it/s] 72%|███████▏  | 169/236 [01:56<00:44,  1.51it/s] 72%|███████▏  | 170/236 [01:57<00:43,  1.51it/s] 72%|███████▏  | 171/236 [01:58<00:42,  1.51it/s] 73%|███████▎  | 172/236 [01:58<00:42,  1.50it/s] 73%|███████▎  | 173/236 [01:59<00:41,  1.50it/s] 74%|███████▎  | 174/236 [02:00<00:41,  1.51it/s] 74%|███████▍  | 175/236 [02:00<00:40,  1.51it/s] 75%|███████▍  | 176/236 [02:01<00:40,  1.49it/s] 75%|███████▌  | 177/236 [02:02<00:39,  1.48it/s] 75%|███████▌  | 178/236 [02:02<00:38,  1.49it/s] 76%|███████▌  | 179/236 [02:03<00:38,  1.49it/s] 76%|███████▋  | 180/236 [02:04<00:37,  1.50it/s] 77%|███████▋  | 181/236 [02:04<00:36,  1.50it/s] 77%|███████▋  | 182/236 [02:05<00:35,  1.51it/s] 78%|███████▊  | 183/236 [02:06<00:35,  1.51it/s] 78%|███████▊  | 184/236 [02:06<00:34,  1.51it/s] 78%|███████▊  | 185/236 [02:07<00:33,  1.51it/s] 79%|███████▉  | 186/236 [02:08<00:33,  1.51it/s] 79%|███████▉  | 187/236 [02:08<00:32,  1.51it/s] 80%|███████▉  | 188/236 [02:09<00:32,  1.49it/s] 80%|████████  | 189/236 [02:10<00:31,  1.49it/s] 81%|████████  | 190/236 [02:10<00:30,  1.50it/s] 81%|████████  | 191/236 [02:11<00:29,  1.51it/s] 81%|████████▏ | 192/236 [02:12<00:29,  1.49it/s] 82%|████████▏ | 193/236 [02:12<00:28,  1.49it/s] 82%|████████▏ | 194/236 [02:13<00:28,  1.49it/s] 83%|████████▎ | 195/236 [02:14<00:27,  1.47it/s] 83%|████████▎ | 196/236 [02:15<00:27,  1.48it/s] 83%|████████▎ | 197/236 [02:15<00:26,  1.49it/s] 84%|████████▍ | 198/236 [02:16<00:25,  1.50it/s] 84%|████████▍ | 199/236 [02:17<00:25,  1.48it/s] 85%|████████▍ | 200/236 [02:17<00:24,  1.47it/s] 85%|████████▌ | 201/236 [02:18<00:23,  1.48it/s] 86%|████████▌ | 202/236 [02:19<00:23,  1.46it/s] 86%|████████▌ | 203/236 [02:19<00:22,  1.46it/s] 86%|████████▋ | 204/236 [02:20<00:21,  1.48it/s] 87%|████████▋ | 205/236 [02:21<00:20,  1.48it/s] 87%|████████▋ | 206/236 [02:21<00:20,  1.49it/s] 88%|████████▊ | 207/236 [02:22<00:19,  1.50it/s] 88%|████████▊ | 208/236 [02:23<00:18,  1.51it/s] 89%|████████▊ | 209/236 [02:23<00:18,  1.49it/s] 89%|████████▉ | 210/236 [02:24<00:17,  1.50it/s] 89%|████████▉ | 211/236 [02:25<00:16,  1.50it/s] 90%|████████▉ | 212/236 [02:25<00:15,  1.51it/s] 90%|█████████ | 213/236 [02:26<00:15,  1.50it/s] 91%|█████████ | 214/236 [02:27<00:14,  1.50it/s] 91%|█████████ | 215/236 [02:27<00:13,  1.51it/s] 92%|█████████▏| 216/236 [02:28<00:13,  1.51it/s] 92%|█████████▏| 217/236 [02:29<00:12,  1.48it/s] 92%|█████████▏| 218/236 [02:29<00:12,  1.48it/s] 93%|█████████▎| 219/236 [02:30<00:11,  1.49it/s] 93%|█████████▎| 220/236 [02:31<00:10,  1.47it/s] 94%|█████████▎| 221/236 [02:31<00:10,  1.47it/s] 94%|█████████▍| 222/236 [02:32<00:09,  1.48it/s] 94%|█████████▍| 223/236 [02:33<00:08,  1.46it/s] 95%|█████████▍| 224/236 [02:33<00:08,  1.47it/s] 95%|█████████▌| 225/236 [02:34<00:07,  1.49it/s] 96%|█████████▌| 226/236 [02:35<00:06,  1.50it/s] 96%|█████████▌| 227/236 [02:35<00:06,  1.48it/s] 97%|█████████▋| 228/236 [02:36<00:05,  1.49it/s] 97%|█████████▋| 229/236 [02:37<00:04,  1.50it/s] 97%|█████████▋| 230/236 [02:37<00:03,  1.51it/s] 98%|█████████▊| 231/236 [02:38<00:03,  1.49it/s] 98%|█████████▊| 232/236 [02:39<00:02,  1.49it/s] 99%|█████████▊| 233/236 [02:39<00:02,  1.49it/s] 99%|█████████▉| 234/236 [02:40<00:01,  1.47it/s]100%|█████████▉| 235/236 [02:41<00:00,  1.48it/s]100%|██████████| 236/236 [02:41<00:00,  1.82it/s][INFO|trainer.py:2340] 2023-11-01 17:08:49,657 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-236
[INFO|configuration_utils.py:446] 2023-11-01 17:08:49,666 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-236/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:08:51,431 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:08:51,436 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:08:51,442 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/checkpoint-236/special_tokens_map.json
[INFO|trainer.py:1662] 2023-11-01 17:08:55,318 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 236/236 [02:47<00:00,  1.82it/s]100%|██████████| 236/236 [02:47<00:00,  1.41it/s]
[INFO|trainer.py:2340] 2023-11-01 17:08:55,335 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1
[INFO|configuration_utils.py:446] 2023-11-01 17:08:55,342 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:08:57,170 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:08:57,177 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:08:57,181 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.1/special_tokens_map.json
{'train_runtime': 167.191, 'train_samples_per_second': 89.813, 'train_steps_per_second': 1.412, 'train_loss': 0.6942591263076007, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.6943
  train_runtime            = 0:02:47.19
  train_samples            =       7508
  train_samples_per_second =     89.813
  train_steps_per_second   =      1.412
11/01/2023 17:08:57 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-11-01 17:08:57,268 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-11-01 17:08:57,269 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:08:57,269 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-11-01 17:08:57,269 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-11-01 17:08:57,269 >>   Num examples = 780
[INFO|trainer.py:2595] 2023-11-01 17:08:57,269 >>   Batch size = 64
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:01,  8.88it/s] 23%|██▎       | 3/13 [00:00<00:01,  6.28it/s] 31%|███       | 4/13 [00:00<00:01,  5.44it/s] 38%|███▊      | 5/13 [00:00<00:01,  5.03it/s] 46%|████▌     | 6/13 [00:01<00:01,  4.82it/s] 54%|█████▍    | 7/13 [00:01<00:01,  4.69it/s] 62%|██████▏   | 8/13 [00:01<00:01,  4.61it/s] 69%|██████▉   | 9/13 [00:01<00:00,  4.56it/s] 77%|███████▋  | 10/13 [00:02<00:00,  4.52it/s] 85%|████████▍ | 11/13 [00:02<00:00,  4.49it/s] 92%|█████████▏| 12/13 [00:02<00:00,  4.47it/s][WARNING|training_args.py:1095] 2023-11-01 17:09:00,040 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 13/13 [00:02<00:00,  5.08it/s]
[WARNING|training_args.py:1095] 2023-11-01 17:09:00,085 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:09:00,085 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-11-01 17:09:01,027 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5089743733406067}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =      0.509
  eval_loss               =     0.6928
  eval_runtime            = 0:00:02.77
  eval_samples            =        780
  eval_samples_per_second =    281.472
  eval_steps_per_second   =      4.691
Processing train_examples_0.5.csv
11/01/2023 17:09:08 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/01/2023 17:09:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/runs/Nov01_17-09-08_clip02.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/01/2023 17:09:08 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/experiments/pretraining-vs-finetuning/finetuning_data/snli/train_examples_0.5.csv
11/01/2023 17:09:08 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/snli/analysis_model_examples/dev_examples.csv
11/01/2023 17:09:08 - WARNING - datasets.builder - Using custom data configuration default-f63ef5233e0bcf01
11/01/2023 17:09:09 - INFO - datasets.builder - Generating dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-f63ef5233e0bcf01/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Downloading and preparing dataset csv/default to /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-f63ef5233e0bcf01/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 10143.42it/s]11/01/2023 17:09:09 - INFO - datasets.download.download_manager - Downloading took 0.0 min
11/01/2023 17:09:09 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min

Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 60.14it/s]11/01/2023 17:09:09 - INFO - datasets.utils.info_utils - Unable to verify checksums.
11/01/2023 17:09:09 - INFO - datasets.builder - Generating train split

0 tables [00:00, ? tables/s]                            11/01/2023 17:09:09 - INFO - datasets.builder - Generating validation split
0 tables [00:00, ? tables/s]                            11/01/2023 17:09:09 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-f63ef5233e0bcf01/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 173.05it/s]
[INFO|configuration_utils.py:659] 2023-11-01 17:09:09,436 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:09:09,440 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-11-01 17:09:09,485 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-11-01 17:09:09,531 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:09:09,532 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:09:09,896 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:09:09,896 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:09:09,896 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:09:09,896 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-11-01 17:09:09,896 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-11-01 17:09:09,948 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-11-01 17:09:09,949 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-11-01 17:09:10,240 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-11-01 17:09:11,447 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-11-01 17:09:11,447 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
11/01/2023 17:09:12 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fbad227d0d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/38 [00:00<?, ?ba/s]11/01/2023 17:09:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-f63ef5233e0bcf01/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:   3%|▎         | 1/38 [00:00<00:14,  2.51ba/s]Running tokenizer on dataset:   5%|▌         | 2/38 [00:00<00:13,  2.69ba/s]Running tokenizer on dataset:   8%|▊         | 3/38 [00:01<00:12,  2.79ba/s]Running tokenizer on dataset:  11%|█         | 4/38 [00:01<00:12,  2.83ba/s]Running tokenizer on dataset:  13%|█▎        | 5/38 [00:01<00:11,  2.91ba/s]Running tokenizer on dataset:  16%|█▌        | 6/38 [00:02<00:10,  2.97ba/s]Running tokenizer on dataset:  18%|█▊        | 7/38 [00:02<00:10,  2.83ba/s]Running tokenizer on dataset:  21%|██        | 8/38 [00:02<00:10,  2.89ba/s]Running tokenizer on dataset:  24%|██▎       | 9/38 [00:03<00:09,  2.95ba/s]Running tokenizer on dataset:  26%|██▋       | 10/38 [00:03<00:09,  2.98ba/s]Running tokenizer on dataset:  29%|██▉       | 11/38 [00:03<00:09,  2.99ba/s]Running tokenizer on dataset:  32%|███▏      | 12/38 [00:04<00:08,  3.03ba/s]Running tokenizer on dataset:  34%|███▍      | 13/38 [00:04<00:08,  3.07ba/s]Running tokenizer on dataset:  37%|███▋      | 14/38 [00:04<00:07,  3.05ba/s]Running tokenizer on dataset:  39%|███▉      | 15/38 [00:05<00:07,  3.05ba/s]Running tokenizer on dataset:  42%|████▏     | 16/38 [00:05<00:07,  3.04ba/s]Running tokenizer on dataset:  45%|████▍     | 17/38 [00:05<00:06,  3.07ba/s]Running tokenizer on dataset:  47%|████▋     | 18/38 [00:06<00:06,  3.08ba/s]Running tokenizer on dataset:  50%|█████     | 19/38 [00:06<00:06,  3.11ba/s]Running tokenizer on dataset:  53%|█████▎    | 20/38 [00:06<00:05,  3.12ba/s]Running tokenizer on dataset:  55%|█████▌    | 21/38 [00:07<00:05,  3.11ba/s]Running tokenizer on dataset:  58%|█████▊    | 22/38 [00:07<00:05,  3.14ba/s]Running tokenizer on dataset:  61%|██████    | 23/38 [00:07<00:04,  3.15ba/s]Running tokenizer on dataset:  63%|██████▎   | 24/38 [00:07<00:04,  3.15ba/s]Running tokenizer on dataset:  66%|██████▌   | 25/38 [00:08<00:04,  3.18ba/s]Running tokenizer on dataset:  68%|██████▊   | 26/38 [00:08<00:03,  3.21ba/s]Running tokenizer on dataset:  71%|███████   | 27/38 [00:08<00:03,  3.21ba/s]Running tokenizer on dataset:  74%|███████▎  | 28/38 [00:09<00:03,  3.19ba/s]Running tokenizer on dataset:  76%|███████▋  | 29/38 [00:09<00:03,  2.98ba/s]Running tokenizer on dataset:  79%|███████▉  | 30/38 [00:09<00:02,  3.04ba/s]Running tokenizer on dataset:  82%|████████▏ | 31/38 [00:10<00:02,  3.09ba/s]Running tokenizer on dataset:  84%|████████▍ | 32/38 [00:10<00:01,  3.13ba/s]Running tokenizer on dataset:  87%|████████▋ | 33/38 [00:10<00:01,  3.15ba/s]Running tokenizer on dataset:  89%|████████▉ | 34/38 [00:11<00:01,  3.16ba/s]Running tokenizer on dataset:  92%|█████████▏| 35/38 [00:11<00:00,  3.17ba/s]Running tokenizer on dataset:  95%|█████████▍| 36/38 [00:11<00:00,  3.17ba/s]Running tokenizer on dataset:  97%|█████████▋| 37/38 [00:12<00:00,  3.18ba/s]Running tokenizer on dataset: 100%|██████████| 38/38 [00:12<00:00,  3.58ba/s]Running tokenizer on dataset: 100%|██████████| 38/38 [00:12<00:00,  3.09ba/s]11/01/2023 17:09:25 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fbad22d1790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]11/01/2023 17:09:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/cache/csv/default-f63ef5233e0bcf01/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  3.78ba/s]Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  3.78ba/s]hi
11/01/2023 17:09:25 - INFO - __main__ - Sample 18024 of the training set: {'sentence1': 'two white dogs are running and jumping along a beach with the ocean behind them. some dogs are running in and out of the ocean waves.', 'sentence2': 'The dogs are looking towards the waves and running in that direction.', 'label': 1, 'input_ids': [0, 7109, 1104, 3678, 32, 878, 8, 9755, 552, 10, 4105, 19, 5, 6444, 639, 106, 4, 103, 3678, 32, 878, 11, 8, 66, 9, 5, 6444, 6995, 4, 2, 2, 133, 3678, 32, 546, 1567, 5, 6995, 8, 878, 11, 14, 2698, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:09:25 - INFO - __main__ - Sample 16049 of the training set: {'sentence1': 'A man wearing a gray shirt, blue jeans and a neon green safety vest is standing on a railroad track with a white truck and a white building in the background. The man is waiting for the train.', 'sentence2': 'The man steps onto the platform and waits.', 'label': 1, 'input_ids': [0, 250, 313, 2498, 10, 12339, 6399, 6, 2440, 10844, 8, 10, 22239, 2272, 1078, 15203, 16, 2934, 15, 10, 17958, 1349, 19, 10, 1104, 2484, 8, 10, 1104, 745, 11, 5, 3618, 4, 20, 313, 16, 2445, 13, 5, 2341, 4, 2, 2, 133, 313, 2402, 2500, 5, 1761, 8, 23120, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/01/2023 17:09:25 - INFO - __main__ - Sample 14628 of the training set: {'sentence1': 'A man is videotaping another man who is beginning to climb up a metal structure. A man making an instructional video', 'sentence2': 'A man below makes notes on a clipboard.', 'label': 1, 'input_ids': [0, 250, 313, 16, 32779, 11470, 277, 313, 54, 16, 1786, 7, 8264, 62, 10, 4204, 3184, 4, 83, 313, 442, 41, 35289, 569, 2, 2, 250, 313, 874, 817, 2775, 15, 10, 44595, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:622] 2023-11-01 17:09:29,374 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-11-01 17:09:29,391 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-11-01 17:09:29,391 >>   Num examples = 37653
[INFO|trainer.py:1421] 2023-11-01 17:09:29,391 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-11-01 17:09:29,391 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-11-01 17:09:29,391 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-11-01 17:09:29,391 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-11-01 17:09:29,391 >>   Total optimization steps = 1178
[WARNING|training_args.py:1095] 2023-11-01 17:09:29,407 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/1178 [00:00<?, ?it/s]  0%|          | 1/1178 [00:00<12:34,  1.56it/s]  0%|          | 2/1178 [00:01<12:31,  1.56it/s]  0%|          | 3/1178 [00:01<12:31,  1.56it/s]  0%|          | 4/1178 [00:02<12:33,  1.56it/s]  0%|          | 5/1178 [00:03<12:32,  1.56it/s]  1%|          | 6/1178 [00:03<12:30,  1.56it/s]  1%|          | 7/1178 [00:04<12:30,  1.56it/s]  1%|          | 8/1178 [00:05<12:29,  1.56it/s]  1%|          | 9/1178 [00:05<12:29,  1.56it/s]  1%|          | 10/1178 [00:06<12:29,  1.56it/s]  1%|          | 11/1178 [00:07<12:27,  1.56it/s]  1%|          | 12/1178 [00:07<12:27,  1.56it/s]  1%|          | 13/1178 [00:08<12:27,  1.56it/s]  1%|          | 14/1178 [00:08<12:27,  1.56it/s]  1%|▏         | 15/1178 [00:09<12:27,  1.56it/s]  1%|▏         | 16/1178 [00:10<12:26,  1.56it/s]  1%|▏         | 17/1178 [00:10<12:26,  1.56it/s]  2%|▏         | 18/1178 [00:11<12:25,  1.56it/s]  2%|▏         | 19/1178 [00:12<12:25,  1.56it/s]  2%|▏         | 20/1178 [00:12<12:25,  1.55it/s]  2%|▏         | 21/1178 [00:13<12:25,  1.55it/s]  2%|▏         | 22/1178 [00:14<12:24,  1.55it/s]  2%|▏         | 23/1178 [00:14<12:24,  1.55it/s]  2%|▏         | 24/1178 [00:15<12:24,  1.55it/s]  2%|▏         | 25/1178 [00:16<12:24,  1.55it/s]  2%|▏         | 26/1178 [00:16<12:24,  1.55it/s]  2%|▏         | 27/1178 [00:17<12:24,  1.55it/s]  2%|▏         | 28/1178 [00:18<12:23,  1.55it/s]  2%|▏         | 29/1178 [00:18<12:23,  1.55it/s]  3%|▎         | 30/1178 [00:19<12:22,  1.55it/s]  3%|▎         | 31/1178 [00:19<12:22,  1.54it/s]  3%|▎         | 32/1178 [00:20<12:22,  1.54it/s]  3%|▎         | 33/1178 [00:21<12:22,  1.54it/s]  3%|▎         | 34/1178 [00:21<12:21,  1.54it/s]  3%|▎         | 35/1178 [00:22<12:21,  1.54it/s]  3%|▎         | 36/1178 [00:23<12:20,  1.54it/s]  3%|▎         | 37/1178 [00:23<12:19,  1.54it/s]  3%|▎         | 38/1178 [00:24<12:19,  1.54it/s]  3%|▎         | 39/1178 [00:25<12:19,  1.54it/s]  3%|▎         | 40/1178 [00:25<12:19,  1.54it/s]  3%|▎         | 41/1178 [00:26<12:17,  1.54it/s]  4%|▎         | 42/1178 [00:27<12:17,  1.54it/s]  4%|▎         | 43/1178 [00:27<12:19,  1.54it/s]  4%|▎         | 44/1178 [00:28<12:19,  1.53it/s]  4%|▍         | 45/1178 [00:29<12:18,  1.53it/s]  4%|▍         | 46/1178 [00:29<12:27,  1.51it/s]  4%|▍         | 47/1178 [00:30<12:28,  1.51it/s]  4%|▍         | 48/1178 [00:31<12:25,  1.52it/s]  4%|▍         | 49/1178 [00:31<12:24,  1.52it/s]  4%|▍         | 50/1178 [00:32<12:35,  1.49it/s]  4%|▍         | 51/1178 [00:33<12:32,  1.50it/s]  4%|▍         | 52/1178 [00:33<12:29,  1.50it/s]  4%|▍         | 53/1178 [00:34<12:27,  1.50it/s]  5%|▍         | 54/1178 [00:35<12:38,  1.48it/s]  5%|▍         | 55/1178 [00:35<12:35,  1.49it/s]  5%|▍         | 56/1178 [00:36<12:32,  1.49it/s]  5%|▍         | 57/1178 [00:37<12:45,  1.47it/s]  5%|▍         | 58/1178 [00:37<12:40,  1.47it/s]  5%|▌         | 59/1178 [00:38<12:33,  1.48it/s]  5%|▌         | 60/1178 [00:39<12:28,  1.49it/s]  5%|▌         | 61/1178 [00:39<12:28,  1.49it/s]  5%|▌         | 62/1178 [00:40<12:22,  1.50it/s]  5%|▌         | 63/1178 [00:41<12:18,  1.51it/s]  5%|▌         | 64/1178 [00:41<12:14,  1.52it/s]  6%|▌         | 65/1178 [00:42<12:22,  1.50it/s]  6%|▌         | 66/1178 [00:43<12:21,  1.50it/s]  6%|▌         | 67/1178 [00:43<12:17,  1.51it/s]  6%|▌         | 68/1178 [00:44<12:15,  1.51it/s]  6%|▌         | 69/1178 [00:45<12:19,  1.50it/s]  6%|▌         | 70/1178 [00:45<12:17,  1.50it/s]  6%|▌         | 71/1178 [00:46<12:14,  1.51it/s]  6%|▌         | 72/1178 [00:47<12:13,  1.51it/s]  6%|▌         | 73/1178 [00:47<12:25,  1.48it/s]  6%|▋         | 74/1178 [00:48<12:22,  1.49it/s]  6%|▋         | 75/1178 [00:49<12:18,  1.49it/s]  6%|▋         | 76/1178 [00:49<12:28,  1.47it/s]  7%|▋         | 77/1178 [00:50<12:26,  1.47it/s]  7%|▋         | 78/1178 [00:51<12:20,  1.49it/s]  7%|▋         | 79/1178 [00:51<12:14,  1.50it/s]  7%|▋         | 80/1178 [00:52<12:24,  1.47it/s]  7%|▋         | 81/1178 [00:53<12:23,  1.48it/s]  7%|▋         | 82/1178 [00:53<12:17,  1.49it/s]  7%|▋         | 83/1178 [00:54<12:23,  1.47it/s]  7%|▋         | 84/1178 [00:55<12:19,  1.48it/s]  7%|▋         | 85/1178 [00:55<12:15,  1.49it/s]  7%|▋         | 86/1178 [00:56<12:26,  1.46it/s]  7%|▋         | 87/1178 [00:57<12:21,  1.47it/s]  7%|▋         | 88/1178 [00:57<12:14,  1.48it/s]  8%|▊         | 89/1178 [00:58<12:15,  1.48it/s]  8%|▊         | 90/1178 [00:59<12:10,  1.49it/s]  8%|▊         | 91/1178 [00:59<12:06,  1.50it/s]  8%|▊         | 92/1178 [01:00<12:04,  1.50it/s]  8%|▊         | 93/1178 [01:01<12:16,  1.47it/s]  8%|▊         | 94/1178 [01:01<12:15,  1.47it/s]  8%|▊         | 95/1178 [01:02<12:10,  1.48it/s]  8%|▊         | 96/1178 [01:03<12:12,  1.48it/s]  8%|▊         | 97/1178 [01:03<12:05,  1.49it/s]  8%|▊         | 98/1178 [01:04<12:00,  1.50it/s]  8%|▊         | 99/1178 [01:05<11:57,  1.50it/s]  8%|▊         | 100/1178 [01:05<12:07,  1.48it/s]  9%|▊         | 101/1178 [01:06<12:07,  1.48it/s]  9%|▊         | 102/1178 [01:07<12:04,  1.49it/s]  9%|▊         | 103/1178 [01:08<12:08,  1.48it/s]  9%|▉         | 104/1178 [01:08<12:00,  1.49it/s]  9%|▉         | 105/1178 [01:09<11:54,  1.50it/s]  9%|▉         | 106/1178 [01:09<11:50,  1.51it/s]  9%|▉         | 107/1178 [01:10<11:46,  1.52it/s]  9%|▉         | 108/1178 [01:11<11:51,  1.50it/s]  9%|▉         | 109/1178 [01:11<11:49,  1.51it/s]  9%|▉         | 110/1178 [01:12<11:47,  1.51it/s]  9%|▉         | 111/1178 [01:13<11:47,  1.51it/s] 10%|▉         | 112/1178 [01:14<11:57,  1.49it/s] 10%|▉         | 113/1178 [01:14<11:52,  1.49it/s] 10%|▉         | 114/1178 [01:15<11:49,  1.50it/s] 10%|▉         | 115/1178 [01:16<11:58,  1.48it/s] 10%|▉         | 116/1178 [01:16<11:56,  1.48it/s] 10%|▉         | 117/1178 [01:17<11:50,  1.49it/s] 10%|█         | 118/1178 [01:18<11:47,  1.50it/s] 10%|█         | 119/1178 [01:18<11:58,  1.47it/s] 10%|█         | 120/1178 [01:19<11:56,  1.48it/s] 10%|█         | 121/1178 [01:20<11:50,  1.49it/s] 10%|█         | 122/1178 [01:20<11:51,  1.48it/s] 10%|█         | 123/1178 [01:21<11:45,  1.50it/s] 11%|█         | 124/1178 [01:22<11:38,  1.51it/s] 11%|█         | 125/1178 [01:22<11:35,  1.51it/s] 11%|█         | 126/1178 [01:23<11:33,  1.52it/s] 11%|█         | 127/1178 [01:24<11:40,  1.50it/s] 11%|█         | 128/1178 [01:24<11:38,  1.50it/s] 11%|█         | 129/1178 [01:25<11:34,  1.51it/s] 11%|█         | 130/1178 [01:26<11:33,  1.51it/s] 11%|█         | 131/1178 [01:26<11:37,  1.50it/s] 11%|█         | 132/1178 [01:27<11:35,  1.50it/s] 11%|█▏        | 133/1178 [01:28<11:31,  1.51it/s] 11%|█▏        | 134/1178 [01:28<11:29,  1.51it/s] 11%|█▏        | 135/1178 [01:29<11:40,  1.49it/s] 12%|█▏        | 136/1178 [01:30<11:42,  1.48it/s] 12%|█▏        | 137/1178 [01:30<11:39,  1.49it/s] 12%|█▏        | 138/1178 [01:31<11:46,  1.47it/s] 12%|█▏        | 139/1178 [01:32<11:40,  1.48it/s] 12%|█▏        | 140/1178 [01:32<11:34,  1.49it/s] 12%|█▏        | 141/1178 [01:33<11:30,  1.50it/s] 12%|█▏        | 142/1178 [01:34<11:39,  1.48it/s] 12%|█▏        | 143/1178 [01:34<11:42,  1.47it/s] 12%|█▏        | 144/1178 [01:35<11:37,  1.48it/s] 12%|█▏        | 145/1178 [01:36<11:41,  1.47it/s] 12%|█▏        | 146/1178 [01:36<11:37,  1.48it/s] 12%|█▏        | 147/1178 [01:37<11:33,  1.49it/s] 13%|█▎        | 148/1178 [01:38<11:42,  1.47it/s] 13%|█▎        | 149/1178 [01:38<11:39,  1.47it/s] 13%|█▎        | 150/1178 [01:39<11:32,  1.48it/s] 13%|█▎        | 151/1178 [01:40<11:27,  1.49it/s] 13%|█▎        | 152/1178 [01:40<11:33,  1.48it/s] 13%|█▎        | 153/1178 [01:41<11:33,  1.48it/s] 13%|█▎        | 154/1178 [01:42<11:27,  1.49it/s] 13%|█▎        | 155/1178 [01:42<11:35,  1.47it/s] 13%|█▎        | 156/1178 [01:43<11:35,  1.47it/s] 13%|█▎        | 157/1178 [01:44<11:31,  1.48it/s] 13%|█▎        | 158/1178 [01:44<11:38,  1.46it/s] 13%|█▎        | 159/1178 [01:45<11:34,  1.47it/s] 14%|█▎        | 160/1178 [01:46<11:28,  1.48it/s] 14%|█▎        | 161/1178 [01:46<11:37,  1.46it/s] 14%|█▍        | 162/1178 [01:47<11:33,  1.46it/s] 14%|█▍        | 163/1178 [01:48<11:28,  1.48it/s] 14%|█▍        | 164/1178 [01:49<11:33,  1.46it/s] 14%|█▍        | 165/1178 [01:49<11:24,  1.48it/s] 14%|█▍        | 166/1178 [01:50<11:18,  1.49it/s] 14%|█▍        | 167/1178 [01:50<11:14,  1.50it/s] 14%|█▍        | 168/1178 [01:51<11:20,  1.48it/s] 14%|█▍        | 169/1178 [01:52<11:13,  1.50it/s] 14%|█▍        | 170/1178 [01:52<11:08,  1.51it/s] 15%|█▍        | 171/1178 [01:53<11:06,  1.51it/s] 15%|█▍        | 172/1178 [01:54<11:10,  1.50it/s] 15%|█▍        | 173/1178 [01:54<11:08,  1.50it/s] 15%|█▍        | 174/1178 [01:55<11:05,  1.51it/s] 15%|█▍        | 175/1178 [01:56<11:02,  1.51it/s] 15%|█▍        | 176/1178 [01:56<11:07,  1.50it/s] 15%|█▌        | 177/1178 [01:57<11:05,  1.50it/s] 15%|█▌        | 178/1178 [01:58<11:01,  1.51it/s] 15%|█▌        | 179/1178 [01:58<11:01,  1.51it/s] 15%|█▌        | 180/1178 [01:59<11:03,  1.50it/s] 15%|█▌        | 181/1178 [02:00<11:01,  1.51it/s] 15%|█▌        | 182/1178 [02:00<10:57,  1.51it/s] 16%|█▌        | 183/1178 [02:01<10:54,  1.52it/s] 16%|█▌        | 184/1178 [02:02<10:52,  1.52it/s] 16%|█▌        | 185/1178 [02:02<10:59,  1.50it/s] 16%|█▌        | 186/1178 [02:03<10:58,  1.51it/s] 16%|█▌        | 187/1178 [02:04<10:56,  1.51it/s] 16%|█▌        | 188/1178 [02:04<10:55,  1.51it/s] 16%|█▌        | 189/1178 [02:05<11:03,  1.49it/s] 16%|█▌        | 190/1178 [02:06<10:59,  1.50it/s] 16%|█▌        | 191/1178 [02:06<10:54,  1.51it/s] 16%|█▋        | 192/1178 [02:07<10:51,  1.51it/s] 16%|█▋        | 193/1178 [02:08<11:00,  1.49it/s] 16%|█▋        | 194/1178 [02:08<10:58,  1.49it/s] 17%|█▋        | 195/1178 [02:09<10:53,  1.50it/s] 17%|█▋        | 196/1178 [02:10<10:51,  1.51it/s] 17%|█▋        | 197/1178 [02:10<10:57,  1.49it/s] 17%|█▋        | 198/1178 [02:11<10:57,  1.49it/s] 17%|█▋        | 199/1178 [02:12<10:54,  1.50it/s] 17%|█▋        | 200/1178 [02:12<11:03,  1.47it/s] 17%|█▋        | 201/1178 [02:13<11:03,  1.47it/s] 17%|█▋        | 202/1178 [02:14<10:59,  1.48it/s] 17%|█▋        | 203/1178 [02:14<10:52,  1.49it/s] 17%|█▋        | 204/1178 [02:15<10:55,  1.49it/s] 17%|█▋        | 205/1178 [02:16<10:53,  1.49it/s] 17%|█▋        | 206/1178 [02:16<10:49,  1.50it/s] 18%|█▊        | 207/1178 [02:17<10:56,  1.48it/s] 18%|█▊        | 208/1178 [02:18<10:55,  1.48it/s] 18%|█▊        | 209/1178 [02:19<10:51,  1.49it/s] 18%|█▊        | 210/1178 [02:19<10:47,  1.50it/s] 18%|█▊        | 211/1178 [02:20<10:50,  1.49it/s] 18%|█▊        | 212/1178 [02:21<10:46,  1.49it/s] 18%|█▊        | 213/1178 [02:21<10:40,  1.51it/s] 18%|█▊        | 214/1178 [02:22<10:36,  1.51it/s] 18%|█▊        | 215/1178 [02:22<10:39,  1.51it/s] 18%|█▊        | 216/1178 [02:23<10:37,  1.51it/s] 18%|█▊        | 217/1178 [02:24<10:34,  1.51it/s] 19%|█▊        | 218/1178 [02:24<10:34,  1.51it/s] 19%|█▊        | 219/1178 [02:25<10:44,  1.49it/s] 19%|█▊        | 220/1178 [02:26<10:40,  1.49it/s] 19%|█▉        | 221/1178 [02:26<10:35,  1.51it/s] 19%|█▉        | 222/1178 [02:27<10:32,  1.51it/s] 19%|█▉        | 223/1178 [02:28<10:40,  1.49it/s] 19%|█▉        | 224/1178 [02:29<10:45,  1.48it/s] 19%|█▉        | 225/1178 [02:29<10:44,  1.48it/s] 19%|█▉        | 226/1178 [02:30<10:37,  1.49it/s] 19%|█▉        | 227/1178 [02:30<10:31,  1.51it/s] 19%|█▉        | 228/1178 [02:31<10:27,  1.51it/s] 19%|█▉        | 229/1178 [02:32<10:26,  1.52it/s] 20%|█▉        | 230/1178 [02:32<10:24,  1.52it/s] 20%|█▉        | 231/1178 [02:33<10:28,  1.51it/s] 20%|█▉        | 232/1178 [02:34<10:26,  1.51it/s] 20%|█▉        | 233/1178 [02:34<10:23,  1.52it/s] 20%|█▉        | 234/1178 [02:35<10:23,  1.51it/s] 20%|█▉        | 235/1178 [02:36<10:36,  1.48it/s] 20%|██        | 236/1178 [02:37<10:39,  1.47it/s] 20%|██        | 237/1178 [02:37<10:35,  1.48it/s] 20%|██        | 238/1178 [02:38<10:41,  1.47it/s] 20%|██        | 239/1178 [02:39<10:32,  1.48it/s] 20%|██        | 240/1178 [02:39<10:26,  1.50it/s] 20%|██        | 241/1178 [02:40<10:23,  1.50it/s] 21%|██        | 242/1178 [02:41<10:29,  1.49it/s] 21%|██        | 243/1178 [02:41<10:32,  1.48it/s] 21%|██        | 244/1178 [02:42<10:30,  1.48it/s] 21%|██        | 245/1178 [02:43<10:40,  1.46it/s] 21%|██        | 246/1178 [02:43<10:47,  1.44it/s] 21%|██        | 247/1178 [02:44<10:49,  1.43it/s] 21%|██        | 248/1178 [02:45<10:43,  1.45it/s] 21%|██        | 249/1178 [02:45<10:34,  1.47it/s] 21%|██        | 250/1178 [02:46<10:26,  1.48it/s] 21%|██▏       | 251/1178 [02:47<10:24,  1.48it/s] 21%|██▏       | 252/1178 [02:47<10:18,  1.50it/s] 21%|██▏       | 253/1178 [02:48<10:16,  1.50it/s] 22%|██▏       | 254/1178 [02:49<10:13,  1.51it/s] 22%|██▏       | 255/1178 [02:49<10:19,  1.49it/s] 22%|██▏       | 256/1178 [02:50<10:16,  1.49it/s] 22%|██▏       | 257/1178 [02:51<10:14,  1.50it/s] 22%|██▏       | 258/1178 [02:51<10:10,  1.51it/s] 22%|██▏       | 259/1178 [02:52<10:13,  1.50it/s] 22%|██▏       | 260/1178 [02:53<10:10,  1.50it/s] 22%|██▏       | 261/1178 [02:53<10:06,  1.51it/s] 22%|██▏       | 262/1178 [02:54<10:04,  1.52it/s] 22%|██▏       | 263/1178 [02:55<10:11,  1.50it/s] 22%|██▏       | 264/1178 [02:55<10:07,  1.51it/s] 22%|██▏       | 265/1178 [02:56<10:04,  1.51it/s] 23%|██▎       | 266/1178 [02:57<10:03,  1.51it/s] 23%|██▎       | 267/1178 [02:57<10:14,  1.48it/s] 23%|██▎       | 268/1178 [02:58<10:14,  1.48it/s] 23%|██▎       | 269/1178 [02:59<10:11,  1.49it/s] 23%|██▎       | 270/1178 [02:59<10:17,  1.47it/s] 23%|██▎       | 271/1178 [03:00<10:10,  1.48it/s] 23%|██▎       | 272/1178 [03:01<10:06,  1.49it/s] 23%|██▎       | 273/1178 [03:01<10:01,  1.50it/s] 23%|██▎       | 274/1178 [03:02<10:07,  1.49it/s] 23%|██▎       | 275/1178 [03:03<10:06,  1.49it/s] 23%|██▎       | 276/1178 [03:03<10:02,  1.50it/s] 24%|██▎       | 277/1178 [03:04<10:01,  1.50it/s] 24%|██▎       | 278/1178 [03:05<10:04,  1.49it/s] 24%|██▎       | 279/1178 [03:05<10:00,  1.50it/s] 24%|██▍       | 280/1178 [03:06<09:55,  1.51it/s] 24%|██▍       | 281/1178 [03:07<09:59,  1.50it/s] 24%|██▍       | 282/1178 [03:07<09:56,  1.50it/s] 24%|██▍       | 283/1178 [03:08<09:54,  1.50it/s] 24%|██▍       | 284/1178 [03:09<09:51,  1.51it/s] 24%|██▍       | 285/1178 [03:09<09:54,  1.50it/s] 24%|██▍       | 286/1178 [03:10<09:51,  1.51it/s] 24%|██▍       | 287/1178 [03:11<09:47,  1.52it/s] 24%|██▍       | 288/1178 [03:11<09:46,  1.52it/s] 25%|██▍       | 289/1178 [03:12<09:45,  1.52it/s] 25%|██▍       | 290/1178 [03:13<09:51,  1.50it/s] 25%|██▍       | 291/1178 [03:13<09:48,  1.51it/s] 25%|██▍       | 292/1178 [03:14<09:46,  1.51it/s] 25%|██▍       | 293/1178 [03:15<09:44,  1.51it/s] 25%|██▍       | 294/1178 [03:15<09:47,  1.50it/s] 25%|██▌       | 295/1178 [03:16<09:47,  1.50it/s] 25%|██▌       | 296/1178 [03:17<09:44,  1.51it/s] 25%|██▌       | 297/1178 [03:17<09:42,  1.51it/s] 25%|██▌       | 298/1178 [03:18<09:43,  1.51it/s] 25%|██▌       | 299/1178 [03:19<09:49,  1.49it/s] 25%|██▌       | 300/1178 [03:19<09:48,  1.49it/s] 26%|██▌       | 301/1178 [03:20<09:43,  1.50it/s] 26%|██▌       | 302/1178 [03:21<09:45,  1.50it/s] 26%|██▌       | 303/1178 [03:21<09:43,  1.50it/s] 26%|██▌       | 304/1178 [03:22<09:40,  1.51it/s] 26%|██▌       | 305/1178 [03:23<09:36,  1.51it/s] 26%|██▌       | 306/1178 [03:23<09:38,  1.51it/s] 26%|██▌       | 307/1178 [03:24<09:37,  1.51it/s] 26%|██▌       | 308/1178 [03:25<09:36,  1.51it/s] 26%|██▌       | 309/1178 [03:25<09:37,  1.51it/s] 26%|██▋       | 310/1178 [03:26<09:45,  1.48it/s] 26%|██▋       | 311/1178 [03:27<09:45,  1.48it/s] 26%|██▋       | 312/1178 [03:27<09:40,  1.49it/s] 27%|██▋       | 313/1178 [03:28<09:38,  1.49it/s] 27%|██▋       | 314/1178 [03:29<09:47,  1.47it/s] 27%|██▋       | 315/1178 [03:29<09:44,  1.48it/s] 27%|██▋       | 316/1178 [03:30<09:38,  1.49it/s] 27%|██▋       | 317/1178 [03:31<09:41,  1.48it/s] 27%|██▋       | 318/1178 [03:31<09:36,  1.49it/s] 27%|██▋       | 319/1178 [03:32<09:30,  1.50it/s] 27%|██▋       | 320/1178 [03:33<09:28,  1.51it/s] 27%|██▋       | 321/1178 [03:33<09:34,  1.49it/s] 27%|██▋       | 322/1178 [03:34<09:35,  1.49it/s] 27%|██▋       | 323/1178 [03:35<09:33,  1.49it/s] 28%|██▊       | 324/1178 [03:35<09:41,  1.47it/s] 28%|██▊       | 325/1178 [03:36<09:38,  1.47it/s] 28%|██▊       | 326/1178 [03:37<09:32,  1.49it/s] 28%|██▊       | 327/1178 [03:37<09:28,  1.50it/s] 28%|██▊       | 328/1178 [03:38<09:28,  1.50it/s] 28%|██▊       | 329/1178 [03:39<09:25,  1.50it/s] 28%|██▊       | 330/1178 [03:39<09:23,  1.50it/s] 28%|██▊       | 331/1178 [03:40<09:20,  1.51it/s] 28%|██▊       | 332/1178 [03:41<09:23,  1.50it/s] 28%|██▊       | 333/1178 [03:41<09:21,  1.50it/s] 28%|██▊       | 334/1178 [03:42<09:19,  1.51it/s] 28%|██▊       | 335/1178 [03:43<09:18,  1.51it/s] 29%|██▊       | 336/1178 [03:43<09:26,  1.49it/s] 29%|██▊       | 337/1178 [03:44<09:23,  1.49it/s] 29%|██▊       | 338/1178 [03:45<09:20,  1.50it/s] 29%|██▉       | 339/1178 [03:45<09:20,  1.50it/s] 29%|██▉       | 340/1178 [03:46<09:26,  1.48it/s] 29%|██▉       | 341/1178 [03:47<09:23,  1.48it/s] 29%|██▉       | 342/1178 [03:47<09:20,  1.49it/s] 29%|██▉       | 343/1178 [03:48<09:28,  1.47it/s] 29%|██▉       | 344/1178 [03:49<09:28,  1.47it/s] 29%|██▉       | 345/1178 [03:49<09:22,  1.48it/s] 29%|██▉       | 346/1178 [03:50<09:29,  1.46it/s] 29%|██▉       | 347/1178 [03:51<09:26,  1.47it/s] 30%|██▉       | 348/1178 [03:52<09:20,  1.48it/s] 30%|██▉       | 349/1178 [03:52<09:27,  1.46it/s] 30%|██▉       | 350/1178 [03:53<09:22,  1.47it/s] 30%|██▉       | 351/1178 [03:54<09:16,  1.49it/s] 30%|██▉       | 352/1178 [03:54<09:14,  1.49it/s] 30%|██▉       | 353/1178 [03:55<09:11,  1.50it/s] 30%|███       | 354/1178 [03:56<09:07,  1.50it/s] 30%|███       | 355/1178 [03:56<09:05,  1.51it/s] 30%|███       | 356/1178 [03:57<09:03,  1.51it/s] 30%|███       | 357/1178 [03:58<09:11,  1.49it/s] 30%|███       | 358/1178 [03:58<09:12,  1.48it/s] 30%|███       | 359/1178 [03:59<09:10,  1.49it/s] 31%|███       | 360/1178 [04:00<09:18,  1.47it/s] 31%|███       | 361/1178 [04:00<09:21,  1.45it/s] 31%|███       | 362/1178 [04:01<09:17,  1.46it/s] 31%|███       | 363/1178 [04:02<09:13,  1.47it/s] 31%|███       | 364/1178 [04:02<09:06,  1.49it/s] 31%|███       | 365/1178 [04:03<09:02,  1.50it/s] 31%|███       | 366/1178 [04:04<09:00,  1.50it/s] 31%|███       | 367/1178 [04:04<09:03,  1.49it/s] 31%|███       | 368/1178 [04:05<09:00,  1.50it/s] 31%|███▏      | 369/1178 [04:06<08:57,  1.51it/s] 31%|███▏      | 370/1178 [04:06<08:53,  1.51it/s] 31%|███▏      | 371/1178 [04:07<08:55,  1.51it/s] 32%|███▏      | 372/1178 [04:08<08:54,  1.51it/s] 32%|███▏      | 373/1178 [04:08<08:53,  1.51it/s] 32%|███▏      | 374/1178 [04:09<08:52,  1.51it/s] 32%|███▏      | 375/1178 [04:10<09:01,  1.48it/s] 32%|███▏      | 376/1178 [04:10<09:00,  1.48it/s] 32%|███▏      | 377/1178 [04:11<08:56,  1.49it/s] 32%|███▏      | 378/1178 [04:12<08:51,  1.51it/s] 32%|███▏      | 379/1178 [04:12<08:53,  1.50it/s] 32%|███▏      | 380/1178 [04:13<08:51,  1.50it/s] 32%|███▏      | 381/1178 [04:14<08:48,  1.51it/s] 32%|███▏      | 382/1178 [04:14<08:44,  1.52it/s] 33%|███▎      | 383/1178 [04:15<08:50,  1.50it/s] 33%|███▎      | 384/1178 [04:16<08:48,  1.50it/s] 33%|███▎      | 385/1178 [04:16<08:46,  1.50it/s] 33%|███▎      | 386/1178 [04:17<08:46,  1.51it/s] 33%|███▎      | 387/1178 [04:18<08:50,  1.49it/s] 33%|███▎      | 388/1178 [04:18<08:48,  1.49it/s] 33%|███▎      | 389/1178 [04:19<08:44,  1.50it/s] 33%|███▎      | 390/1178 [04:20<08:43,  1.51it/s] 33%|███▎      | 391/1178 [04:20<08:50,  1.48it/s] 33%|███▎      | 392/1178 [04:21<08:47,  1.49it/s] 33%|███▎      | 393/1178 [04:22<08:44,  1.50it/s] 33%|███▎      | 394/1178 [04:22<08:49,  1.48it/s] 34%|███▎      | 395/1178 [04:23<08:46,  1.49it/s] 34%|███▎      | 396/1178 [04:24<08:42,  1.50it/s] 34%|███▎      | 397/1178 [04:24<08:38,  1.51it/s] 34%|███▍      | 398/1178 [04:25<08:44,  1.49it/s] 34%|███▍      | 399/1178 [04:26<08:40,  1.50it/s] 34%|███▍      | 400/1178 [04:26<08:37,  1.50it/s] 34%|███▍      | 401/1178 [04:27<08:34,  1.51it/s] 34%|███▍      | 402/1178 [04:28<08:40,  1.49it/s] 34%|███▍      | 403/1178 [04:28<08:40,  1.49it/s] 34%|███▍      | 404/1178 [04:29<08:36,  1.50it/s] 34%|███▍      | 405/1178 [04:30<08:32,  1.51it/s] 34%|███▍      | 406/1178 [04:30<08:36,  1.49it/s] 35%|███▍      | 407/1178 [04:31<08:34,  1.50it/s] 35%|███▍      | 408/1178 [04:32<08:31,  1.51it/s] 35%|███▍      | 409/1178 [04:32<08:30,  1.51it/s] 35%|███▍      | 410/1178 [04:33<08:37,  1.48it/s] 35%|███▍      | 411/1178 [04:34<08:35,  1.49it/s] 35%|███▍      | 412/1178 [04:34<08:32,  1.49it/s] 35%|███▌      | 413/1178 [04:35<08:40,  1.47it/s] 35%|███▌      | 414/1178 [04:36<08:38,  1.47it/s] 35%|███▌      | 415/1178 [04:36<08:34,  1.48it/s] 35%|███▌      | 416/1178 [04:37<08:37,  1.47it/s] 35%|███▌      | 417/1178 [04:38<08:32,  1.49it/s] 35%|███▌      | 418/1178 [04:38<08:27,  1.50it/s] 36%|███▌      | 419/1178 [04:39<08:23,  1.51it/s] 36%|███▌      | 420/1178 [04:40<08:23,  1.51it/s] 36%|███▌      | 421/1178 [04:40<08:23,  1.50it/s] 36%|███▌      | 422/1178 [04:41<08:20,  1.51it/s] 36%|███▌      | 423/1178 [04:42<08:18,  1.51it/s] 36%|███▌      | 424/1178 [04:42<08:19,  1.51it/s] 36%|███▌      | 425/1178 [04:43<08:26,  1.49it/s] 36%|███▌      | 426/1178 [04:44<08:22,  1.50it/s] 36%|███▌      | 427/1178 [04:44<08:20,  1.50it/s] 36%|███▋      | 428/1178 [04:45<08:21,  1.50it/s] 36%|███▋      | 429/1178 [04:46<08:18,  1.50it/s] 37%|███▋      | 430/1178 [04:46<08:14,  1.51it/s] 37%|███▋      | 431/1178 [04:47<08:12,  1.52it/s] 37%|███▋      | 432/1178 [04:48<08:12,  1.51it/s] 37%|███▋      | 433/1178 [04:48<08:16,  1.50it/s] 37%|███▋      | 434/1178 [04:49<08:14,  1.51it/s] 37%|███▋      | 435/1178 [04:50<08:11,  1.51it/s] 37%|███▋      | 436/1178 [04:50<08:09,  1.52it/s] 37%|███▋      | 437/1178 [04:51<08:11,  1.51it/s] 37%|███▋      | 438/1178 [04:52<08:10,  1.51it/s] 37%|███▋      | 439/1178 [04:52<08:09,  1.51it/s] 37%|███▋      | 440/1178 [04:53<08:08,  1.51it/s] 37%|███▋      | 441/1178 [04:54<08:15,  1.49it/s] 38%|███▊      | 442/1178 [04:54<08:13,  1.49it/s] 38%|███▊      | 443/1178 [04:55<08:11,  1.50it/s] 38%|███▊      | 444/1178 [04:56<08:07,  1.51it/s] 38%|███▊      | 445/1178 [04:56<08:09,  1.50it/s] 38%|███▊      | 446/1178 [04:57<08:07,  1.50it/s] 38%|███▊      | 447/1178 [04:58<08:05,  1.51it/s] 38%|███▊      | 448/1178 [04:58<08:02,  1.51it/s] 38%|███▊      | 449/1178 [04:59<08:05,  1.50it/s] 38%|███▊      | 450/1178 [05:00<08:04,  1.50it/s] 38%|███▊      | 451/1178 [05:00<08:02,  1.51it/s] 38%|███▊      | 452/1178 [05:01<08:00,  1.51it/s] 38%|███▊      | 453/1178 [05:02<08:07,  1.49it/s] 39%|███▊      | 454/1178 [05:02<08:08,  1.48it/s] 39%|███▊      | 455/1178 [05:03<08:03,  1.49it/s] 39%|███▊      | 456/1178 [05:04<08:04,  1.49it/s] 39%|███▉      | 457/1178 [05:04<08:03,  1.49it/s] 39%|███▉      | 458/1178 [05:05<08:00,  1.50it/s] 39%|███▉      | 459/1178 [05:06<07:58,  1.50it/s] 39%|███▉      | 460/1178 [05:06<08:05,  1.48it/s] 39%|███▉      | 461/1178 [05:07<08:03,  1.48it/s] 39%|███▉      | 462/1178 [05:08<08:00,  1.49it/s] 39%|███▉      | 463/1178 [05:08<07:56,  1.50it/s] 39%|███▉      | 464/1178 [05:09<07:57,  1.49it/s] 39%|███▉      | 465/1178 [05:10<07:55,  1.50it/s] 40%|███▉      | 466/1178 [05:10<07:52,  1.51it/s] 40%|███▉      | 467/1178 [05:11<07:49,  1.51it/s] 40%|███▉      | 468/1178 [05:12<07:53,  1.50it/s] 40%|███▉      | 469/1178 [05:12<07:51,  1.50it/s] 40%|███▉      | 470/1178 [05:13<07:49,  1.51it/s] 40%|███▉      | 471/1178 [05:14<07:49,  1.51it/s] 40%|████      | 472/1178 [05:14<07:52,  1.50it/s] 40%|████      | 473/1178 [05:15<07:50,  1.50it/s] 40%|████      | 474/1178 [05:16<07:47,  1.51it/s] 40%|████      | 475/1178 [05:16<07:46,  1.51it/s] 40%|████      | 476/1178 [05:17<07:54,  1.48it/s] 40%|████      | 477/1178 [05:18<07:52,  1.48it/s] 41%|████      | 478/1178 [05:18<07:49,  1.49it/s] 41%|████      | 479/1178 [05:19<07:56,  1.47it/s] 41%|████      | 480/1178 [05:20<07:54,  1.47it/s] 41%|████      | 481/1178 [05:20<07:51,  1.48it/s] 41%|████      | 482/1178 [05:21<07:55,  1.47it/s] 41%|████      | 483/1178 [05:22<07:50,  1.48it/s] 41%|████      | 484/1178 [05:22<07:45,  1.49it/s] 41%|████      | 485/1178 [05:23<07:42,  1.50it/s] 41%|████▏     | 486/1178 [05:24<07:47,  1.48it/s] 41%|████▏     | 487/1178 [05:24<07:46,  1.48it/s] 41%|████▏     | 488/1178 [05:25<07:43,  1.49it/s] 42%|████▏     | 489/1178 [05:26<07:49,  1.47it/s] 42%|████▏     | 490/1178 [05:26<07:46,  1.48it/s] 42%|████▏     | 491/1178 [05:27<07:42,  1.49it/s] 42%|████▏     | 492/1178 [05:28<07:37,  1.50it/s] 42%|████▏     | 493/1178 [05:28<07:40,  1.49it/s] 42%|████▏     | 494/1178 [05:29<07:40,  1.48it/s] 42%|████▏     | 495/1178 [05:30<07:38,  1.49it/s] 42%|████▏     | 496/1178 [05:31<07:44,  1.47it/s] 42%|████▏     | 497/1178 [05:31<07:43,  1.47it/s] 42%|████▏     | 498/1178 [05:32<07:40,  1.48it/s] 42%|████▏     | 499/1178 [05:33<07:44,  1.46it/s] 42%|████▏     | 500/1178 [05:33<07:42,  1.46it/s]                                                   42%|████▏     | 500/1178 [05:33<07:42,  1.46it/s] 43%|████▎     | 501/1178 [05:34<07:41,  1.47it/s] 43%|████▎     | 502/1178 [05:35<07:40,  1.47it/s] 43%|████▎     | 503/1178 [05:35<07:35,  1.48it/s] 43%|████▎     | 504/1178 [05:36<07:30,  1.50it/s] 43%|████▎     | 505/1178 [05:37<07:27,  1.50it/s] 43%|████▎     | 506/1178 [05:37<07:33,  1.48it/s] 43%|████▎     | 507/1178 [05:38<07:33,  1.48it/s] 43%|████▎     | 508/1178 [05:39<07:29,  1.49it/s] 43%|████▎     | 509/1178 [05:39<07:35,  1.47it/s] 43%|████▎     | 510/1178 [05:40<07:33,  1.47it/s] 43%|████▎     | 511/1178 [05:41<07:29,  1.48it/s] 43%|████▎     | 512/1178 [05:41<07:32,  1.47it/s] 44%|████▎     | 513/1178 [05:42<07:27,  1.49it/s] 44%|████▎     | 514/1178 [05:43<07:22,  1.50it/s] 44%|████▎     | 515/1178 [05:43<07:19,  1.51it/s] 44%|████▍     | 516/1178 [05:44<07:18,  1.51it/s] 44%|████▍     | 517/1178 [05:45<07:22,  1.49it/s] 44%|████▍     | 518/1178 [05:45<07:18,  1.50it/s] 44%|████▍     | 519/1178 [05:46<07:16,  1.51it/s] 44%|████▍     | 520/1178 [05:47<07:14,  1.52it/s] 44%|████▍     | 521/1178 [05:47<07:15,  1.51it/s] 44%|████▍     | 522/1178 [05:48<07:14,  1.51it/s] 44%|████▍     | 523/1178 [05:49<07:12,  1.52it/s] 44%|████▍     | 524/1178 [05:49<07:11,  1.52it/s] 45%|████▍     | 525/1178 [05:50<07:11,  1.51it/s] 45%|████▍     | 526/1178 [05:51<07:09,  1.52it/s] 45%|████▍     | 527/1178 [05:51<07:08,  1.52it/s] 45%|████▍     | 528/1178 [05:52<07:08,  1.52it/s] 45%|████▍     | 529/1178 [05:53<07:07,  1.52it/s] 45%|████▍     | 530/1178 [05:53<07:11,  1.50it/s] 45%|████▌     | 531/1178 [05:54<07:09,  1.51it/s] 45%|████▌     | 532/1178 [05:55<07:08,  1.51it/s] 45%|████▌     | 533/1178 [05:55<07:07,  1.51it/s] 45%|████▌     | 534/1178 [05:56<07:14,  1.48it/s] 45%|████▌     | 535/1178 [05:57<07:13,  1.48it/s] 46%|████▌     | 536/1178 [05:57<07:10,  1.49it/s] 46%|████▌     | 537/1178 [05:58<07:13,  1.48it/s] 46%|████▌     | 538/1178 [05:59<07:09,  1.49it/s] 46%|████▌     | 539/1178 [05:59<07:06,  1.50it/s] 46%|████▌     | 540/1178 [06:00<07:04,  1.50it/s] 46%|████▌     | 541/1178 [06:01<07:09,  1.48it/s] 46%|████▌     | 542/1178 [06:01<07:10,  1.48it/s] 46%|████▌     | 543/1178 [06:02<07:07,  1.49it/s] 46%|████▌     | 544/1178 [06:03<07:13,  1.46it/s] 46%|████▋     | 545/1178 [06:03<07:11,  1.47it/s] 46%|████▋     | 546/1178 [06:04<07:06,  1.48it/s] 46%|████▋     | 547/1178 [06:05<07:11,  1.46it/s] 47%|████▋     | 548/1178 [06:05<07:08,  1.47it/s] 47%|████▋     | 549/1178 [06:06<07:02,  1.49it/s] 47%|████▋     | 550/1178 [06:07<07:00,  1.49it/s] 47%|████▋     | 551/1178 [06:07<07:04,  1.48it/s] 47%|████▋     | 552/1178 [06:08<07:01,  1.49it/s] 47%|████▋     | 553/1178 [06:09<06:57,  1.50it/s] 47%|████▋     | 554/1178 [06:09<06:59,  1.49it/s] 47%|████▋     | 555/1178 [06:10<06:56,  1.50it/s] 47%|████▋     | 556/1178 [06:11<06:52,  1.51it/s] 47%|████▋     | 557/1178 [06:11<06:50,  1.51it/s] 47%|████▋     | 558/1178 [06:12<06:48,  1.52it/s] 47%|████▋     | 559/1178 [06:13<06:51,  1.50it/s] 48%|████▊     | 560/1178 [06:13<06:50,  1.51it/s] 48%|████▊     | 561/1178 [06:14<06:49,  1.51it/s] 48%|████▊     | 562/1178 [06:15<06:46,  1.51it/s] 48%|████▊     | 563/1178 [06:15<06:48,  1.51it/s] 48%|████▊     | 564/1178 [06:16<06:47,  1.51it/s] 48%|████▊     | 565/1178 [06:17<06:45,  1.51it/s] 48%|████▊     | 566/1178 [06:17<06:44,  1.51it/s] 48%|████▊     | 567/1178 [06:18<06:52,  1.48it/s] 48%|████▊     | 568/1178 [06:19<06:51,  1.48it/s] 48%|████▊     | 569/1178 [06:19<06:49,  1.49it/s] 48%|████▊     | 570/1178 [06:20<06:53,  1.47it/s] 48%|████▊     | 571/1178 [06:21<06:49,  1.48it/s] 49%|████▊     | 572/1178 [06:21<06:46,  1.49it/s] 49%|████▊     | 573/1178 [06:22<06:42,  1.50it/s] 49%|████▊     | 574/1178 [06:23<06:46,  1.48it/s] 49%|████▉     | 575/1178 [06:23<06:45,  1.49it/s] 49%|████▉     | 576/1178 [06:24<06:41,  1.50it/s] 49%|████▉     | 577/1178 [06:25<06:42,  1.49it/s] 49%|████▉     | 578/1178 [06:25<06:39,  1.50it/s] 49%|████▉     | 579/1178 [06:26<06:37,  1.51it/s] 49%|████▉     | 580/1178 [06:27<06:35,  1.51it/s] 49%|████▉     | 581/1178 [06:27<06:33,  1.52it/s] 49%|████▉     | 582/1178 [06:28<06:35,  1.51it/s] 49%|████▉     | 583/1178 [06:29<06:34,  1.51it/s] 50%|████▉     | 584/1178 [06:29<06:33,  1.51it/s] 50%|████▉     | 585/1178 [06:30<06:32,  1.51it/s] 50%|████▉     | 586/1178 [06:31<06:35,  1.50it/s] 50%|████▉     | 587/1178 [06:31<06:33,  1.50it/s] 50%|████▉     | 588/1178 [06:32<06:32,  1.50it/s] 50%|█████     | 589/1178 [06:32<05:21,  1.83it/s][INFO|trainer.py:2340] 2023-11-01 17:16:02,257 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-589
[INFO|configuration_utils.py:446] 2023-11-01 17:16:02,265 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-589/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:16:04,171 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-589/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:16:04,178 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-589/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:16:04,184 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-589/special_tokens_map.json
 50%|█████     | 590/1178 [06:39<22:27,  2.29s/it] 50%|█████     | 591/1178 [06:39<17:35,  1.80s/it] 50%|█████     | 592/1178 [06:40<14:11,  1.45s/it] 50%|█████     | 593/1178 [06:41<11:48,  1.21s/it] 50%|█████     | 594/1178 [06:41<10:08,  1.04s/it] 51%|█████     | 595/1178 [06:42<08:58,  1.08it/s] 51%|█████     | 596/1178 [06:43<08:09,  1.19it/s] 51%|█████     | 597/1178 [06:43<07:35,  1.28it/s] 51%|█████     | 598/1178 [06:44<07:11,  1.34it/s] 51%|█████     | 599/1178 [06:45<06:54,  1.40it/s] 51%|█████     | 600/1178 [06:45<06:42,  1.44it/s] 51%|█████     | 601/1178 [06:46<06:33,  1.47it/s] 51%|█████     | 602/1178 [06:46<06:26,  1.49it/s] 51%|█████     | 603/1178 [06:47<06:22,  1.50it/s] 51%|█████▏    | 604/1178 [06:48<06:19,  1.51it/s] 51%|█████▏    | 605/1178 [06:48<06:17,  1.52it/s] 51%|█████▏    | 606/1178 [06:49<06:14,  1.53it/s] 52%|█████▏    | 607/1178 [06:50<06:13,  1.53it/s] 52%|█████▏    | 608/1178 [06:50<06:14,  1.52it/s] 52%|█████▏    | 609/1178 [06:51<06:13,  1.52it/s] 52%|█████▏    | 610/1178 [06:52<06:13,  1.52it/s] 52%|█████▏    | 611/1178 [06:52<06:13,  1.52it/s] 52%|█████▏    | 612/1178 [06:53<06:11,  1.52it/s] 52%|█████▏    | 613/1178 [06:54<06:15,  1.50it/s] 52%|█████▏    | 614/1178 [06:54<06:13,  1.51it/s] 52%|█████▏    | 615/1178 [06:55<06:11,  1.52it/s] 52%|█████▏    | 616/1178 [06:56<06:10,  1.52it/s] 52%|█████▏    | 617/1178 [06:56<06:13,  1.50it/s] 52%|█████▏    | 618/1178 [06:57<06:13,  1.50it/s] 53%|█████▎    | 619/1178 [06:58<06:11,  1.50it/s] 53%|█████▎    | 620/1178 [06:58<06:09,  1.51it/s] 53%|█████▎    | 621/1178 [06:59<06:11,  1.50it/s] 53%|█████▎    | 622/1178 [07:00<06:11,  1.50it/s] 53%|█████▎    | 623/1178 [07:00<06:10,  1.50it/s] 53%|█████▎    | 624/1178 [07:01<06:07,  1.51it/s] 53%|█████▎    | 625/1178 [07:02<06:11,  1.49it/s] 53%|█████▎    | 626/1178 [07:02<06:07,  1.50it/s] 53%|█████▎    | 627/1178 [07:03<06:05,  1.51it/s] 53%|█████▎    | 628/1178 [07:04<06:05,  1.51it/s] 53%|█████▎    | 629/1178 [07:04<06:09,  1.49it/s] 53%|█████▎    | 630/1178 [07:05<06:06,  1.49it/s] 54%|█████▎    | 631/1178 [07:06<06:05,  1.50it/s] 54%|█████▎    | 632/1178 [07:06<06:09,  1.48it/s] 54%|█████▎    | 633/1178 [07:07<06:05,  1.49it/s] 54%|█████▍    | 634/1178 [07:08<06:02,  1.50it/s] 54%|█████▍    | 635/1178 [07:08<06:01,  1.50it/s] 54%|█████▍    | 636/1178 [07:09<06:05,  1.48it/s] 54%|█████▍    | 637/1178 [07:10<06:04,  1.48it/s] 54%|█████▍    | 638/1178 [07:10<06:01,  1.49it/s] 54%|█████▍    | 639/1178 [07:11<06:02,  1.49it/s] 54%|█████▍    | 640/1178 [07:12<05:59,  1.50it/s] 54%|█████▍    | 641/1178 [07:12<05:57,  1.50it/s] 54%|█████▍    | 642/1178 [07:13<05:54,  1.51it/s] 55%|█████▍    | 643/1178 [07:14<05:57,  1.50it/s] 55%|█████▍    | 644/1178 [07:14<06:00,  1.48it/s] 55%|█████▍    | 645/1178 [07:15<05:56,  1.50it/s] 55%|█████▍    | 646/1178 [07:16<05:53,  1.51it/s] 55%|█████▍    | 647/1178 [07:16<05:55,  1.49it/s] 55%|█████▌    | 648/1178 [07:17<05:53,  1.50it/s] 55%|█████▌    | 649/1178 [07:18<05:51,  1.50it/s] 55%|█████▌    | 650/1178 [07:18<05:50,  1.51it/s] 55%|█████▌    | 651/1178 [07:19<05:53,  1.49it/s] 55%|█████▌    | 652/1178 [07:20<05:50,  1.50it/s] 55%|█████▌    | 653/1178 [07:20<05:47,  1.51it/s] 56%|█████▌    | 654/1178 [07:21<05:46,  1.51it/s] 56%|█████▌    | 655/1178 [07:22<05:47,  1.50it/s] 56%|█████▌    | 656/1178 [07:22<05:46,  1.51it/s] 56%|█████▌    | 657/1178 [07:23<05:45,  1.51it/s] 56%|█████▌    | 658/1178 [07:24<05:43,  1.51it/s] 56%|█████▌    | 659/1178 [07:24<05:41,  1.52it/s] 56%|█████▌    | 660/1178 [07:25<05:45,  1.50it/s] 56%|█████▌    | 661/1178 [07:26<05:43,  1.51it/s] 56%|█████▌    | 662/1178 [07:26<05:41,  1.51it/s] 56%|█████▋    | 663/1178 [07:27<05:41,  1.51it/s] 56%|█████▋    | 664/1178 [07:28<05:47,  1.48it/s] 56%|█████▋    | 665/1178 [07:28<05:47,  1.48it/s] 57%|█████▋    | 666/1178 [07:29<05:44,  1.49it/s] 57%|█████▋    | 667/1178 [07:30<05:46,  1.48it/s] 57%|█████▋    | 668/1178 [07:30<05:42,  1.49it/s] 57%|█████▋    | 669/1178 [07:31<05:40,  1.50it/s] 57%|█████▋    | 670/1178 [07:32<05:41,  1.49it/s] 57%|█████▋    | 671/1178 [07:32<05:38,  1.50it/s] 57%|█████▋    | 672/1178 [07:33<05:35,  1.51it/s] 57%|█████▋    | 673/1178 [07:34<05:34,  1.51it/s] 57%|█████▋    | 674/1178 [07:34<05:33,  1.51it/s] 57%|█████▋    | 675/1178 [07:35<05:34,  1.50it/s] 57%|█████▋    | 676/1178 [07:36<05:33,  1.50it/s] 57%|█████▋    | 677/1178 [07:36<05:32,  1.51it/s] 58%|█████▊    | 678/1178 [07:37<05:30,  1.51it/s] 58%|█████▊    | 679/1178 [07:38<05:31,  1.50it/s] 58%|█████▊    | 680/1178 [07:38<05:31,  1.50it/s] 58%|█████▊    | 681/1178 [07:39<05:29,  1.51it/s] 58%|█████▊    | 682/1178 [07:40<05:28,  1.51it/s] 58%|█████▊    | 683/1178 [07:40<05:33,  1.48it/s] 58%|█████▊    | 684/1178 [07:41<05:31,  1.49it/s] 58%|█████▊    | 685/1178 [07:42<05:29,  1.50it/s] 58%|█████▊    | 686/1178 [07:42<05:30,  1.49it/s] 58%|█████▊    | 687/1178 [07:43<05:32,  1.48it/s] 58%|█████▊    | 688/1178 [07:44<05:30,  1.48it/s] 58%|█████▊    | 689/1178 [07:44<05:27,  1.49it/s] 59%|█████▊    | 690/1178 [07:45<05:31,  1.47it/s] 59%|█████▊    | 691/1178 [07:46<05:30,  1.47it/s] 59%|█████▊    | 692/1178 [07:46<05:27,  1.48it/s] 59%|█████▉    | 693/1178 [07:47<05:29,  1.47it/s] 59%|█████▉    | 694/1178 [07:48<05:26,  1.48it/s] 59%|█████▉    | 695/1178 [07:48<05:22,  1.50it/s] 59%|█████▉    | 696/1178 [07:49<05:20,  1.50it/s] 59%|█████▉    | 697/1178 [07:50<05:24,  1.48it/s] 59%|█████▉    | 698/1178 [07:50<05:25,  1.48it/s] 59%|█████▉    | 699/1178 [07:51<05:23,  1.48it/s] 59%|█████▉    | 700/1178 [07:52<05:27,  1.46it/s] 60%|█████▉    | 701/1178 [07:53<05:24,  1.47it/s] 60%|█████▉    | 702/1178 [07:53<05:21,  1.48it/s] 60%|█████▉    | 703/1178 [07:54<05:23,  1.47it/s] 60%|█████▉    | 704/1178 [07:55<05:20,  1.48it/s] 60%|█████▉    | 705/1178 [07:55<05:17,  1.49it/s] 60%|█████▉    | 706/1178 [07:56<05:14,  1.50it/s] 60%|██████    | 707/1178 [07:57<05:16,  1.49it/s] 60%|██████    | 708/1178 [07:57<05:14,  1.50it/s] 60%|██████    | 709/1178 [07:58<05:12,  1.50it/s] 60%|██████    | 710/1178 [07:59<05:14,  1.49it/s] 60%|██████    | 711/1178 [07:59<05:13,  1.49it/s] 60%|██████    | 712/1178 [08:00<05:11,  1.50it/s] 61%|██████    | 713/1178 [08:01<05:08,  1.51it/s] 61%|██████    | 714/1178 [08:01<05:13,  1.48it/s] 61%|██████    | 715/1178 [08:02<05:10,  1.49it/s] 61%|██████    | 716/1178 [08:03<05:08,  1.50it/s] 61%|██████    | 717/1178 [08:03<05:06,  1.51it/s] 61%|██████    | 718/1178 [08:04<05:09,  1.49it/s] 61%|██████    | 719/1178 [08:05<05:06,  1.50it/s] 61%|██████    | 720/1178 [08:05<05:04,  1.50it/s] 61%|██████    | 721/1178 [08:06<05:03,  1.50it/s] 61%|██████▏   | 722/1178 [08:07<05:08,  1.48it/s] 61%|██████▏   | 723/1178 [08:07<05:06,  1.48it/s] 61%|██████▏   | 724/1178 [08:08<05:03,  1.50it/s] 62%|██████▏   | 725/1178 [08:09<05:02,  1.50it/s] 62%|██████▏   | 726/1178 [08:09<05:01,  1.50it/s] 62%|██████▏   | 727/1178 [08:10<04:59,  1.51it/s] 62%|██████▏   | 728/1178 [08:11<04:57,  1.51it/s] 62%|██████▏   | 729/1178 [08:11<04:56,  1.51it/s] 62%|██████▏   | 730/1178 [08:12<04:59,  1.49it/s] 62%|██████▏   | 731/1178 [08:13<04:59,  1.49it/s] 62%|██████▏   | 732/1178 [08:13<04:58,  1.49it/s] 62%|██████▏   | 733/1178 [08:14<05:01,  1.47it/s] 62%|██████▏   | 734/1178 [08:15<05:04,  1.46it/s] 62%|██████▏   | 735/1178 [08:15<05:02,  1.47it/s] 62%|██████▏   | 736/1178 [08:16<05:04,  1.45it/s] 63%|██████▎   | 737/1178 [08:17<05:01,  1.46it/s] 63%|██████▎   | 738/1178 [08:17<04:58,  1.47it/s] 63%|██████▎   | 739/1178 [08:18<05:00,  1.46it/s] 63%|██████▎   | 740/1178 [08:19<04:57,  1.47it/s] 63%|██████▎   | 741/1178 [08:19<04:54,  1.48it/s] 63%|██████▎   | 742/1178 [08:20<04:55,  1.48it/s] 63%|██████▎   | 743/1178 [08:21<04:53,  1.48it/s] 63%|██████▎   | 744/1178 [08:21<04:51,  1.49it/s] 63%|██████▎   | 745/1178 [08:22<04:48,  1.50it/s] 63%|██████▎   | 746/1178 [08:23<04:49,  1.49it/s] 63%|██████▎   | 747/1178 [08:23<04:47,  1.50it/s] 63%|██████▎   | 748/1178 [08:24<04:45,  1.51it/s] 64%|██████▎   | 749/1178 [08:25<04:43,  1.51it/s] 64%|██████▎   | 750/1178 [08:25<04:43,  1.51it/s] 64%|██████▍   | 751/1178 [08:26<04:44,  1.50it/s] 64%|██████▍   | 752/1178 [08:27<04:41,  1.51it/s] 64%|██████▍   | 753/1178 [08:27<04:40,  1.52it/s] 64%|██████▍   | 754/1178 [08:28<04:41,  1.51it/s] 64%|██████▍   | 755/1178 [08:29<04:43,  1.49it/s] 64%|██████▍   | 756/1178 [08:29<04:41,  1.50it/s] 64%|██████▍   | 757/1178 [08:30<04:39,  1.50it/s] 64%|██████▍   | 758/1178 [08:31<04:43,  1.48it/s] 64%|██████▍   | 759/1178 [08:31<04:42,  1.48it/s] 65%|██████▍   | 760/1178 [08:32<04:39,  1.49it/s] 65%|██████▍   | 761/1178 [08:33<04:37,  1.50it/s] 65%|██████▍   | 762/1178 [08:33<04:38,  1.50it/s] 65%|██████▍   | 763/1178 [08:34<04:36,  1.50it/s] 65%|██████▍   | 764/1178 [08:35<04:34,  1.51it/s] 65%|██████▍   | 765/1178 [08:35<04:32,  1.51it/s] 65%|██████▌   | 766/1178 [08:36<04:34,  1.50it/s] 65%|██████▌   | 767/1178 [08:37<04:33,  1.50it/s] 65%|██████▌   | 768/1178 [08:37<04:31,  1.51it/s] 65%|██████▌   | 769/1178 [08:38<04:30,  1.51it/s] 65%|██████▌   | 770/1178 [08:39<04:35,  1.48it/s] 65%|██████▌   | 771/1178 [08:39<04:34,  1.48it/s] 66%|██████▌   | 772/1178 [08:40<04:32,  1.49it/s] 66%|██████▌   | 773/1178 [08:41<04:36,  1.47it/s] 66%|██████▌   | 774/1178 [08:41<04:34,  1.47it/s] 66%|██████▌   | 775/1178 [08:42<04:30,  1.49it/s] 66%|██████▌   | 776/1178 [08:43<04:28,  1.50it/s] 66%|██████▌   | 777/1178 [08:43<04:30,  1.48it/s] 66%|██████▌   | 778/1178 [08:44<04:28,  1.49it/s] 66%|██████▌   | 779/1178 [08:45<04:27,  1.49it/s] 66%|██████▌   | 780/1178 [08:45<04:27,  1.49it/s] 66%|██████▋   | 781/1178 [08:46<04:25,  1.50it/s] 66%|██████▋   | 782/1178 [08:47<04:22,  1.51it/s] 66%|██████▋   | 783/1178 [08:47<04:21,  1.51it/s] 67%|██████▋   | 784/1178 [08:48<04:19,  1.52it/s] 67%|██████▋   | 785/1178 [08:49<04:21,  1.50it/s] 67%|██████▋   | 786/1178 [08:49<04:20,  1.51it/s] 67%|██████▋   | 787/1178 [08:50<04:19,  1.51it/s] 67%|██████▋   | 788/1178 [08:51<04:17,  1.51it/s] 67%|██████▋   | 789/1178 [08:51<04:18,  1.51it/s] 67%|██████▋   | 790/1178 [08:52<04:17,  1.51it/s] 67%|██████▋   | 791/1178 [08:53<04:16,  1.51it/s] 67%|██████▋   | 792/1178 [08:53<04:15,  1.51it/s] 67%|██████▋   | 793/1178 [08:54<04:19,  1.48it/s] 67%|██████▋   | 794/1178 [08:55<04:17,  1.49it/s] 67%|██████▋   | 795/1178 [08:55<04:16,  1.50it/s] 68%|██████▊   | 796/1178 [08:56<04:16,  1.49it/s] 68%|██████▊   | 797/1178 [08:57<04:13,  1.50it/s] 68%|██████▊   | 798/1178 [08:57<04:11,  1.51it/s] 68%|██████▊   | 799/1178 [08:58<04:10,  1.51it/s] 68%|██████▊   | 800/1178 [08:59<04:09,  1.52it/s] 68%|██████▊   | 801/1178 [08:59<04:11,  1.50it/s] 68%|██████▊   | 802/1178 [09:00<04:10,  1.50it/s] 68%|██████▊   | 803/1178 [09:01<04:08,  1.51it/s] 68%|██████▊   | 804/1178 [09:01<04:06,  1.52it/s] 68%|██████▊   | 805/1178 [09:02<04:10,  1.49it/s] 68%|██████▊   | 806/1178 [09:03<04:09,  1.49it/s] 69%|██████▊   | 807/1178 [09:03<04:07,  1.50it/s] 69%|██████▊   | 808/1178 [09:04<04:06,  1.50it/s] 69%|██████▊   | 809/1178 [09:05<04:07,  1.49it/s] 69%|██████▉   | 810/1178 [09:05<04:05,  1.50it/s] 69%|██████▉   | 811/1178 [09:06<04:03,  1.51it/s] 69%|██████▉   | 812/1178 [09:07<04:02,  1.51it/s] 69%|██████▉   | 813/1178 [09:07<04:04,  1.49it/s] 69%|██████▉   | 814/1178 [09:08<04:03,  1.49it/s] 69%|██████▉   | 815/1178 [09:09<04:02,  1.50it/s] 69%|██████▉   | 816/1178 [09:09<04:04,  1.48it/s] 69%|██████▉   | 817/1178 [09:10<04:03,  1.48it/s] 69%|██████▉   | 818/1178 [09:11<04:01,  1.49it/s] 70%|██████▉   | 819/1178 [09:11<03:59,  1.50it/s] 70%|██████▉   | 820/1178 [09:12<03:59,  1.49it/s] 70%|██████▉   | 821/1178 [09:13<03:58,  1.50it/s] 70%|██████▉   | 822/1178 [09:13<03:56,  1.50it/s] 70%|██████▉   | 823/1178 [09:14<03:54,  1.51it/s] 70%|██████▉   | 824/1178 [09:15<03:56,  1.50it/s] 70%|███████   | 825/1178 [09:15<03:54,  1.50it/s] 70%|███████   | 826/1178 [09:16<03:53,  1.51it/s] 70%|███████   | 827/1178 [09:17<03:52,  1.51it/s] 70%|███████   | 828/1178 [09:17<03:53,  1.50it/s] 70%|███████   | 829/1178 [09:18<03:52,  1.50it/s] 70%|███████   | 830/1178 [09:19<03:50,  1.51it/s] 71%|███████   | 831/1178 [09:19<03:48,  1.52it/s] 71%|███████   | 832/1178 [09:20<03:48,  1.52it/s] 71%|███████   | 833/1178 [09:21<03:48,  1.51it/s] 71%|███████   | 834/1178 [09:21<03:47,  1.51it/s] 71%|███████   | 835/1178 [09:22<03:46,  1.51it/s] 71%|███████   | 836/1178 [09:23<03:45,  1.52it/s] 71%|███████   | 837/1178 [09:23<03:46,  1.50it/s] 71%|███████   | 838/1178 [09:24<03:45,  1.51it/s] 71%|███████   | 839/1178 [09:25<03:44,  1.51it/s] 71%|███████▏  | 840/1178 [09:25<03:43,  1.51it/s] 71%|███████▏  | 841/1178 [09:26<03:47,  1.48it/s] 71%|███████▏  | 842/1178 [09:27<03:45,  1.49it/s] 72%|███████▏  | 843/1178 [09:27<03:44,  1.50it/s] 72%|███████▏  | 844/1178 [09:28<03:41,  1.51it/s] 72%|███████▏  | 845/1178 [09:29<03:42,  1.50it/s] 72%|███████▏  | 846/1178 [09:29<03:41,  1.50it/s] 72%|███████▏  | 847/1178 [09:30<03:40,  1.50it/s] 72%|███████▏  | 848/1178 [09:31<03:38,  1.51it/s] 72%|███████▏  | 849/1178 [09:31<03:39,  1.50it/s] 72%|███████▏  | 850/1178 [09:32<03:38,  1.50it/s] 72%|███████▏  | 851/1178 [09:33<03:36,  1.51it/s] 72%|███████▏  | 852/1178 [09:33<03:35,  1.51it/s] 72%|███████▏  | 853/1178 [09:34<03:38,  1.49it/s] 72%|███████▏  | 854/1178 [09:35<03:37,  1.49it/s] 73%|███████▎  | 855/1178 [09:35<03:35,  1.50it/s] 73%|███████▎  | 856/1178 [09:36<03:35,  1.50it/s] 73%|███████▎  | 857/1178 [09:37<03:34,  1.49it/s] 73%|███████▎  | 858/1178 [09:37<03:32,  1.50it/s] 73%|███████▎  | 859/1178 [09:38<03:31,  1.51it/s] 73%|███████▎  | 860/1178 [09:39<03:30,  1.51it/s] 73%|███████▎  | 861/1178 [09:39<03:31,  1.50it/s] 73%|███████▎  | 862/1178 [09:40<03:30,  1.50it/s] 73%|███████▎  | 863/1178 [09:41<03:28,  1.51it/s] 73%|███████▎  | 864/1178 [09:41<03:28,  1.51it/s] 73%|███████▎  | 865/1178 [09:42<03:29,  1.50it/s] 74%|███████▎  | 866/1178 [09:43<03:27,  1.50it/s] 74%|███████▎  | 867/1178 [09:43<03:26,  1.51it/s] 74%|███████▎  | 868/1178 [09:44<03:24,  1.51it/s] 74%|███████▍  | 869/1178 [09:45<03:26,  1.50it/s] 74%|███████▍  | 870/1178 [09:45<03:24,  1.50it/s] 74%|███████▍  | 871/1178 [09:46<03:23,  1.51it/s] 74%|███████▍  | 872/1178 [09:47<03:22,  1.51it/s] 74%|███████▍  | 873/1178 [09:47<03:25,  1.48it/s] 74%|███████▍  | 874/1178 [09:48<03:24,  1.48it/s] 74%|███████▍  | 875/1178 [09:49<03:22,  1.50it/s] 74%|███████▍  | 876/1178 [09:49<03:22,  1.49it/s] 74%|███████▍  | 877/1178 [09:50<03:21,  1.49it/s] 75%|███████▍  | 878/1178 [09:51<03:20,  1.50it/s] 75%|███████▍  | 879/1178 [09:51<03:19,  1.50it/s] 75%|███████▍  | 880/1178 [09:52<03:21,  1.48it/s] 75%|███████▍  | 881/1178 [09:53<03:22,  1.46it/s] 75%|███████▍  | 882/1178 [09:53<03:20,  1.48it/s] 75%|███████▍  | 883/1178 [09:54<03:20,  1.47it/s] 75%|███████▌  | 884/1178 [09:55<03:18,  1.48it/s] 75%|███████▌  | 885/1178 [09:55<03:16,  1.49it/s] 75%|███████▌  | 886/1178 [09:56<03:14,  1.50it/s] 75%|███████▌  | 887/1178 [09:57<03:14,  1.49it/s] 75%|███████▌  | 888/1178 [09:57<03:13,  1.50it/s] 75%|███████▌  | 889/1178 [09:58<03:12,  1.50it/s] 76%|███████▌  | 890/1178 [09:59<03:11,  1.51it/s] 76%|███████▌  | 891/1178 [09:59<03:11,  1.49it/s] 76%|███████▌  | 892/1178 [10:00<03:10,  1.50it/s] 76%|███████▌  | 893/1178 [10:01<03:09,  1.51it/s] 76%|███████▌  | 894/1178 [10:01<03:08,  1.51it/s] 76%|███████▌  | 895/1178 [10:02<03:10,  1.49it/s] 76%|███████▌  | 896/1178 [10:03<03:08,  1.49it/s] 76%|███████▌  | 897/1178 [10:03<03:07,  1.50it/s] 76%|███████▌  | 898/1178 [10:04<03:08,  1.49it/s] 76%|███████▋  | 899/1178 [10:05<03:06,  1.50it/s] 76%|███████▋  | 900/1178 [10:05<03:04,  1.51it/s] 76%|███████▋  | 901/1178 [10:06<03:03,  1.51it/s] 77%|███████▋  | 902/1178 [10:07<03:02,  1.51it/s] 77%|███████▋  | 903/1178 [10:07<03:03,  1.50it/s] 77%|███████▋  | 904/1178 [10:08<03:02,  1.50it/s] 77%|███████▋  | 905/1178 [10:09<03:01,  1.51it/s] 77%|███████▋  | 906/1178 [10:09<02:59,  1.51it/s] 77%|███████▋  | 907/1178 [10:10<03:00,  1.50it/s] 77%|███████▋  | 908/1178 [10:11<02:59,  1.51it/s] 77%|███████▋  | 909/1178 [10:11<02:58,  1.51it/s] 77%|███████▋  | 910/1178 [10:12<02:57,  1.51it/s] 77%|███████▋  | 911/1178 [10:13<03:00,  1.48it/s] 77%|███████▋  | 912/1178 [10:13<02:58,  1.49it/s] 78%|███████▊  | 913/1178 [10:14<02:56,  1.50it/s] 78%|███████▊  | 914/1178 [10:15<02:56,  1.50it/s] 78%|███████▊  | 915/1178 [10:15<02:55,  1.50it/s] 78%|███████▊  | 916/1178 [10:16<02:53,  1.51it/s] 78%|███████▊  | 917/1178 [10:17<02:52,  1.51it/s] 78%|███████▊  | 918/1178 [10:17<02:51,  1.52it/s] 78%|███████▊  | 919/1178 [10:18<02:52,  1.50it/s] 78%|███████▊  | 920/1178 [10:19<02:51,  1.51it/s] 78%|███████▊  | 921/1178 [10:19<02:50,  1.51it/s] 78%|███████▊  | 922/1178 [10:20<02:49,  1.51it/s] 78%|███████▊  | 923/1178 [10:21<02:51,  1.49it/s] 78%|███████▊  | 924/1178 [10:21<02:49,  1.50it/s] 79%|███████▊  | 925/1178 [10:22<02:48,  1.51it/s] 79%|███████▊  | 926/1178 [10:23<02:47,  1.51it/s] 79%|███████▊  | 927/1178 [10:23<02:49,  1.48it/s] 79%|███████▉  | 928/1178 [10:24<02:49,  1.48it/s] 79%|███████▉  | 929/1178 [10:25<02:47,  1.49it/s] 79%|███████▉  | 930/1178 [10:25<02:47,  1.48it/s] 79%|███████▉  | 931/1178 [10:26<02:46,  1.49it/s] 79%|███████▉  | 932/1178 [10:27<02:44,  1.49it/s] 79%|███████▉  | 933/1178 [10:27<02:43,  1.50it/s] 79%|███████▉  | 934/1178 [10:28<02:44,  1.48it/s] 79%|███████▉  | 935/1178 [10:29<02:43,  1.49it/s] 79%|███████▉  | 936/1178 [10:29<02:41,  1.50it/s] 80%|███████▉  | 937/1178 [10:30<02:39,  1.51it/s] 80%|███████▉  | 938/1178 [10:31<02:40,  1.49it/s] 80%|███████▉  | 939/1178 [10:31<02:39,  1.50it/s] 80%|███████▉  | 940/1178 [10:32<02:37,  1.51it/s] 80%|███████▉  | 941/1178 [10:33<02:36,  1.51it/s] 80%|███████▉  | 942/1178 [10:33<02:38,  1.49it/s] 80%|████████  | 943/1178 [10:34<02:36,  1.50it/s] 80%|████████  | 944/1178 [10:35<02:34,  1.51it/s] 80%|████████  | 945/1178 [10:35<02:33,  1.52it/s] 80%|████████  | 946/1178 [10:36<02:34,  1.50it/s] 80%|████████  | 947/1178 [10:37<02:33,  1.51it/s] 80%|████████  | 948/1178 [10:37<02:32,  1.51it/s] 81%|████████  | 949/1178 [10:38<02:31,  1.52it/s] 81%|████████  | 950/1178 [10:39<02:32,  1.50it/s] 81%|████████  | 951/1178 [10:39<02:30,  1.51it/s] 81%|████████  | 952/1178 [10:40<02:29,  1.51it/s] 81%|████████  | 953/1178 [10:41<02:28,  1.52it/s] 81%|████████  | 954/1178 [10:41<02:29,  1.50it/s] 81%|████████  | 955/1178 [10:42<02:28,  1.50it/s] 81%|████████  | 956/1178 [10:43<02:27,  1.51it/s] 81%|████████  | 957/1178 [10:43<02:26,  1.51it/s] 81%|████████▏ | 958/1178 [10:44<02:27,  1.50it/s] 81%|████████▏ | 959/1178 [10:45<02:25,  1.50it/s] 81%|████████▏ | 960/1178 [10:45<02:24,  1.51it/s] 82%|████████▏ | 961/1178 [10:46<02:23,  1.51it/s] 82%|████████▏ | 962/1178 [10:47<02:22,  1.52it/s] 82%|████████▏ | 963/1178 [10:47<02:23,  1.50it/s] 82%|████████▏ | 964/1178 [10:48<02:22,  1.50it/s] 82%|████████▏ | 965/1178 [10:49<02:21,  1.51it/s] 82%|████████▏ | 966/1178 [10:49<02:19,  1.52it/s] 82%|████████▏ | 967/1178 [10:50<02:20,  1.50it/s] 82%|████████▏ | 968/1178 [10:51<02:20,  1.50it/s] 82%|████████▏ | 969/1178 [10:51<02:19,  1.50it/s] 82%|████████▏ | 970/1178 [10:52<02:19,  1.49it/s] 82%|████████▏ | 971/1178 [10:53<02:18,  1.50it/s] 83%|████████▎ | 972/1178 [10:53<02:16,  1.51it/s] 83%|████████▎ | 973/1178 [10:54<02:15,  1.51it/s] 83%|████████▎ | 974/1178 [10:55<02:15,  1.51it/s] 83%|████████▎ | 975/1178 [10:55<02:14,  1.51it/s] 83%|████████▎ | 976/1178 [10:56<02:14,  1.51it/s] 83%|████████▎ | 977/1178 [10:57<02:12,  1.51it/s] 83%|████████▎ | 978/1178 [10:57<02:12,  1.51it/s] 83%|████████▎ | 979/1178 [10:58<02:12,  1.50it/s] 83%|████████▎ | 980/1178 [10:59<02:11,  1.51it/s] 83%|████████▎ | 981/1178 [10:59<02:10,  1.51it/s] 83%|████████▎ | 982/1178 [11:00<02:09,  1.52it/s] 83%|████████▎ | 983/1178 [11:01<02:10,  1.49it/s] 84%|████████▎ | 984/1178 [11:01<02:11,  1.47it/s] 84%|████████▎ | 985/1178 [11:02<02:10,  1.48it/s] 84%|████████▎ | 986/1178 [11:03<02:11,  1.46it/s] 84%|████████▍ | 987/1178 [11:03<02:09,  1.47it/s] 84%|████████▍ | 988/1178 [11:04<02:08,  1.48it/s] 84%|████████▍ | 989/1178 [11:05<02:08,  1.47it/s] 84%|████████▍ | 990/1178 [11:05<02:06,  1.48it/s] 84%|████████▍ | 991/1178 [11:06<02:05,  1.49it/s] 84%|████████▍ | 992/1178 [11:07<02:04,  1.50it/s] 84%|████████▍ | 993/1178 [11:07<02:04,  1.48it/s] 84%|████████▍ | 994/1178 [11:08<02:03,  1.49it/s] 84%|████████▍ | 995/1178 [11:09<02:01,  1.51it/s] 85%|████████▍ | 996/1178 [11:09<02:00,  1.51it/s] 85%|████████▍ | 997/1178 [11:10<02:01,  1.49it/s] 85%|████████▍ | 998/1178 [11:11<02:00,  1.49it/s] 85%|████████▍ | 999/1178 [11:11<01:59,  1.49it/s] 85%|████████▍ | 1000/1178 [11:12<02:01,  1.47it/s]                                                    85%|████████▍ | 1000/1178 [11:12<02:01,  1.47it/s] 85%|████████▍ | 1001/1178 [11:13<02:00,  1.47it/s] 85%|████████▌ | 1002/1178 [11:13<01:59,  1.48it/s] 85%|████████▌ | 1003/1178 [11:14<01:59,  1.47it/s] 85%|████████▌ | 1004/1178 [11:15<01:57,  1.48it/s] 85%|████████▌ | 1005/1178 [11:16<01:55,  1.49it/s] 85%|████████▌ | 1006/1178 [11:16<01:54,  1.50it/s] 85%|████████▌ | 1007/1178 [11:17<01:55,  1.48it/s] 86%|████████▌ | 1008/1178 [11:18<01:54,  1.49it/s] 86%|████████▌ | 1009/1178 [11:18<01:52,  1.50it/s] 86%|████████▌ | 1010/1178 [11:19<01:51,  1.51it/s] 86%|████████▌ | 1011/1178 [11:20<01:51,  1.50it/s] 86%|████████▌ | 1012/1178 [11:20<01:50,  1.50it/s] 86%|████████▌ | 1013/1178 [11:21<01:49,  1.51it/s] 86%|████████▌ | 1014/1178 [11:21<01:48,  1.51it/s] 86%|████████▌ | 1015/1178 [11:22<01:48,  1.50it/s] 86%|████████▌ | 1016/1178 [11:23<01:47,  1.50it/s] 86%|████████▋ | 1017/1178 [11:23<01:46,  1.51it/s] 86%|████████▋ | 1018/1178 [11:24<01:45,  1.51it/s] 87%|████████▋ | 1019/1178 [11:25<01:46,  1.49it/s] 87%|████████▋ | 1020/1178 [11:26<01:46,  1.49it/s] 87%|████████▋ | 1021/1178 [11:26<01:45,  1.49it/s] 87%|████████▋ | 1022/1178 [11:27<01:45,  1.48it/s] 87%|████████▋ | 1023/1178 [11:28<01:44,  1.49it/s] 87%|████████▋ | 1024/1178 [11:28<01:42,  1.50it/s] 87%|████████▋ | 1025/1178 [11:29<01:41,  1.50it/s] 87%|████████▋ | 1026/1178 [11:30<01:42,  1.48it/s] 87%|████████▋ | 1027/1178 [11:30<01:42,  1.48it/s] 87%|████████▋ | 1028/1178 [11:31<01:40,  1.49it/s] 87%|████████▋ | 1029/1178 [11:32<01:41,  1.47it/s] 87%|████████▋ | 1030/1178 [11:32<01:39,  1.48it/s] 88%|████████▊ | 1031/1178 [11:33<01:38,  1.50it/s] 88%|████████▊ | 1032/1178 [11:34<01:36,  1.51it/s] 88%|████████▊ | 1033/1178 [11:34<01:36,  1.51it/s] 88%|████████▊ | 1034/1178 [11:35<01:35,  1.51it/s] 88%|████████▊ | 1035/1178 [11:36<01:34,  1.51it/s] 88%|████████▊ | 1036/1178 [11:36<01:33,  1.51it/s] 88%|████████▊ | 1037/1178 [11:37<01:33,  1.52it/s] 88%|████████▊ | 1038/1178 [11:38<01:32,  1.51it/s] 88%|████████▊ | 1039/1178 [11:38<01:32,  1.51it/s] 88%|████████▊ | 1040/1178 [11:39<01:31,  1.51it/s] 88%|████████▊ | 1041/1178 [11:40<01:30,  1.52it/s] 88%|████████▊ | 1042/1178 [11:40<01:31,  1.49it/s] 89%|████████▊ | 1043/1178 [11:41<01:30,  1.49it/s] 89%|████████▊ | 1044/1178 [11:42<01:29,  1.50it/s] 89%|████████▊ | 1045/1178 [11:42<01:28,  1.51it/s] 89%|████████▉ | 1046/1178 [11:43<01:28,  1.49it/s] 89%|████████▉ | 1047/1178 [11:44<01:27,  1.49it/s] 89%|████████▉ | 1048/1178 [11:44<01:26,  1.50it/s] 89%|████████▉ | 1049/1178 [11:45<01:27,  1.47it/s] 89%|████████▉ | 1050/1178 [11:46<01:26,  1.48it/s] 89%|████████▉ | 1051/1178 [11:46<01:25,  1.49it/s] 89%|████████▉ | 1052/1178 [11:47<01:23,  1.50it/s] 89%|████████▉ | 1053/1178 [11:48<01:24,  1.49it/s] 89%|████████▉ | 1054/1178 [11:48<01:23,  1.48it/s] 90%|████████▉ | 1055/1178 [11:49<01:22,  1.50it/s] 90%|████████▉ | 1056/1178 [11:50<01:21,  1.50it/s] 90%|████████▉ | 1057/1178 [11:50<01:20,  1.51it/s] 90%|████████▉ | 1058/1178 [11:51<01:19,  1.51it/s] 90%|████████▉ | 1059/1178 [11:52<01:18,  1.51it/s] 90%|████████▉ | 1060/1178 [11:52<01:17,  1.52it/s] 90%|█████████ | 1061/1178 [11:53<01:18,  1.49it/s] 90%|█████████ | 1062/1178 [11:54<01:17,  1.50it/s] 90%|█████████ | 1063/1178 [11:54<01:16,  1.50it/s] 90%|█████████ | 1064/1178 [11:55<01:15,  1.51it/s] 90%|█████████ | 1065/1178 [11:56<01:15,  1.49it/s] 90%|█████████ | 1066/1178 [11:56<01:15,  1.49it/s] 91%|█████████ | 1067/1178 [11:57<01:14,  1.49it/s] 91%|█████████ | 1068/1178 [11:58<01:14,  1.47it/s] 91%|█████████ | 1069/1178 [11:58<01:14,  1.47it/s] 91%|█████████ | 1070/1178 [11:59<01:13,  1.48it/s] 91%|█████████ | 1071/1178 [12:00<01:13,  1.46it/s] 91%|█████████ | 1072/1178 [12:00<01:12,  1.47it/s] 91%|█████████ | 1073/1178 [12:01<01:10,  1.49it/s] 91%|█████████ | 1074/1178 [12:02<01:09,  1.49it/s] 91%|█████████▏| 1075/1178 [12:02<01:08,  1.50it/s] 91%|█████████▏| 1076/1178 [12:03<01:07,  1.51it/s] 91%|█████████▏| 1077/1178 [12:04<01:06,  1.51it/s] 92%|█████████▏| 1078/1178 [12:04<01:06,  1.51it/s] 92%|█████████▏| 1079/1178 [12:05<01:06,  1.50it/s] 92%|█████████▏| 1080/1178 [12:06<01:05,  1.50it/s] 92%|█████████▏| 1081/1178 [12:06<01:04,  1.51it/s] 92%|█████████▏| 1082/1178 [12:07<01:03,  1.51it/s] 92%|█████████▏| 1083/1178 [12:08<01:03,  1.49it/s] 92%|█████████▏| 1084/1178 [12:08<01:03,  1.49it/s] 92%|█████████▏| 1085/1178 [12:09<01:01,  1.50it/s] 92%|█████████▏| 1086/1178 [12:10<01:00,  1.51it/s] 92%|█████████▏| 1087/1178 [12:10<01:01,  1.49it/s] 92%|█████████▏| 1088/1178 [12:11<01:00,  1.50it/s] 92%|█████████▏| 1089/1178 [12:12<00:59,  1.50it/s] 93%|█████████▎| 1090/1178 [12:12<00:58,  1.51it/s] 93%|█████████▎| 1091/1178 [12:13<00:58,  1.50it/s] 93%|█████████▎| 1092/1178 [12:14<00:57,  1.51it/s] 93%|█████████▎| 1093/1178 [12:14<00:56,  1.51it/s] 93%|█████████▎| 1094/1178 [12:15<00:55,  1.51it/s] 93%|█████████▎| 1095/1178 [12:16<00:55,  1.49it/s] 93%|█████████▎| 1096/1178 [12:16<00:55,  1.49it/s] 93%|█████████▎| 1097/1178 [12:17<00:54,  1.49it/s] 93%|█████████▎| 1098/1178 [12:18<00:54,  1.47it/s] 93%|█████████▎| 1099/1178 [12:18<00:53,  1.47it/s] 93%|█████████▎| 1100/1178 [12:19<00:52,  1.48it/s] 93%|█████████▎| 1101/1178 [12:20<00:52,  1.47it/s] 94%|█████████▎| 1102/1178 [12:20<00:51,  1.47it/s] 94%|█████████▎| 1103/1178 [12:21<00:50,  1.48it/s] 94%|█████████▎| 1104/1178 [12:22<00:49,  1.49it/s] 94%|█████████▍| 1105/1178 [12:22<00:49,  1.49it/s] 94%|█████████▍| 1106/1178 [12:23<00:48,  1.50it/s] 94%|█████████▍| 1107/1178 [12:24<00:47,  1.51it/s] 94%|█████████▍| 1108/1178 [12:24<00:46,  1.51it/s] 94%|█████████▍| 1109/1178 [12:25<00:45,  1.50it/s] 94%|█████████▍| 1110/1178 [12:26<00:45,  1.50it/s] 94%|█████████▍| 1111/1178 [12:26<00:44,  1.51it/s] 94%|█████████▍| 1112/1178 [12:27<00:43,  1.51it/s] 94%|█████████▍| 1113/1178 [12:28<00:43,  1.50it/s] 95%|█████████▍| 1114/1178 [12:28<00:42,  1.51it/s] 95%|█████████▍| 1115/1178 [12:29<00:41,  1.51it/s] 95%|█████████▍| 1116/1178 [12:30<00:41,  1.51it/s] 95%|█████████▍| 1117/1178 [12:30<00:41,  1.48it/s] 95%|█████████▍| 1118/1178 [12:31<00:40,  1.48it/s] 95%|█████████▍| 1119/1178 [12:32<00:39,  1.49it/s] 95%|█████████▌| 1120/1178 [12:32<00:39,  1.48it/s] 95%|█████████▌| 1121/1178 [12:33<00:38,  1.49it/s] 95%|█████████▌| 1122/1178 [12:34<00:37,  1.50it/s] 95%|█████████▌| 1123/1178 [12:34<00:36,  1.51it/s] 95%|█████████▌| 1124/1178 [12:35<00:35,  1.51it/s] 96%|█████████▌| 1125/1178 [12:36<00:35,  1.51it/s] 96%|█████████▌| 1126/1178 [12:36<00:34,  1.51it/s] 96%|█████████▌| 1127/1178 [12:37<00:33,  1.51it/s] 96%|█████████▌| 1128/1178 [12:38<00:32,  1.52it/s] 96%|█████████▌| 1129/1178 [12:38<00:32,  1.50it/s] 96%|█████████▌| 1130/1178 [12:39<00:31,  1.50it/s] 96%|█████████▌| 1131/1178 [12:40<00:31,  1.51it/s] 96%|█████████▌| 1132/1178 [12:40<00:30,  1.51it/s] 96%|█████████▌| 1133/1178 [12:41<00:30,  1.50it/s] 96%|█████████▋| 1134/1178 [12:42<00:29,  1.50it/s] 96%|█████████▋| 1135/1178 [12:42<00:28,  1.51it/s] 96%|█████████▋| 1136/1178 [12:43<00:27,  1.51it/s] 97%|█████████▋| 1137/1178 [12:44<00:27,  1.49it/s] 97%|█████████▋| 1138/1178 [12:44<00:26,  1.49it/s] 97%|█████████▋| 1139/1178 [12:45<00:26,  1.49it/s] 97%|█████████▋| 1140/1178 [12:46<00:25,  1.47it/s] 97%|█████████▋| 1141/1178 [12:46<00:25,  1.47it/s] 97%|█████████▋| 1142/1178 [12:47<00:24,  1.48it/s] 97%|█████████▋| 1143/1178 [12:48<00:23,  1.47it/s] 97%|█████████▋| 1144/1178 [12:48<00:22,  1.48it/s] 97%|█████████▋| 1145/1178 [12:49<00:22,  1.49it/s] 97%|█████████▋| 1146/1178 [12:50<00:21,  1.50it/s] 97%|█████████▋| 1147/1178 [12:50<00:21,  1.47it/s] 97%|█████████▋| 1148/1178 [12:51<00:20,  1.48it/s] 98%|█████████▊| 1149/1178 [12:52<00:19,  1.49it/s] 98%|█████████▊| 1150/1178 [12:52<00:18,  1.49it/s] 98%|█████████▊| 1151/1178 [12:53<00:18,  1.50it/s] 98%|█████████▊| 1152/1178 [12:54<00:17,  1.50it/s] 98%|█████████▊| 1153/1178 [12:54<00:16,  1.51it/s] 98%|█████████▊| 1154/1178 [12:55<00:15,  1.51it/s] 98%|█████████▊| 1155/1178 [12:56<00:15,  1.49it/s] 98%|█████████▊| 1156/1178 [12:56<00:14,  1.50it/s] 98%|█████████▊| 1157/1178 [12:57<00:13,  1.50it/s] 98%|█████████▊| 1158/1178 [12:58<00:13,  1.51it/s] 98%|█████████▊| 1159/1178 [12:58<00:12,  1.49it/s] 98%|█████████▊| 1160/1178 [12:59<00:12,  1.49it/s] 99%|█████████▊| 1161/1178 [13:00<00:11,  1.50it/s] 99%|█████████▊| 1162/1178 [13:00<00:10,  1.48it/s] 99%|█████████▊| 1163/1178 [13:01<00:10,  1.48it/s] 99%|█████████▉| 1164/1178 [13:02<00:09,  1.49it/s] 99%|█████████▉| 1165/1178 [13:02<00:08,  1.49it/s] 99%|█████████▉| 1166/1178 [13:03<00:08,  1.47it/s] 99%|█████████▉| 1167/1178 [13:04<00:07,  1.47it/s] 99%|█████████▉| 1168/1178 [13:05<00:06,  1.46it/s] 99%|█████████▉| 1169/1178 [13:05<00:06,  1.48it/s] 99%|█████████▉| 1170/1178 [13:06<00:05,  1.49it/s] 99%|█████████▉| 1171/1178 [13:07<00:04,  1.50it/s] 99%|█████████▉| 1172/1178 [13:07<00:04,  1.47it/s]100%|█████████▉| 1173/1178 [13:08<00:03,  1.48it/s]100%|█████████▉| 1174/1178 [13:09<00:02,  1.49it/s]100%|█████████▉| 1175/1178 [13:09<00:01,  1.50it/s]100%|█████████▉| 1176/1178 [13:10<00:01,  1.50it/s]100%|█████████▉| 1177/1178 [13:11<00:00,  1.50it/s]100%|██████████| 1178/1178 [13:11<00:00,  1.84it/s][INFO|trainer.py:2340] 2023-11-01 17:22:40,770 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-1178
[INFO|configuration_utils.py:446] 2023-11-01 17:22:40,780 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-1178/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:22:42,625 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-1178/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:22:42,633 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-1178/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:22:42,652 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/checkpoint-1178/special_tokens_map.json
[INFO|trainer.py:1662] 2023-11-01 17:22:46,314 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 1178/1178 [13:16<00:00,  1.84it/s]100%|██████████| 1178/1178 [13:16<00:00,  1.48it/s]
[INFO|trainer.py:2340] 2023-11-01 17:22:46,333 >> Saving model checkpoint to varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5
[INFO|configuration_utils.py:446] 2023-11-01 17:22:46,340 >> Configuration saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/config.json
[INFO|modeling_utils.py:1542] 2023-11-01 17:22:48,150 >> Model weights saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-11-01 17:22:48,157 >> tokenizer config file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-11-01 17:22:48,162 >> Special tokens file saved in varying-finetuning/chkpts/snli/d-snli-roberta-base-0.5/special_tokens_map.json
{'loss': 0.6632, 'learning_rate': 2.877758913412564e-06, 'epoch': 0.85}
{'loss': 0.5505, 'learning_rate': 7.555178268251275e-07, 'epoch': 1.7}
{'train_runtime': 796.9233, 'train_samples_per_second': 94.496, 'train_steps_per_second': 1.478, 'train_loss': 0.5962158423328238, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.5962
  train_runtime            = 0:13:16.92
  train_samples            =      37653
  train_samples_per_second =     94.496
  train_steps_per_second   =      1.478
11/01/2023 17:22:48 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-11-01 17:22:48,301 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-11-01 17:22:48,302 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:22:48,303 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-11-01 17:22:48,303 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-11-01 17:22:48,303 >>   Num examples = 780
[INFO|trainer.py:2595] 2023-11-01 17:22:48,303 >>   Batch size = 64
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:01,  8.76it/s] 23%|██▎       | 3/13 [00:00<00:01,  6.24it/s] 31%|███       | 4/13 [00:00<00:01,  5.41it/s] 38%|███▊      | 5/13 [00:00<00:01,  5.03it/s] 46%|████▌     | 6/13 [00:01<00:01,  4.81it/s] 54%|█████▍    | 7/13 [00:01<00:01,  4.69it/s] 62%|██████▏   | 8/13 [00:01<00:01,  4.60it/s] 69%|██████▉   | 9/13 [00:01<00:00,  4.54it/s] 77%|███████▋  | 10/13 [00:02<00:00,  4.51it/s] 85%|████████▍ | 11/13 [00:02<00:00,  4.48it/s] 92%|█████████▏| 12/13 [00:02<00:00,  4.47it/s][WARNING|training_args.py:1095] 2023-11-01 17:22:51,087 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 13/13 [00:02<00:00,  5.08it/s]
[WARNING|training_args.py:1095] 2023-11-01 17:22:51,125 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-11-01 17:22:51,125 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-11-01 17:22:51,331 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7423076629638672}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.7423
  eval_loss               =     0.5381
  eval_runtime            = 0:00:02.78
  eval_samples            =        780
  eval_samples_per_second =    280.071
  eval_steps_per_second   =      4.668
