11/14/2022 18:11:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/14/2022 18:11:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=chkpts/aflite_embedding_models/d-social-roberta-base/runs/Nov14_18-11-07_clip09.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
output_dir=chkpts/aflite_embedding_models/d-social-roberta-base,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=chkpts/aflite_embedding_models/d-social-roberta-base,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/14/2022 18:11:07 - INFO - __main__ - load a local file for train: /fs/clip-scratch/nehasrik/paraphrase-nlu/data_selection/defeasible/social/aflite_train.csv
11/14/2022 18:11:07 - INFO - __main__ - load a local file for validation: /fs/clip-scratch/nehasrik/paraphrase-nlu/data_selection/defeasible/social/aflite_dev.csv
11/14/2022 18:11:07 - WARNING - datasets.builder - Using custom data configuration default-64967cd709b15006
11/14/2022 18:11:07 - INFO - datasets.info - Loading Dataset Infos from /fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/datasets/packaged_modules/csv
11/14/2022 18:11:07 - INFO - datasets.builder - Generating dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-64967cd709b15006/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
Downloading and preparing dataset csv/default to /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-64967cd709b15006/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 12595.51it/s]11/14/2022 18:11:07 - INFO - datasets.download.download_manager - Downloading took 0.0 min
11/14/2022 18:11:07 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min

Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 560.55it/s]11/14/2022 18:11:07 - INFO - datasets.utils.info_utils - Unable to verify checksums.
11/14/2022 18:11:07 - INFO - datasets.builder - Generating train split

0 tables [00:00, ? tables/s]                            11/14/2022 18:11:07 - INFO - datasets.builder - Generating validation split
0 tables [00:00, ? tables/s]                            11/14/2022 18:11:07 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-64967cd709b15006/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 395.07it/s]
[INFO|configuration_utils.py:583] 2022-11-14 18:11:07,903 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
/fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
[INFO|configuration_utils.py:620] 2022-11-14 18:11:07,909 >> Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:334] 2022-11-14 18:11:07,953 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:583] 2022-11-14 18:11:07,985 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:620] 2022-11-14 18:11:07,986 >> Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:11:08,225 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:11:08,225 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:11:08,225 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:11:08,225 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:11:08,225 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:11:08,225 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|configuration_utils.py:583] 2022-11-14 18:11:08,259 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /nfshomes/nehasrik/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:620] 2022-11-14 18:11:08,260 >> Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1323] 2022-11-14 18:11:08,502 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:1580] 2022-11-14 18:11:14,346 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1591] 2022-11-14 18:11:14,346 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
Running tokenizer on dataset:   0%|          | 0/8 [00:00<?, ?ba/s]11/14/2022 18:11:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-64967cd709b15006/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ad7a6373d87b809c.arrow
Running tokenizer on dataset:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:01,  3.64ba/s]Running tokenizer on dataset:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.99ba/s]Running tokenizer on dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.17ba/s]Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  4.29ba/s]Running tokenizer on dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.39ba/s]Running tokenizer on dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.48ba/s]Running tokenizer on dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.54ba/s]Running tokenizer on dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  3.98ba/s]
Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]11/14/2022 18:11:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-64967cd709b15006/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-60de1a7c2133aab5.arrow
Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.60ba/s]Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.55ba/s]hi
11/14/2022 18:11:17 - INFO - __main__ - Sample 5238 of the training set: {'sentence1': " It is bad to try to control somebody else's relationship.", 'sentence2': "you're a marriage counselor.", 'label': 0, 'input_ids': [0, 85, 16, 1099, 7, 860, 7, 797, 4909, 1493, 18, 1291, 4, 2, 2, 6968, 214, 10, 3397, 20493, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/14/2022 18:11:17 - INFO - __main__ - Sample 912 of the training set: {'sentence1': ' Kindness to homeless animals is good.', 'sentence2': 'You work for PETA', 'label': 0, 'input_ids': [0, 17161, 1825, 7, 5506, 3122, 16, 205, 4, 2, 2, 1185, 173, 13, 221, 19739, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/14/2022 18:11:17 - INFO - __main__ - Sample 204 of the training set: {'sentence1': " It's fine to want to sleep in your fiancee's house.", 'sentence2': 'Your fiancee sleeps at your house sometimes.', 'label': 1, 'input_ids': [0, 85, 18, 2051, 7, 236, 7, 3581, 11, 110, 19960, 242, 18, 790, 4, 2, 2, 12861, 19960, 242, 36831, 23, 110, 790, 2128, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

run_glue.py:456: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("accuracy")
[INFO|trainer.py:541] 2022-11-14 18:11:22,629 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.
[INFO|trainer.py:1196] 2022-11-14 18:11:22,642 >> ***** Running training *****
[INFO|trainer.py:1197] 2022-11-14 18:11:22,642 >>   Num examples = 7709
[INFO|trainer.py:1198] 2022-11-14 18:11:22,642 >>   Num Epochs = 2
[INFO|trainer.py:1199] 2022-11-14 18:11:22,642 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1200] 2022-11-14 18:11:22,642 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1201] 2022-11-14 18:11:22,642 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1202] 2022-11-14 18:11:22,642 >>   Total optimization steps = 482
  0%|          | 0/482 [00:00<?, ?it/s]  0%|          | 1/482 [00:06<49:43,  6.20s/it]  0%|          | 2/482 [00:09<35:31,  4.44s/it]  1%|          | 3/482 [00:12<30:57,  3.88s/it]  1%|          | 4/482 [00:15<28:47,  3.61s/it]  1%|          | 5/482 [00:19<27:33,  3.47s/it]  1%|          | 6/482 [00:22<26:48,  3.38s/it]  1%|â–         | 7/482 [00:25<26:19,  3.33s/it]  2%|â–         | 8/482 [00:28<25:58,  3.29s/it]  2%|â–         | 9/482 [00:31<25:44,  3.26s/it]  2%|â–         | 10/482 [00:35<25:33,  3.25s/it]  2%|â–         | 11/482 [00:38<25:24,  3.24s/it]  2%|â–         | 12/482 [00:41<25:18,  3.23s/it]  3%|â–Ž         | 13/482 [00:44<25:16,  3.23s/it]  3%|â–Ž         | 14/482 [00:48<25:14,  3.24s/it]  3%|â–Ž         | 15/482 [00:51<25:11,  3.24s/it]  3%|â–Ž         | 16/482 [00:54<25:08,  3.24s/it]  4%|â–Ž         | 17/482 [00:57<25:06,  3.24s/it]  4%|â–Ž         | 18/482 [01:00<25:04,  3.24s/it]  4%|â–         | 19/482 [01:04<25:01,  3.24s/it]  4%|â–         | 20/482 [01:07<24:58,  3.24s/it]  4%|â–         | 21/482 [01:10<24:54,  3.24s/it]  5%|â–         | 22/482 [01:13<24:51,  3.24s/it]  5%|â–         | 23/482 [01:17<24:48,  3.24s/it]  5%|â–         | 24/482 [01:20<24:45,  3.24s/it]  5%|â–Œ         | 25/482 [01:23<24:41,  3.24s/it]  5%|â–Œ         | 26/482 [01:26<24:37,  3.24s/it]  6%|â–Œ         | 27/482 [01:30<24:34,  3.24s/it]  6%|â–Œ         | 28/482 [01:33<24:32,  3.24s/it]  6%|â–Œ         | 29/482 [01:36<24:28,  3.24s/it]  6%|â–Œ         | 30/482 [01:39<24:25,  3.24s/it]  6%|â–‹         | 31/482 [01:43<24:23,  3.24s/it]  7%|â–‹         | 32/482 [01:46<24:20,  3.24s/it]  7%|â–‹         | 33/482 [01:49<24:16,  3.24s/it]  7%|â–‹         | 34/482 [01:52<24:14,  3.25s/it]  7%|â–‹         | 35/482 [01:56<24:11,  3.25s/it]  7%|â–‹         | 36/482 [01:59<24:07,  3.25s/it]  8%|â–Š         | 37/482 [02:02<24:04,  3.25s/it]  8%|â–Š         | 38/482 [02:05<24:01,  3.25s/it]  8%|â–Š         | 39/482 [02:09<23:58,  3.25s/it]  8%|â–Š         | 40/482 [02:12<23:54,  3.25s/it]  9%|â–Š         | 41/482 [02:15<23:51,  3.25s/it]  9%|â–Š         | 42/482 [02:18<23:48,  3.25s/it]  9%|â–‰         | 43/482 [02:22<23:44,  3.25s/it]  9%|â–‰         | 44/482 [02:25<23:41,  3.25s/it]  9%|â–‰         | 45/482 [02:28<23:38,  3.25s/it] 10%|â–‰         | 46/482 [02:31<23:34,  3.24s/it] 10%|â–‰         | 47/482 [02:35<23:32,  3.25s/it] 10%|â–‰         | 48/482 [02:38<23:28,  3.25s/it] 10%|â–ˆ         | 49/482 [02:41<23:25,  3.24s/it] 10%|â–ˆ         | 50/482 [02:44<23:22,  3.25s/it] 11%|â–ˆ         | 51/482 [02:48<23:19,  3.25s/it] 11%|â–ˆ         | 52/482 [02:51<23:16,  3.25s/it] 11%|â–ˆ         | 53/482 [02:54<23:13,  3.25s/it] 11%|â–ˆ         | 54/482 [02:57<23:10,  3.25s/it] 11%|â–ˆâ–        | 55/482 [03:01<23:06,  3.25s/it] 12%|â–ˆâ–        | 56/482 [03:04<23:03,  3.25s/it] 12%|â–ˆâ–        | 57/482 [03:07<23:00,  3.25s/it] 12%|â–ˆâ–        | 58/482 [03:10<22:57,  3.25s/it] 12%|â–ˆâ–        | 59/482 [03:14<22:53,  3.25s/it] 12%|â–ˆâ–        | 60/482 [03:17<22:49,  3.25s/it] 13%|â–ˆâ–Ž        | 61/482 [03:20<22:47,  3.25s/it] 13%|â–ˆâ–Ž        | 62/482 [03:23<22:44,  3.25s/it] 13%|â–ˆâ–Ž        | 63/482 [03:27<22:41,  3.25s/it] 13%|â–ˆâ–Ž        | 64/482 [03:30<22:38,  3.25s/it] 13%|â–ˆâ–Ž        | 65/482 [03:33<22:35,  3.25s/it] 14%|â–ˆâ–Ž        | 66/482 [03:36<22:32,  3.25s/it] 14%|â–ˆâ–        | 67/482 [03:40<22:28,  3.25s/it] 14%|â–ˆâ–        | 68/482 [03:43<22:24,  3.25s/it] 14%|â–ˆâ–        | 69/482 [03:46<22:21,  3.25s/it] 15%|â–ˆâ–        | 70/482 [03:49<22:17,  3.25s/it] 15%|â–ˆâ–        | 71/482 [03:53<22:14,  3.25s/it] 15%|â–ˆâ–        | 72/482 [03:56<22:11,  3.25s/it] 15%|â–ˆâ–Œ        | 73/482 [03:59<22:08,  3.25s/it] 15%|â–ˆâ–Œ        | 74/482 [04:02<22:05,  3.25s/it] 16%|â–ˆâ–Œ        | 75/482 [04:06<22:04,  3.25s/it] 16%|â–ˆâ–Œ        | 76/482 [04:09<22:02,  3.26s/it] 16%|â–ˆâ–Œ        | 77/482 [04:12<22:02,  3.26s/it] 16%|â–ˆâ–Œ        | 78/482 [04:15<21:59,  3.27s/it] 16%|â–ˆâ–‹        | 79/482 [04:19<21:57,  3.27s/it] 17%|â–ˆâ–‹        | 80/482 [04:22<21:54,  3.27s/it] 17%|â–ˆâ–‹        | 81/482 [04:25<21:51,  3.27s/it] 17%|â–ˆâ–‹        | 82/482 [04:28<21:49,  3.27s/it] 17%|â–ˆâ–‹        | 83/482 [04:32<21:46,  3.28s/it] 17%|â–ˆâ–‹        | 84/482 [04:35<21:43,  3.28s/it] 18%|â–ˆâ–Š        | 85/482 [04:38<21:40,  3.27s/it] 18%|â–ˆâ–Š        | 86/482 [04:42<21:37,  3.28s/it] 18%|â–ˆâ–Š        | 87/482 [04:45<21:34,  3.28s/it] 18%|â–ˆâ–Š        | 88/482 [04:48<21:30,  3.28s/it] 18%|â–ˆâ–Š        | 89/482 [04:51<21:27,  3.28s/it] 19%|â–ˆâ–Š        | 90/482 [04:55<21:24,  3.28s/it] 19%|â–ˆâ–‰        | 91/482 [04:58<21:20,  3.28s/it] 19%|â–ˆâ–‰        | 92/482 [05:01<21:18,  3.28s/it] 19%|â–ˆâ–‰        | 93/482 [05:05<21:14,  3.28s/it] 20%|â–ˆâ–‰        | 94/482 [05:08<21:11,  3.28s/it] 20%|â–ˆâ–‰        | 95/482 [05:11<21:08,  3.28s/it] 20%|â–ˆâ–‰        | 96/482 [05:14<21:05,  3.28s/it] 20%|â–ˆâ–ˆ        | 97/482 [05:18<21:02,  3.28s/it] 20%|â–ˆâ–ˆ        | 98/482 [05:21<20:59,  3.28s/it] 21%|â–ˆâ–ˆ        | 99/482 [05:24<20:55,  3.28s/it] 21%|â–ˆâ–ˆ        | 100/482 [05:27<20:52,  3.28s/it] 21%|â–ˆâ–ˆ        | 101/482 [05:31<20:48,  3.28s/it] 21%|â–ˆâ–ˆ        | 102/482 [05:34<20:45,  3.28s/it] 21%|â–ˆâ–ˆâ–       | 103/482 [05:37<20:42,  3.28s/it] 22%|â–ˆâ–ˆâ–       | 104/482 [05:41<20:38,  3.28s/it] 22%|â–ˆâ–ˆâ–       | 105/482 [05:44<20:35,  3.28s/it] 22%|â–ˆâ–ˆâ–       | 106/482 [05:47<20:31,  3.28s/it] 22%|â–ˆâ–ˆâ–       | 107/482 [05:50<20:28,  3.27s/it] 22%|â–ˆâ–ˆâ–       | 108/482 [05:54<20:25,  3.28s/it] 23%|â–ˆâ–ˆâ–Ž       | 109/482 [05:57<20:22,  3.28s/it] 23%|â–ˆâ–ˆâ–Ž       | 110/482 [06:00<20:18,  3.28s/it] 23%|â–ˆâ–ˆâ–Ž       | 111/482 [06:03<20:15,  3.28s/it] 23%|â–ˆâ–ˆâ–Ž       | 112/482 [06:07<20:11,  3.28s/it] 23%|â–ˆâ–ˆâ–Ž       | 113/482 [06:10<20:08,  3.28s/it] 24%|â–ˆâ–ˆâ–Ž       | 114/482 [06:13<20:05,  3.28s/it] 24%|â–ˆâ–ˆâ–       | 115/482 [06:17<20:02,  3.28s/it] 24%|â–ˆâ–ˆâ–       | 116/482 [06:20<19:58,  3.27s/it] 24%|â–ˆâ–ˆâ–       | 117/482 [06:23<19:55,  3.28s/it] 24%|â–ˆâ–ˆâ–       | 118/482 [06:26<19:52,  3.27s/it] 25%|â–ˆâ–ˆâ–       | 119/482 [06:30<19:49,  3.28s/it] 25%|â–ˆâ–ˆâ–       | 120/482 [06:33<19:45,  3.27s/it] 25%|â–ˆâ–ˆâ–Œ       | 121/482 [06:36<19:41,  3.27s/it] 25%|â–ˆâ–ˆâ–Œ       | 122/482 [06:40<19:38,  3.27s/it] 26%|â–ˆâ–ˆâ–Œ       | 123/482 [06:43<19:36,  3.28s/it] 26%|â–ˆâ–ˆâ–Œ       | 124/482 [06:46<19:33,  3.28s/it] 26%|â–ˆâ–ˆâ–Œ       | 125/482 [06:49<19:29,  3.28s/it] 26%|â–ˆâ–ˆâ–Œ       | 126/482 [06:53<19:26,  3.28s/it] 26%|â–ˆâ–ˆâ–‹       | 127/482 [06:56<19:23,  3.28s/it] 27%|â–ˆâ–ˆâ–‹       | 128/482 [06:59<19:20,  3.28s/it] 27%|â–ˆâ–ˆâ–‹       | 129/482 [07:02<19:17,  3.28s/it] 27%|â–ˆâ–ˆâ–‹       | 130/482 [07:06<19:13,  3.28s/it] 27%|â–ˆâ–ˆâ–‹       | 131/482 [07:09<19:10,  3.28s/it] 27%|â–ˆâ–ˆâ–‹       | 132/482 [07:12<19:06,  3.28s/it] 28%|â–ˆâ–ˆâ–Š       | 133/482 [07:16<19:03,  3.28s/it] 28%|â–ˆâ–ˆâ–Š       | 134/482 [07:19<18:59,  3.28s/it] 28%|â–ˆâ–ˆâ–Š       | 135/482 [07:22<18:56,  3.28s/it] 28%|â–ˆâ–ˆâ–Š       | 136/482 [07:25<18:53,  3.28s/it] 28%|â–ˆâ–ˆâ–Š       | 137/482 [07:29<18:50,  3.28s/it] 29%|â–ˆâ–ˆâ–Š       | 138/482 [07:32<18:47,  3.28s/it] 29%|â–ˆâ–ˆâ–‰       | 139/482 [07:35<18:43,  3.28s/it] 29%|â–ˆâ–ˆâ–‰       | 140/482 [07:39<18:40,  3.28s/it] 29%|â–ˆâ–ˆâ–‰       | 141/482 [07:42<18:37,  3.28s/it] 29%|â–ˆâ–ˆâ–‰       | 142/482 [07:45<18:34,  3.28s/it] 30%|â–ˆâ–ˆâ–‰       | 143/482 [07:48<18:31,  3.28s/it] 30%|â–ˆâ–ˆâ–‰       | 144/482 [07:52<18:27,  3.28s/it] 30%|â–ˆâ–ˆâ–ˆ       | 145/482 [07:55<18:24,  3.28s/it] 30%|â–ˆâ–ˆâ–ˆ       | 146/482 [07:58<18:21,  3.28s/it] 30%|â–ˆâ–ˆâ–ˆ       | 147/482 [08:01<18:18,  3.28s/it] 31%|â–ˆâ–ˆâ–ˆ       | 148/482 [08:05<18:14,  3.28s/it] 31%|â–ˆâ–ˆâ–ˆ       | 149/482 [08:08<18:11,  3.28s/it] 31%|â–ˆâ–ˆâ–ˆ       | 150/482 [08:11<18:07,  3.28s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 151/482 [08:15<18:05,  3.28s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 152/482 [08:18<18:01,  3.28s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 153/482 [08:21<17:58,  3.28s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 154/482 [08:24<17:55,  3.28s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 155/482 [08:28<17:51,  3.28s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 156/482 [08:31<17:48,  3.28s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 157/482 [08:34<17:45,  3.28s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 158/482 [08:38<17:41,  3.28s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 159/482 [08:41<17:38,  3.28s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 160/482 [08:44<17:34,  3.27s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 161/482 [08:47<17:31,  3.28s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 162/482 [08:51<17:28,  3.28s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 163/482 [08:54<17:25,  3.28s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 164/482 [08:57<17:22,  3.28s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 165/482 [09:00<17:18,  3.28s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 166/482 [09:04<17:15,  3.28s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 167/482 [09:07<17:11,  3.28s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 168/482 [09:10<17:07,  3.27s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 169/482 [09:14<17:04,  3.27s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 170/482 [09:17<17:01,  3.27s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 171/482 [09:20<16:58,  3.28s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 172/482 [09:23<16:55,  3.28s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 173/482 [09:27<16:51,  3.28s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 174/482 [09:30<16:48,  3.28s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 175/482 [09:33<16:45,  3.28s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 176/482 [09:36<16:42,  3.28s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 177/482 [09:40<16:39,  3.28s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 178/482 [09:43<16:36,  3.28s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 179/482 [09:46<16:32,  3.28s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 180/482 [09:50<16:29,  3.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 181/482 [09:53<16:26,  3.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 182/482 [09:56<16:23,  3.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 183/482 [09:59<16:19,  3.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 184/482 [10:03<16:17,  3.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 185/482 [10:06<16:13,  3.28s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 186/482 [10:09<16:10,  3.28s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 187/482 [10:13<16:07,  3.28s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 188/482 [10:16<16:03,  3.28s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 189/482 [10:19<16:00,  3.28s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 190/482 [10:22<15:56,  3.28s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 191/482 [10:26<15:53,  3.28s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 192/482 [10:29<15:50,  3.28s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 193/482 [10:32<15:47,  3.28s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 194/482 [10:35<15:43,  3.28s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 195/482 [10:39<15:40,  3.28s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 196/482 [10:42<15:37,  3.28s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 197/482 [10:45<15:34,  3.28s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 198/482 [10:49<15:30,  3.28s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 199/482 [10:52<15:27,  3.28s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200/482 [10:55<15:23,  3.28s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 201/482 [10:58<15:20,  3.28s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 202/482 [11:02<15:17,  3.28s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 203/482 [11:05<15:14,  3.28s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 204/482 [11:08<15:11,  3.28s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 205/482 [11:12<15:07,  3.28s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 206/482 [11:15<15:04,  3.28s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 207/482 [11:18<15:01,  3.28s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 208/482 [11:21<14:57,  3.28s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 209/482 [11:25<14:54,  3.28s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 210/482 [11:28<14:51,  3.28s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 211/482 [11:31<14:47,  3.28s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 212/482 [11:34<14:44,  3.28s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 213/482 [11:38<14:41,  3.28s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 214/482 [11:41<14:38,  3.28s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 215/482 [11:44<14:35,  3.28s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 216/482 [11:48<14:31,  3.28s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 217/482 [11:51<14:28,  3.28s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 218/482 [11:54<14:24,  3.28s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 219/482 [11:57<14:21,  3.28s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 220/482 [12:01<14:18,  3.28s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 221/482 [12:04<14:15,  3.28s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 222/482 [12:07<14:11,  3.28s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 223/482 [12:10<14:08,  3.27s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 224/482 [12:14<14:05,  3.28s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 225/482 [12:17<14:02,  3.28s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 226/482 [12:20<13:59,  3.28s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 227/482 [12:24<13:55,  3.28s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 228/482 [12:27<13:52,  3.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 229/482 [12:30<13:48,  3.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 230/482 [12:33<13:45,  3.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 231/482 [12:37<13:41,  3.27s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 232/482 [12:40<13:38,  3.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 233/482 [12:43<13:35,  3.28s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 234/482 [12:47<13:32,  3.28s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 235/482 [12:50<13:29,  3.28s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 236/482 [12:53<13:26,  3.28s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 237/482 [12:56<13:23,  3.28s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 238/482 [13:00<13:20,  3.28s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 239/482 [13:03<13:16,  3.28s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 240/482 [13:06<13:13,  3.28s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 241/482 [13:09<12:49,  3.19s/it][INFO|trainer.py:1987] 2022-11-14 18:24:32,349 >> Saving model checkpoint to chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-241
/fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
[INFO|configuration_utils.py:413] 2022-11-14 18:24:32,353 >> Configuration saved in chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-241/config.json
[INFO|modeling_utils.py:1041] 2022-11-14 18:24:37,100 >> Model weights saved in chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-241/pytorch_model.bin
[INFO|tokenization_utils_base.py:2033] 2022-11-14 18:24:37,105 >> tokenizer config file saved in chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-241/tokenizer_config.json
[INFO|tokenization_utils_base.py:2039] 2022-11-14 18:24:37,108 >> Special tokens file saved in chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-241/special_tokens_map.json
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 242/482 [13:27<30:10,  7.54s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 243/482 [13:30<24:56,  6.26s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 244/482 [13:33<21:17,  5.37s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 245/482 [13:37<18:43,  4.74s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 246/482 [13:40<16:55,  4.30s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 247/482 [13:43<15:38,  3.99s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248/482 [13:47<14:44,  3.78s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 249/482 [13:50<14:05,  3.63s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 250/482 [13:53<13:38,  3.53s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 251/482 [13:56<13:17,  3.45s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 252/482 [14:00<13:01,  3.40s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 253/482 [14:03<12:49,  3.36s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 254/482 [14:06<12:40,  3.34s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 255/482 [14:10<12:33,  3.32s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 256/482 [14:13<12:26,  3.31s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 257/482 [14:16<12:21,  3.30s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 258/482 [14:19<12:17,  3.29s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 259/482 [14:23<12:13,  3.29s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 260/482 [14:26<12:09,  3.28s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 261/482 [14:29<12:05,  3.28s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 262/482 [14:32<12:01,  3.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 263/482 [14:36<11:58,  3.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 264/482 [14:39<11:54,  3.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 265/482 [14:42<11:51,  3.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 266/482 [14:46<11:48,  3.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 267/482 [14:49<11:44,  3.28s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 268/482 [14:52<11:41,  3.28s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 269/482 [14:55<11:38,  3.28s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 270/482 [14:59<11:34,  3.28s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 271/482 [15:02<11:31,  3.28s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 272/482 [15:05<11:28,  3.28s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 273/482 [15:08<11:25,  3.28s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 274/482 [15:12<11:21,  3.28s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 275/482 [15:15<11:18,  3.28s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 276/482 [15:18<11:14,  3.28s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 277/482 [15:22<11:11,  3.28s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 278/482 [15:25<11:08,  3.28s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 279/482 [15:28<11:05,  3.28s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 280/482 [15:31<11:02,  3.28s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 281/482 [15:35<10:58,  3.28s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 282/482 [15:38<10:55,  3.28s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 283/482 [15:41<10:52,  3.28s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 284/482 [15:45<10:48,  3.28s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 285/482 [15:48<10:45,  3.28s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 286/482 [15:51<10:42,  3.28s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 287/482 [15:54<10:38,  3.28s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 288/482 [15:58<10:35,  3.28s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 289/482 [16:01<10:32,  3.28s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 290/482 [16:04<10:29,  3.28s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 291/482 [16:07<10:26,  3.28s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 292/482 [16:11<10:23,  3.28s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 293/482 [16:14<10:19,  3.28s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 294/482 [16:17<10:16,  3.28s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 295/482 [16:21<10:12,  3.28s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 296/482 [16:24<10:09,  3.28s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 297/482 [16:27<10:06,  3.28s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 298/482 [16:30<10:03,  3.28s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 299/482 [16:34<09:59,  3.28s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 300/482 [16:37<09:56,  3.28s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 301/482 [16:40<09:53,  3.28s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 302/482 [16:44<09:49,  3.28s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 303/482 [16:47<09:46,  3.28s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 304/482 [16:50<09:43,  3.28s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 305/482 [16:53<09:40,  3.28s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 306/482 [16:57<09:36,  3.28s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 307/482 [17:00<09:33,  3.28s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 308/482 [17:03<09:29,  3.28s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 309/482 [17:06<09:26,  3.27s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 310/482 [17:10<09:23,  3.27s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 311/482 [17:13<09:20,  3.28s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 312/482 [17:16<09:16,  3.28s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 313/482 [17:20<09:13,  3.28s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 314/482 [17:23<09:10,  3.28s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 315/482 [17:26<09:07,  3.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 316/482 [17:29<09:03,  3.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 317/482 [17:33<09:00,  3.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 318/482 [17:36<08:57,  3.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 319/482 [17:39<08:54,  3.28s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 320/482 [17:43<08:50,  3.28s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 321/482 [17:46<08:47,  3.28s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 322/482 [17:49<08:43,  3.27s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 323/482 [17:52<08:40,  3.28s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 324/482 [17:56<08:37,  3.28s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 325/482 [17:59<08:34,  3.28s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 326/482 [18:02<08:31,  3.28s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 327/482 [18:05<08:27,  3.28s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 328/482 [18:09<08:24,  3.28s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 329/482 [18:12<08:21,  3.28s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 330/482 [18:15<08:17,  3.28s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 331/482 [18:19<08:14,  3.27s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 332/482 [18:22<08:11,  3.28s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 333/482 [18:25<08:08,  3.28s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 334/482 [18:28<08:04,  3.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 335/482 [18:32<08:01,  3.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 336/482 [18:35<07:58,  3.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 337/482 [18:38<07:55,  3.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 338/482 [18:41<07:51,  3.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 339/482 [18:45<07:48,  3.28s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 340/482 [18:48<07:45,  3.28s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 341/482 [18:51<07:41,  3.27s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 342/482 [18:55<07:38,  3.27s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 343/482 [18:58<07:35,  3.27s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 344/482 [19:01<07:31,  3.27s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 345/482 [19:04<07:28,  3.27s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 346/482 [19:08<07:25,  3.27s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 347/482 [19:11<07:21,  3.27s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 348/482 [19:14<07:18,  3.27s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 349/482 [19:17<07:15,  3.27s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 350/482 [19:21<07:12,  3.28s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 351/482 [19:24<07:09,  3.28s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 352/482 [19:27<07:05,  3.28s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 353/482 [19:31<07:02,  3.28s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 354/482 [19:34<06:59,  3.28s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 355/482 [19:37<06:56,  3.28s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 356/482 [19:40<06:52,  3.28s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 357/482 [19:44<06:49,  3.28s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 358/482 [19:47<06:46,  3.28s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 359/482 [19:50<06:43,  3.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 360/482 [19:54<06:39,  3.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 361/482 [19:57<06:36,  3.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 362/482 [20:00<06:33,  3.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 363/482 [20:03<06:29,  3.27s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 364/482 [20:07<06:26,  3.28s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 365/482 [20:10<06:23,  3.28s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 366/482 [20:13<06:19,  3.27s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 367/482 [20:16<06:16,  3.28s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 368/482 [20:20<06:13,  3.28s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 369/482 [20:23<06:10,  3.28s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 370/482 [20:26<06:07,  3.28s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 371/482 [20:30<06:03,  3.28s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 372/482 [20:33<06:00,  3.28s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 373/482 [20:36<05:57,  3.28s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 374/482 [20:39<05:53,  3.28s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 375/482 [20:43<05:50,  3.28s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 376/482 [20:46<05:47,  3.28s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 377/482 [20:49<05:43,  3.27s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 378/482 [20:52<05:40,  3.27s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 379/482 [20:56<05:37,  3.28s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 380/482 [20:59<05:34,  3.27s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 381/482 [21:02<05:30,  3.28s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 382/482 [21:06<05:27,  3.28s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 383/482 [21:09<05:24,  3.27s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 384/482 [21:12<05:21,  3.28s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 385/482 [21:15<05:17,  3.27s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 386/482 [21:19<05:14,  3.28s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 387/482 [21:22<05:11,  3.28s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 388/482 [21:25<05:08,  3.28s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 389/482 [21:29<05:04,  3.28s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 390/482 [21:32<05:01,  3.28s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 391/482 [21:35<04:57,  3.27s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 392/482 [21:38<04:54,  3.27s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 393/482 [21:42<04:51,  3.27s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 394/482 [21:45<04:48,  3.27s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 395/482 [21:48<04:44,  3.27s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 396/482 [21:51<04:41,  3.28s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 397/482 [21:55<04:38,  3.28s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 398/482 [21:58<04:35,  3.28s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 399/482 [22:01<04:31,  3.28s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 400/482 [22:05<04:28,  3.28s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 401/482 [22:08<04:25,  3.28s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 402/482 [22:11<04:22,  3.28s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 403/482 [22:14<04:18,  3.28s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 404/482 [22:18<04:15,  3.28s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 405/482 [22:21<04:12,  3.28s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 406/482 [22:24<04:08,  3.27s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 407/482 [22:27<04:05,  3.28s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 408/482 [22:31<04:02,  3.28s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 409/482 [22:34<03:59,  3.28s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 410/482 [22:37<03:55,  3.28s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 411/482 [22:41<03:52,  3.28s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 412/482 [22:44<03:49,  3.28s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 413/482 [22:47<03:46,  3.28s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 414/482 [22:50<03:42,  3.28s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 415/482 [22:54<03:39,  3.27s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 416/482 [22:57<03:36,  3.28s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 417/482 [23:00<03:32,  3.28s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 418/482 [23:04<03:29,  3.28s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 419/482 [23:07<03:26,  3.28s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 420/482 [23:10<03:23,  3.28s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 421/482 [23:13<03:19,  3.27s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 422/482 [23:17<03:16,  3.27s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 423/482 [23:20<03:13,  3.27s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 424/482 [23:23<03:09,  3.28s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 425/482 [23:26<03:06,  3.27s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 426/482 [23:30<03:03,  3.28s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 427/482 [23:33<03:00,  3.27s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 428/482 [23:36<02:56,  3.28s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 429/482 [23:40<02:53,  3.28s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 430/482 [23:43<02:50,  3.28s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 431/482 [23:46<02:47,  3.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 432/482 [23:49<02:43,  3.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 433/482 [23:53<02:40,  3.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 434/482 [23:56<02:37,  3.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 435/482 [23:59<02:34,  3.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 436/482 [24:03<02:30,  3.28s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 437/482 [24:06<02:27,  3.28s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 438/482 [24:09<02:24,  3.28s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 439/482 [24:12<02:20,  3.28s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 440/482 [24:16<02:17,  3.28s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 441/482 [24:19<02:14,  3.28s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 442/482 [24:22<02:11,  3.28s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 443/482 [24:25<02:07,  3.28s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 444/482 [24:29<02:04,  3.28s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 445/482 [24:32<02:01,  3.28s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 446/482 [24:35<01:58,  3.28s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 447/482 [24:39<01:54,  3.28s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 448/482 [24:42<01:51,  3.28s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 449/482 [24:45<01:48,  3.28s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 450/482 [24:48<01:44,  3.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 451/482 [24:52<01:41,  3.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 452/482 [24:55<01:38,  3.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 453/482 [24:58<01:35,  3.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 454/482 [25:01<01:31,  3.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 455/482 [25:05<01:28,  3.28s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 456/482 [25:08<01:25,  3.27s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 457/482 [25:11<01:21,  3.27s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 458/482 [25:15<01:18,  3.27s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 459/482 [25:18<01:15,  3.27s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 460/482 [25:21<01:12,  3.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 461/482 [25:24<01:08,  3.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 462/482 [25:28<01:05,  3.27s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 463/482 [25:31<01:02,  3.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 464/482 [25:34<00:58,  3.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 465/482 [25:38<00:55,  3.27s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 466/482 [25:41<00:52,  3.28s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 467/482 [25:44<00:49,  3.28s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 468/482 [25:47<00:45,  3.28s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 469/482 [25:51<00:42,  3.27s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 470/482 [25:54<00:39,  3.28s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 471/482 [25:57<00:36,  3.28s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 472/482 [26:00<00:32,  3.28s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 473/482 [26:04<00:29,  3.27s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 474/482 [26:07<00:26,  3.27s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 475/482 [26:10<00:22,  3.27s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 476/482 [26:14<00:19,  3.28s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 477/482 [26:17<00:16,  3.28s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 478/482 [26:20<00:13,  3.28s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 479/482 [26:23<00:09,  3.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 480/482 [26:27<00:06,  3.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 481/482 [26:30<00:03,  3.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 482/482 [26:33<00:00,  3.19s/it][INFO|trainer.py:1987] 2022-11-14 18:37:56,087 >> Saving model checkpoint to chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-482
/fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
[INFO|configuration_utils.py:413] 2022-11-14 18:37:56,092 >> Configuration saved in chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-482/config.json
[INFO|modeling_utils.py:1041] 2022-11-14 18:38:00,916 >> Model weights saved in chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-482/pytorch_model.bin
[INFO|tokenization_utils_base.py:2033] 2022-11-14 18:38:00,920 >> tokenizer config file saved in chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-482/tokenizer_config.json
[INFO|tokenization_utils_base.py:2039] 2022-11-14 18:38:00,923 >> Special tokens file saved in chkpts/aflite_embedding_models/d-social-roberta-base/checkpoint-482/special_tokens_map.json
[INFO|trainer.py:1401] 2022-11-14 18:38:10,813 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 482/482 [26:48<00:00,  3.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 482/482 [26:48<00:00,  3.34s/it]
[INFO|trainer.py:1987] 2022-11-14 18:38:10,815 >> Saving model checkpoint to chkpts/aflite_embedding_models/d-social-roberta-base
/fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
[INFO|configuration_utils.py:413] 2022-11-14 18:38:10,820 >> Configuration saved in chkpts/aflite_embedding_models/d-social-roberta-base/config.json
[INFO|modeling_utils.py:1041] 2022-11-14 18:38:15,646 >> Model weights saved in chkpts/aflite_embedding_models/d-social-roberta-base/pytorch_model.bin
[INFO|tokenization_utils_base.py:2033] 2022-11-14 18:38:15,653 >> tokenizer config file saved in chkpts/aflite_embedding_models/d-social-roberta-base/tokenizer_config.json
[INFO|tokenization_utils_base.py:2039] 2022-11-14 18:38:15,657 >> Special tokens file saved in chkpts/aflite_embedding_models/d-social-roberta-base/special_tokens_map.json
{'train_runtime': 1608.1714, 'train_samples_per_second': 9.587, 'train_steps_per_second': 0.3, 'train_loss': 0.5363282642918503, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.5363
  train_runtime            = 0:26:48.17
  train_samples            =       7709
  train_samples_per_second =      9.587
  train_steps_per_second   =        0.3
11/14/2022 18:38:15 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:541] 2022-11-14 18:38:15,882 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.
[WARNING|training_args.py:888] 2022-11-14 18:38:15,883 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2235] 2022-11-14 18:38:15,884 >> ***** Running Evaluation *****
[INFO|trainer.py:2237] 2022-11-14 18:38:15,884 >>   Num examples = 1002
[INFO|trainer.py:2240] 2022-11-14 18:38:15,884 >>   Batch size = 32
  0%|          | 0/32 [00:00<?, ?it/s]  6%|â–‹         | 2/32 [00:01<00:17,  1.75it/s]  9%|â–‰         | 3/32 [00:02<00:23,  1.23it/s] 12%|â–ˆâ–Ž        | 4/32 [00:03<00:26,  1.07it/s] 16%|â–ˆâ–Œ        | 5/32 [00:04<00:27,  1.01s/it] 19%|â–ˆâ–‰        | 6/32 [00:05<00:27,  1.05s/it] 22%|â–ˆâ–ˆâ–       | 7/32 [00:06<00:27,  1.08s/it] 25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:08<00:26,  1.10s/it] 28%|â–ˆâ–ˆâ–Š       | 9/32 [00:09<00:25,  1.11s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:10<00:24,  1.12s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:11<00:23,  1.13s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:12<00:22,  1.13s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:13<00:21,  1.14s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:14<00:20,  1.14s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:16<00:19,  1.14s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:17<00:18,  1.14s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:18<00:17,  1.14s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:19<00:15,  1.14s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:20<00:14,  1.14s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:21<00:13,  1.14s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:22<00:12,  1.14s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:24<00:11,  1.14s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:25<00:10,  1.14s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:26<00:09,  1.14s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:27<00:08,  1.14s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:28<00:06,  1.14s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:29<00:05,  1.14s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:30<00:04,  1.14s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:32<00:03,  1.14s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:33<00:02,  1.14s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:34<00:01,  1.14s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:34<00:00,  1.09it/s][WARNING|training_args.py:888] 2022-11-14 18:38:51,796 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:34<00:00,  1.08s/it]
[WARNING|training_args.py:888] 2022-11-14 18:38:51,805 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:888] 2022-11-14 18:38:51,806 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:446] 2022-11-14 18:38:51,851 >> Dropping the following result as it does not have all the necessary field:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7654690742492676}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.7655
  eval_loss               =     0.4753
  eval_runtime            = 0:00:35.91
  eval_samples            =       1002
  eval_samples_per_second =     27.901
  eval_steps_per_second   =      0.891
