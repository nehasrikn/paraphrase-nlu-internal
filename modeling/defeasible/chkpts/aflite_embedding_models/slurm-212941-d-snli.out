11/14/2022 18:07:31 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/14/2022 18:07:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=chkpts/aflite_embedding_models/d-snli-roberta-base/runs/Nov14_18-07-31_clip03.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
output_dir=chkpts/aflite_embedding_models/d-snli-roberta-base,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=chkpts/aflite_embedding_models/d-snli-roberta-base,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/14/2022 18:07:31 - INFO - __main__ - load a local file for train: /fs/clip-scratch/nehasrik/paraphrase-nlu/data_selection/defeasible/snli/aflite_train.csv
11/14/2022 18:07:31 - INFO - __main__ - load a local file for validation: /fs/clip-scratch/nehasrik/paraphrase-nlu/data_selection/defeasible/snli/aflite_dev.csv
11/14/2022 18:07:32 - WARNING - datasets.builder - Using custom data configuration default-caa08cb95913f62b
11/14/2022 18:07:32 - INFO - datasets.info - Loading Dataset Infos from /fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/datasets/packaged_modules/csv
11/14/2022 18:07:32 - INFO - datasets.builder - Generating dataset csv (/fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-caa08cb95913f62b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
Downloading and preparing dataset csv/default to /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-caa08cb95913f62b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10255.02it/s]11/14/2022 18:07:32 - INFO - datasets.download.download_manager - Downloading took 0.0 min
11/14/2022 18:07:32 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min

Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 247.83it/s]11/14/2022 18:07:32 - INFO - datasets.utils.info_utils - Unable to verify checksums.
11/14/2022 18:07:32 - INFO - datasets.builder - Generating train split

0 tables [00:00, ? tables/s]1 tables [00:00,  3.50 tables/s]                                11/14/2022 18:07:32 - INFO - datasets.builder - Generating validation split
0 tables [00:00, ? tables/s]                            11/14/2022 18:07:32 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-caa08cb95913f62b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 298.52it/s]
[INFO|configuration_utils.py:583] 2022-11-14 18:07:33,178 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
/fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
[INFO|configuration_utils.py:620] 2022-11-14 18:07:33,217 >> Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:334] 2022-11-14 18:07:33,259 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:583] 2022-11-14 18:07:33,295 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:620] 2022-11-14 18:07:33,296 >> Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:07:33,643 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:07:33,643 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:07:33,643 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:07:33,643 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:07:33,644 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1741] 2022-11-14 18:07:33,644 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|configuration_utils.py:583] 2022-11-14 18:07:33,780 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /nfshomes/nehasrik/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:620] 2022-11-14 18:07:33,780 >> Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.11.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1323] 2022-11-14 18:07:35,093 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:1580] 2022-11-14 18:07:46,650 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1591] 2022-11-14 18:07:46,651 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
Running tokenizer on dataset:   0%|          | 0/9 [00:00<?, ?ba/s]11/14/2022 18:07:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-caa08cb95913f62b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0436f02912f5b1f3.arrow
Running tokenizer on dataset:  11%|â–ˆ         | 1/9 [00:00<00:03,  2.00ba/s]Running tokenizer on dataset:  22%|â–ˆâ–ˆâ–       | 2/9 [00:00<00:02,  2.34ba/s]Running tokenizer on dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:01<00:02,  2.51ba/s]Running tokenizer on dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:01<00:01,  2.60ba/s]Running tokenizer on dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:01<00:01,  2.67ba/s]Running tokenizer on dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:02<00:01,  2.72ba/s]Running tokenizer on dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:02<00:00,  2.74ba/s]Running tokenizer on dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:03<00:00,  2.78ba/s]Running tokenizer on dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:03<00:00,  2.41ba/s]
Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]11/14/2022 18:07:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-scratch/nehasrik/paraphrase-nlu/modeling/hf-cache/csv/default-caa08cb95913f62b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f9bcb81ac85db624.arrow
Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.76ba/s]Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.72ba/s]hi
11/14/2022 18:07:52 - INFO - __main__ - Sample 1824 of the training set: {'sentence1': 'Two middle-aged men, one of which is holding a camera, have a conversation on the sidewalk. One man is asking the other if he can take a picture.', 'sentence2': 'The man with the camera is a known landscape photographer.', 'label': 0, 'input_ids': [0, 9058, 1692, 12, 4628, 604, 6, 65, 9, 61, 16, 1826, 10, 2280, 6, 33, 10, 1607, 15, 5, 15032, 4, 509, 313, 16, 1996, 5, 97, 114, 37, 64, 185, 10, 2170, 4, 2, 2, 133, 313, 19, 5, 2280, 16, 10, 684, 5252, 9463, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/14/2022 18:07:52 - INFO - __main__ - Sample 409 of the training set: {'sentence1': 'A man with black hair is talking on a cellphone sitting in a cubicle. The man is working.', 'sentence2': 'The cubicle is filled with game consoles.', 'label': 0, 'input_ids': [0, 250, 313, 19, 909, 2549, 16, 1686, 15, 10, 13605, 2828, 11, 10, 18383, 11317, 4, 20, 313, 16, 447, 4, 2, 2, 133, 18383, 11317, 16, 3820, 19, 177, 25552, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
11/14/2022 18:07:52 - INFO - __main__ - Sample 4506 of the training set: {'sentence1': 'A couple walks through an open air market. A couple is shopping for food.', 'sentence2': 'The couple skipped breakfast.', 'label': 1, 'input_ids': [0, 250, 891, 5792, 149, 41, 490, 935, 210, 4, 83, 891, 16, 3482, 13, 689, 4, 2, 2, 133, 891, 22904, 7080, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

run_glue.py:456: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("accuracy")
[INFO|trainer.py:541] 2022-11-14 18:08:26,957 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.
[INFO|trainer.py:1196] 2022-11-14 18:08:26,994 >> ***** Running training *****
[INFO|trainer.py:1197] 2022-11-14 18:08:26,994 >>   Num examples = 8872
[INFO|trainer.py:1198] 2022-11-14 18:08:26,994 >>   Num Epochs = 2
[INFO|trainer.py:1199] 2022-11-14 18:08:26,994 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1200] 2022-11-14 18:08:26,994 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1201] 2022-11-14 18:08:26,994 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1202] 2022-11-14 18:08:26,994 >>   Total optimization steps = 556
  0%|          | 0/556 [00:00<?, ?it/s]  0%|          | 1/556 [00:25<3:56:24, 25.56s/it]  0%|          | 2/556 [00:25<1:39:21, 10.76s/it]  1%|          | 3/556 [00:26<55:32,  6.03s/it]    1%|          | 4/556 [00:26<34:58,  3.80s/it]  1%|          | 5/556 [00:27<23:36,  2.57s/it]  1%|          | 6/556 [00:27<16:47,  1.83s/it]  1%|â–         | 7/556 [00:27<12:26,  1.36s/it]  1%|â–         | 8/556 [00:28<09:36,  1.05s/it]  2%|â–         | 9/556 [00:28<07:42,  1.18it/s]  2%|â–         | 10/556 [00:29<06:25,  1.42it/s]  2%|â–         | 11/556 [00:29<05:31,  1.64it/s]  2%|â–         | 12/556 [00:29<04:55,  1.84it/s]  2%|â–         | 13/556 [00:30<04:30,  2.01it/s]  3%|â–Ž         | 14/556 [00:30<04:12,  2.15it/s]  3%|â–Ž         | 15/556 [00:31<04:00,  2.25it/s]  3%|â–Ž         | 16/556 [00:31<03:51,  2.33it/s]  3%|â–Ž         | 17/556 [00:31<03:45,  2.39it/s]  3%|â–Ž         | 18/556 [00:32<03:40,  2.44it/s]  3%|â–Ž         | 19/556 [00:32<03:37,  2.47it/s]  4%|â–Ž         | 20/556 [00:33<03:34,  2.49it/s]  4%|â–         | 21/556 [00:33<03:33,  2.51it/s]  4%|â–         | 22/556 [00:33<03:32,  2.52it/s]  4%|â–         | 23/556 [00:34<03:31,  2.53it/s]  4%|â–         | 24/556 [00:34<03:30,  2.53it/s]  4%|â–         | 25/556 [00:34<03:29,  2.53it/s]  5%|â–         | 26/556 [00:35<03:28,  2.54it/s]  5%|â–         | 27/556 [00:35<03:28,  2.54it/s]  5%|â–Œ         | 28/556 [00:36<03:27,  2.54it/s]  5%|â–Œ         | 29/556 [00:36<03:27,  2.54it/s]  5%|â–Œ         | 30/556 [00:36<03:26,  2.54it/s]  6%|â–Œ         | 31/556 [00:37<03:26,  2.54it/s]  6%|â–Œ         | 32/556 [00:37<03:25,  2.55it/s]  6%|â–Œ         | 33/556 [00:38<03:25,  2.54it/s]  6%|â–Œ         | 34/556 [00:38<03:25,  2.54it/s]  6%|â–‹         | 35/556 [00:38<03:25,  2.54it/s]  6%|â–‹         | 36/556 [00:39<03:25,  2.54it/s]  7%|â–‹         | 37/556 [00:39<03:24,  2.54it/s]  7%|â–‹         | 38/556 [00:40<03:24,  2.53it/s]  7%|â–‹         | 39/556 [00:40<03:24,  2.53it/s]  7%|â–‹         | 40/556 [00:40<03:23,  2.53it/s]  7%|â–‹         | 41/556 [00:41<03:23,  2.53it/s]  8%|â–Š         | 42/556 [00:41<03:22,  2.53it/s]  8%|â–Š         | 43/556 [00:42<03:22,  2.54it/s]  8%|â–Š         | 44/556 [00:42<03:21,  2.54it/s]  8%|â–Š         | 45/556 [00:42<03:21,  2.54it/s]  8%|â–Š         | 46/556 [00:43<03:21,  2.54it/s]  8%|â–Š         | 47/556 [00:43<03:21,  2.53it/s]  9%|â–Š         | 48/556 [00:44<03:20,  2.53it/s]  9%|â–‰         | 49/556 [00:44<03:20,  2.53it/s]  9%|â–‰         | 50/556 [00:44<03:20,  2.52it/s]  9%|â–‰         | 51/556 [00:45<03:20,  2.52it/s]  9%|â–‰         | 52/556 [00:45<03:19,  2.53it/s] 10%|â–‰         | 53/556 [00:46<03:19,  2.53it/s] 10%|â–‰         | 54/556 [00:46<03:18,  2.53it/s] 10%|â–‰         | 55/556 [00:46<03:18,  2.53it/s] 10%|â–ˆ         | 56/556 [00:47<03:17,  2.53it/s] 10%|â–ˆ         | 57/556 [00:47<03:17,  2.53it/s] 10%|â–ˆ         | 58/556 [00:48<03:16,  2.53it/s] 11%|â–ˆ         | 59/556 [00:48<03:16,  2.52it/s] 11%|â–ˆ         | 60/556 [00:48<03:16,  2.53it/s] 11%|â–ˆ         | 61/556 [00:49<03:16,  2.52it/s] 11%|â–ˆ         | 62/556 [00:49<03:16,  2.52it/s] 11%|â–ˆâ–        | 63/556 [00:49<03:15,  2.52it/s] 12%|â–ˆâ–        | 64/556 [00:50<03:15,  2.52it/s] 12%|â–ˆâ–        | 65/556 [00:50<03:15,  2.52it/s] 12%|â–ˆâ–        | 66/556 [00:51<03:14,  2.52it/s] 12%|â–ˆâ–        | 67/556 [00:51<03:14,  2.51it/s] 12%|â–ˆâ–        | 68/556 [00:51<03:13,  2.52it/s] 12%|â–ˆâ–        | 69/556 [00:52<03:12,  2.52it/s] 13%|â–ˆâ–Ž        | 70/556 [00:52<03:13,  2.52it/s] 13%|â–ˆâ–Ž        | 71/556 [00:53<03:12,  2.52it/s] 13%|â–ˆâ–Ž        | 72/556 [00:53<03:12,  2.51it/s] 13%|â–ˆâ–Ž        | 73/556 [00:53<03:12,  2.51it/s] 13%|â–ˆâ–Ž        | 74/556 [00:54<03:11,  2.51it/s] 13%|â–ˆâ–Ž        | 75/556 [00:54<03:11,  2.51it/s] 14%|â–ˆâ–Ž        | 76/556 [00:55<03:11,  2.51it/s] 14%|â–ˆâ–        | 77/556 [00:55<03:10,  2.51it/s] 14%|â–ˆâ–        | 78/556 [00:55<03:10,  2.51it/s] 14%|â–ˆâ–        | 79/556 [00:56<03:09,  2.51it/s] 14%|â–ˆâ–        | 80/556 [00:56<03:09,  2.52it/s] 15%|â–ˆâ–        | 81/556 [00:57<03:09,  2.51it/s] 15%|â–ˆâ–        | 82/556 [00:57<03:08,  2.51it/s] 15%|â–ˆâ–        | 83/556 [00:57<03:08,  2.51it/s] 15%|â–ˆâ–Œ        | 84/556 [00:58<03:08,  2.51it/s] 15%|â–ˆâ–Œ        | 85/556 [00:58<03:07,  2.51it/s] 15%|â–ˆâ–Œ        | 86/556 [00:59<03:07,  2.51it/s] 16%|â–ˆâ–Œ        | 87/556 [00:59<03:07,  2.51it/s] 16%|â–ˆâ–Œ        | 88/556 [00:59<03:06,  2.51it/s] 16%|â–ˆâ–Œ        | 89/556 [01:00<03:07,  2.50it/s] 16%|â–ˆâ–Œ        | 90/556 [01:00<03:06,  2.50it/s] 16%|â–ˆâ–‹        | 91/556 [01:01<03:05,  2.50it/s] 17%|â–ˆâ–‹        | 92/556 [01:01<03:05,  2.50it/s] 17%|â–ˆâ–‹        | 93/556 [01:01<03:04,  2.50it/s] 17%|â–ˆâ–‹        | 94/556 [01:02<03:04,  2.50it/s] 17%|â–ˆâ–‹        | 95/556 [01:02<03:04,  2.50it/s] 17%|â–ˆâ–‹        | 96/556 [01:03<03:03,  2.50it/s] 17%|â–ˆâ–‹        | 97/556 [01:03<03:04,  2.49it/s] 18%|â–ˆâ–Š        | 98/556 [01:03<03:03,  2.50it/s] 18%|â–ˆâ–Š        | 99/556 [01:04<03:02,  2.50it/s] 18%|â–ˆâ–Š        | 100/556 [01:04<03:02,  2.49it/s] 18%|â–ˆâ–Š        | 101/556 [01:05<03:02,  2.49it/s] 18%|â–ˆâ–Š        | 102/556 [01:05<03:02,  2.49it/s] 19%|â–ˆâ–Š        | 103/556 [01:05<03:01,  2.49it/s] 19%|â–ˆâ–Š        | 104/556 [01:06<03:03,  2.46it/s] 19%|â–ˆâ–‰        | 105/556 [01:06<03:07,  2.41it/s] 19%|â–ˆâ–‰        | 106/556 [01:07<03:07,  2.40it/s] 19%|â–ˆâ–‰        | 107/556 [01:07<03:07,  2.40it/s] 19%|â–ˆâ–‰        | 108/556 [01:08<03:05,  2.42it/s] 20%|â–ˆâ–‰        | 109/556 [01:08<03:03,  2.44it/s] 20%|â–ˆâ–‰        | 110/556 [01:08<03:01,  2.45it/s] 20%|â–ˆâ–‰        | 111/556 [01:09<03:00,  2.47it/s] 20%|â–ˆâ–ˆ        | 112/556 [01:09<02:59,  2.47it/s] 20%|â–ˆâ–ˆ        | 113/556 [01:10<03:01,  2.45it/s] 21%|â–ˆâ–ˆ        | 114/556 [01:10<03:00,  2.44it/s] 21%|â–ˆâ–ˆ        | 115/556 [01:10<02:59,  2.45it/s] 21%|â–ˆâ–ˆ        | 116/556 [01:11<02:58,  2.46it/s] 21%|â–ˆâ–ˆ        | 117/556 [01:11<03:00,  2.43it/s] 21%|â–ˆâ–ˆ        | 118/556 [01:12<03:03,  2.38it/s] 21%|â–ˆâ–ˆâ–       | 119/556 [01:12<03:05,  2.36it/s] 22%|â–ˆâ–ˆâ–       | 120/556 [01:13<03:03,  2.37it/s] 22%|â–ˆâ–ˆâ–       | 121/556 [01:13<03:02,  2.39it/s] 22%|â–ˆâ–ˆâ–       | 122/556 [01:13<02:59,  2.42it/s] 22%|â–ˆâ–ˆâ–       | 123/556 [01:14<02:57,  2.43it/s] 22%|â–ˆâ–ˆâ–       | 124/556 [01:14<02:56,  2.45it/s] 22%|â–ˆâ–ˆâ–       | 125/556 [01:15<02:54,  2.47it/s] 23%|â–ˆâ–ˆâ–Ž       | 126/556 [01:15<02:53,  2.47it/s] 23%|â–ˆâ–ˆâ–Ž       | 127/556 [01:15<02:53,  2.48it/s] 23%|â–ˆâ–ˆâ–Ž       | 128/556 [01:16<02:52,  2.48it/s] 23%|â–ˆâ–ˆâ–Ž       | 129/556 [01:16<02:52,  2.48it/s] 23%|â–ˆâ–ˆâ–Ž       | 130/556 [01:17<02:51,  2.48it/s] 24%|â–ˆâ–ˆâ–Ž       | 131/556 [01:17<02:50,  2.49it/s] 24%|â–ˆâ–ˆâ–Ž       | 132/556 [01:17<02:52,  2.45it/s] 24%|â–ˆâ–ˆâ–       | 133/556 [01:18<02:53,  2.43it/s] 24%|â–ˆâ–ˆâ–       | 134/556 [01:18<02:52,  2.44it/s] 24%|â–ˆâ–ˆâ–       | 135/556 [01:19<02:50,  2.46it/s] 24%|â–ˆâ–ˆâ–       | 136/556 [01:19<02:51,  2.44it/s] 25%|â–ˆâ–ˆâ–       | 137/556 [01:19<02:53,  2.41it/s] 25%|â–ˆâ–ˆâ–       | 138/556 [01:20<02:51,  2.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 139/556 [01:20<02:49,  2.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 140/556 [01:21<02:49,  2.45it/s] 25%|â–ˆâ–ˆâ–Œ       | 141/556 [01:21<02:49,  2.45it/s] 26%|â–ˆâ–ˆâ–Œ       | 142/556 [01:21<02:48,  2.46it/s] 26%|â–ˆâ–ˆâ–Œ       | 143/556 [01:22<02:49,  2.43it/s] 26%|â–ˆâ–ˆâ–Œ       | 144/556 [01:22<02:50,  2.42it/s] 26%|â–ˆâ–ˆâ–Œ       | 145/556 [01:23<02:48,  2.44it/s] 26%|â–ˆâ–ˆâ–‹       | 146/556 [01:23<02:47,  2.45it/s] 26%|â–ˆâ–ˆâ–‹       | 147/556 [01:24<02:47,  2.44it/s] 27%|â–ˆâ–ˆâ–‹       | 148/556 [01:24<02:46,  2.45it/s] 27%|â–ˆâ–ˆâ–‹       | 149/556 [01:24<02:45,  2.46it/s] 27%|â–ˆâ–ˆâ–‹       | 150/556 [01:25<02:46,  2.44it/s] 27%|â–ˆâ–ˆâ–‹       | 151/556 [01:25<02:46,  2.43it/s] 27%|â–ˆâ–ˆâ–‹       | 152/556 [01:26<02:45,  2.44it/s] 28%|â–ˆâ–ˆâ–Š       | 153/556 [01:26<02:45,  2.43it/s] 28%|â–ˆâ–ˆâ–Š       | 154/556 [01:26<02:44,  2.44it/s] 28%|â–ˆâ–ˆâ–Š       | 155/556 [01:27<02:43,  2.46it/s] 28%|â–ˆâ–ˆâ–Š       | 156/556 [01:27<02:44,  2.43it/s] 28%|â–ˆâ–ˆâ–Š       | 157/556 [01:28<02:42,  2.45it/s] 28%|â–ˆâ–ˆâ–Š       | 158/556 [01:28<02:43,  2.44it/s] 29%|â–ˆâ–ˆâ–Š       | 159/556 [01:28<02:45,  2.40it/s] 29%|â–ˆâ–ˆâ–‰       | 160/556 [01:29<02:43,  2.42it/s] 29%|â–ˆâ–ˆâ–‰       | 161/556 [01:29<02:42,  2.44it/s] 29%|â–ˆâ–ˆâ–‰       | 162/556 [01:30<02:40,  2.45it/s] 29%|â–ˆâ–ˆâ–‰       | 163/556 [01:30<02:39,  2.46it/s] 29%|â–ˆâ–ˆâ–‰       | 164/556 [01:30<02:39,  2.47it/s] 30%|â–ˆâ–ˆâ–‰       | 165/556 [01:31<02:40,  2.44it/s] 30%|â–ˆâ–ˆâ–‰       | 166/556 [01:31<02:41,  2.41it/s] 30%|â–ˆâ–ˆâ–ˆ       | 167/556 [01:32<02:39,  2.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 168/556 [01:32<02:38,  2.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 169/556 [01:33<02:37,  2.46it/s] 31%|â–ˆâ–ˆâ–ˆ       | 170/556 [01:33<02:37,  2.46it/s] 31%|â–ˆâ–ˆâ–ˆ       | 171/556 [01:33<02:38,  2.43it/s] 31%|â–ˆâ–ˆâ–ˆ       | 172/556 [01:34<02:39,  2.41it/s] 31%|â–ˆâ–ˆâ–ˆ       | 173/556 [01:34<02:37,  2.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 174/556 [01:35<02:35,  2.46it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 175/556 [01:35<02:34,  2.46it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 176/556 [01:35<02:34,  2.46it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 177/556 [01:36<02:35,  2.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 178/556 [01:36<02:35,  2.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 179/556 [01:37<02:33,  2.45it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 180/556 [01:37<02:33,  2.46it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 181/556 [01:37<02:32,  2.46it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 182/556 [01:38<02:33,  2.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 183/556 [01:38<02:34,  2.41it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 184/556 [01:39<02:34,  2.41it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 185/556 [01:39<02:33,  2.42it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 186/556 [01:40<02:32,  2.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 187/556 [01:40<02:32,  2.42it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 188/556 [01:40<02:31,  2.42it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 189/556 [01:41<02:31,  2.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 190/556 [01:41<02:30,  2.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 191/556 [01:42<02:29,  2.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 192/556 [01:42<02:28,  2.45it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 193/556 [01:42<02:28,  2.45it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 194/556 [01:43<02:27,  2.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 195/556 [01:43<02:26,  2.47it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 196/556 [01:44<02:26,  2.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 197/556 [01:44<02:26,  2.46it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 198/556 [01:44<02:25,  2.46it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 199/556 [01:45<02:25,  2.46it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 200/556 [01:45<02:25,  2.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 201/556 [01:46<02:26,  2.43it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 202/556 [01:46<02:25,  2.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 203/556 [01:46<02:24,  2.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 204/556 [01:47<02:23,  2.45it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 205/556 [01:47<02:23,  2.45it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 206/556 [01:48<02:22,  2.45it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 207/556 [01:48<02:22,  2.46it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 208/556 [01:48<02:21,  2.46it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 209/556 [01:49<02:23,  2.42it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 210/556 [01:49<02:24,  2.40it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 211/556 [01:50<02:23,  2.41it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 212/556 [01:50<02:21,  2.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 213/556 [01:51<02:20,  2.45it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 214/556 [01:51<02:19,  2.46it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 215/556 [01:51<02:18,  2.46it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 216/556 [01:52<02:18,  2.46it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 217/556 [01:52<02:17,  2.46it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 218/556 [01:53<02:17,  2.46it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 219/556 [01:53<02:18,  2.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 220/556 [01:53<02:18,  2.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 221/556 [01:54<02:16,  2.45it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 222/556 [01:54<02:15,  2.46it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 223/556 [01:55<02:17,  2.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 224/556 [01:55<02:15,  2.45it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 225/556 [01:55<02:15,  2.44it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 226/556 [01:56<02:15,  2.43it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 227/556 [01:56<02:14,  2.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 228/556 [01:57<02:13,  2.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 229/556 [01:57<02:12,  2.46it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 230/556 [01:58<02:13,  2.43it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 231/556 [01:58<02:14,  2.43it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 232/556 [01:58<02:12,  2.44it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 233/556 [01:59<02:11,  2.45it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 234/556 [01:59<02:10,  2.46it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 235/556 [02:00<02:10,  2.46it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 236/556 [02:00<02:12,  2.42it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 237/556 [02:00<02:10,  2.44it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 238/556 [02:01<02:09,  2.45it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 239/556 [02:01<02:09,  2.44it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 240/556 [02:02<02:09,  2.45it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 241/556 [02:02<02:10,  2.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 242/556 [02:02<02:10,  2.40it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 243/556 [02:03<02:09,  2.43it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/556 [02:03<02:08,  2.44it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/556 [02:04<02:07,  2.44it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/556 [02:04<02:06,  2.46it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/556 [02:04<02:05,  2.46it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/556 [02:05<02:05,  2.46it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/556 [02:05<02:04,  2.46it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 250/556 [02:06<02:04,  2.47it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 251/556 [02:06<02:05,  2.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 252/556 [02:07<02:05,  2.41it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 253/556 [02:07<02:04,  2.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 254/556 [02:07<02:03,  2.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 255/556 [02:08<02:02,  2.45it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 256/556 [02:08<02:01,  2.46it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 257/556 [02:09<02:01,  2.47it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 258/556 [02:09<02:01,  2.46it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 259/556 [02:09<02:02,  2.42it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 260/556 [02:10<02:03,  2.40it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 261/556 [02:10<02:01,  2.42it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 262/556 [02:11<02:00,  2.44it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 263/556 [02:11<01:59,  2.45it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 264/556 [02:11<01:58,  2.45it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 265/556 [02:12<01:58,  2.46it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 266/556 [02:12<01:58,  2.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 267/556 [02:13<02:00,  2.39it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 268/556 [02:13<02:02,  2.36it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 269/556 [02:14<02:02,  2.35it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 270/556 [02:14<02:00,  2.37it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 271/556 [02:14<01:58,  2.39it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 272/556 [02:15<01:57,  2.42it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 273/556 [02:15<01:56,  2.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 274/556 [02:16<01:54,  2.46it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 275/556 [02:16<01:54,  2.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 276/556 [02:16<01:53,  2.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 277/556 [02:17<01:52,  2.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 278/556 [02:17<01:30,  3.08it/s][INFO|trainer.py:1987] 2022-11-14 18:10:44,427 >> Saving model checkpoint to chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-278
/fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
[INFO|configuration_utils.py:413] 2022-11-14 18:10:44,434 >> Configuration saved in chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-278/config.json
[INFO|modeling_utils.py:1041] 2022-11-14 18:10:49,424 >> Model weights saved in chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-278/pytorch_model.bin
[INFO|tokenization_utils_base.py:2033] 2022-11-14 18:10:49,432 >> tokenizer config file saved in chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-278/tokenizer_config.json
[INFO|tokenization_utils_base.py:2039] 2022-11-14 18:10:49,436 >> Special tokens file saved in chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-278/special_tokens_map.json
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 279/556 [02:33<22:41,  4.91s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 280/556 [02:33<16:22,  3.56s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 281/556 [02:33<11:57,  2.61s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 282/556 [02:34<08:53,  1.95s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 283/556 [02:34<06:44,  1.48s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 284/556 [02:35<05:14,  1.16s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 285/556 [02:35<04:11,  1.08it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 286/556 [02:35<03:27,  1.30it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 287/556 [02:36<02:57,  1.52it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 288/556 [02:36<02:35,  1.72it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 289/556 [02:37<02:20,  1.90it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 290/556 [02:37<02:09,  2.05it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 291/556 [02:37<02:02,  2.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 292/556 [02:38<01:56,  2.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 293/556 [02:38<01:52,  2.33it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 294/556 [02:39<01:50,  2.37it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 295/556 [02:39<01:48,  2.41it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 296/556 [02:39<01:47,  2.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 297/556 [02:40<01:45,  2.45it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 298/556 [02:40<01:44,  2.46it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 299/556 [02:41<01:44,  2.46it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/556 [02:41<01:43,  2.47it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/556 [02:41<01:42,  2.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/556 [02:42<01:42,  2.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/556 [02:42<01:41,  2.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/556 [02:43<01:41,  2.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/556 [02:43<01:40,  2.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 306/556 [02:43<01:40,  2.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 307/556 [02:44<01:40,  2.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 308/556 [02:44<01:39,  2.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 309/556 [02:45<01:39,  2.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 310/556 [02:45<01:38,  2.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 311/556 [02:45<01:38,  2.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 312/556 [02:46<01:37,  2.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 313/556 [02:46<01:37,  2.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 314/556 [02:47<01:37,  2.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 315/556 [02:47<01:37,  2.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 316/556 [02:47<01:36,  2.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 317/556 [02:48<01:36,  2.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 318/556 [02:48<01:36,  2.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 319/556 [02:49<01:35,  2.47it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 320/556 [02:49<01:35,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 321/556 [02:49<01:34,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 322/556 [02:50<01:34,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 323/556 [02:50<01:34,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 324/556 [02:51<01:33,  2.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 325/556 [02:51<01:33,  2.47it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 326/556 [02:51<01:33,  2.47it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 327/556 [02:52<01:32,  2.46it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 328/556 [02:52<01:32,  2.47it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 329/556 [02:53<01:31,  2.47it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 330/556 [02:53<01:31,  2.47it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 331/556 [02:53<01:31,  2.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 332/556 [02:54<01:31,  2.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 333/556 [02:54<01:30,  2.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 334/556 [02:55<01:30,  2.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 335/556 [02:55<01:29,  2.47it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 336/556 [02:55<01:29,  2.47it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 337/556 [02:56<01:28,  2.47it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 338/556 [02:56<01:28,  2.47it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 339/556 [02:57<01:28,  2.46it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 340/556 [02:57<01:27,  2.46it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 341/556 [02:58<01:27,  2.47it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 342/556 [02:58<01:27,  2.44it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 343/556 [02:58<01:27,  2.44it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 344/556 [02:59<01:26,  2.44it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 345/556 [02:59<01:26,  2.45it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 346/556 [03:00<01:25,  2.46it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 347/556 [03:00<01:26,  2.41it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 348/556 [03:00<01:26,  2.41it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 349/556 [03:01<01:25,  2.42it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 350/556 [03:01<01:24,  2.44it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 351/556 [03:02<01:23,  2.44it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 352/556 [03:02<01:23,  2.45it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 353/556 [03:02<01:22,  2.46it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 354/556 [03:03<01:22,  2.44it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 355/556 [03:03<01:22,  2.44it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 356/556 [03:04<01:21,  2.45it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 357/556 [03:04<01:21,  2.45it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 358/556 [03:04<01:21,  2.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/556 [03:05<01:21,  2.42it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/556 [03:05<01:20,  2.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/556 [03:06<01:19,  2.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 362/556 [03:06<01:19,  2.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 363/556 [03:07<01:18,  2.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 364/556 [03:07<01:18,  2.45it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 365/556 [03:07<01:17,  2.46it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 366/556 [03:08<01:17,  2.45it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 367/556 [03:08<01:17,  2.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 368/556 [03:09<01:17,  2.42it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 369/556 [03:09<01:16,  2.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 370/556 [03:09<01:16,  2.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 371/556 [03:10<01:15,  2.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 372/556 [03:10<01:14,  2.46it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 373/556 [03:11<01:15,  2.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 374/556 [03:11<01:15,  2.42it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 375/556 [03:11<01:14,  2.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 376/556 [03:12<01:13,  2.45it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 377/556 [03:12<01:12,  2.46it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 378/556 [03:13<01:12,  2.46it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 379/556 [03:13<01:11,  2.46it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 380/556 [03:13<01:11,  2.47it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 381/556 [03:14<01:11,  2.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 382/556 [03:14<01:12,  2.41it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 383/556 [03:15<01:12,  2.38it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 384/556 [03:15<01:11,  2.41it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 385/556 [03:16<01:10,  2.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 386/556 [03:16<01:09,  2.45it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 387/556 [03:16<01:08,  2.46it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 388/556 [03:17<01:08,  2.46it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 389/556 [03:17<01:07,  2.46it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 390/556 [03:18<01:07,  2.47it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 391/556 [03:18<01:06,  2.47it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 392/556 [03:18<01:07,  2.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 393/556 [03:19<01:07,  2.41it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 394/556 [03:19<01:06,  2.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 395/556 [03:20<01:05,  2.45it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 396/556 [03:20<01:05,  2.46it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 397/556 [03:20<01:04,  2.46it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 398/556 [03:21<01:04,  2.45it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 399/556 [03:21<01:04,  2.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 400/556 [03:22<01:03,  2.45it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 401/556 [03:22<01:03,  2.46it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 402/556 [03:22<01:03,  2.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 403/556 [03:23<01:03,  2.39it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 404/556 [03:23<01:03,  2.40it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 405/556 [03:24<01:02,  2.42it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 406/556 [03:24<01:01,  2.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 407/556 [03:25<01:00,  2.45it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 408/556 [03:25<00:59,  2.47it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 409/556 [03:25<00:59,  2.47it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 410/556 [03:26<00:59,  2.45it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 411/556 [03:26<00:59,  2.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 412/556 [03:27<00:59,  2.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 413/556 [03:27<00:58,  2.46it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 414/556 [03:27<00:57,  2.47it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 415/556 [03:28<00:57,  2.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 416/556 [03:28<00:58,  2.41it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 417/556 [03:29<00:57,  2.42it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 418/556 [03:29<00:56,  2.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 419/556 [03:29<00:55,  2.45it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 420/556 [03:30<00:55,  2.46it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 421/556 [03:30<00:54,  2.47it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 422/556 [03:31<00:54,  2.45it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 423/556 [03:31<00:55,  2.41it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 424/556 [03:32<00:54,  2.41it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 425/556 [03:32<00:53,  2.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 426/556 [03:32<00:53,  2.45it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 427/556 [03:33<00:52,  2.46it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 428/556 [03:33<00:51,  2.47it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 429/556 [03:34<00:52,  2.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 430/556 [03:34<00:52,  2.41it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 431/556 [03:34<00:51,  2.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 432/556 [03:35<00:50,  2.45it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 433/556 [03:35<00:49,  2.46it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 434/556 [03:36<00:49,  2.45it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 435/556 [03:36<00:49,  2.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 436/556 [03:36<00:48,  2.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 437/556 [03:37<00:49,  2.42it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 438/556 [03:37<00:48,  2.42it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 439/556 [03:38<00:48,  2.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 440/556 [03:38<00:47,  2.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 441/556 [03:38<00:46,  2.46it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 442/556 [03:39<00:46,  2.47it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 443/556 [03:39<00:46,  2.44it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 444/556 [03:40<00:46,  2.40it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 445/556 [03:40<00:46,  2.37it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 446/556 [03:41<00:46,  2.35it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 447/556 [03:41<00:46,  2.36it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 448/556 [03:41<00:45,  2.37it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 449/556 [03:42<00:44,  2.40it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 450/556 [03:42<00:43,  2.43it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 451/556 [03:43<00:42,  2.45it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 452/556 [03:43<00:42,  2.47it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 453/556 [03:43<00:41,  2.47it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 454/556 [03:44<00:41,  2.49it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 455/556 [03:44<00:40,  2.49it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 456/556 [03:45<00:40,  2.49it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 457/556 [03:45<00:39,  2.49it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 458/556 [03:45<00:39,  2.49it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 459/556 [03:46<00:38,  2.49it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 460/556 [03:46<00:39,  2.46it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 461/556 [03:47<00:39,  2.41it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 462/556 [03:47<00:38,  2.41it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 463/556 [03:48<00:38,  2.42it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 464/556 [03:48<00:37,  2.45it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 465/556 [03:48<00:36,  2.46it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 466/556 [03:49<00:36,  2.47it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 467/556 [03:49<00:36,  2.45it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 468/556 [03:50<00:36,  2.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 469/556 [03:50<00:36,  2.37it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 470/556 [03:50<00:36,  2.35it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 471/556 [03:51<00:36,  2.35it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 472/556 [03:51<00:35,  2.37it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 473/556 [03:52<00:34,  2.40it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 474/556 [03:52<00:33,  2.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 475/556 [03:52<00:33,  2.45it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 476/556 [03:53<00:32,  2.47it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 477/556 [03:53<00:31,  2.48it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 478/556 [03:54<00:31,  2.49it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 479/556 [03:54<00:30,  2.49it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 480/556 [03:54<00:30,  2.50it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 481/556 [03:55<00:29,  2.50it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 482/556 [03:55<00:29,  2.50it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 483/556 [03:56<00:29,  2.48it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 484/556 [03:56<00:29,  2.46it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 485/556 [03:56<00:28,  2.46it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 486/556 [03:57<00:28,  2.48it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 487/556 [03:57<00:28,  2.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 488/556 [03:58<00:27,  2.46it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 489/556 [03:58<00:27,  2.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 490/556 [03:59<00:27,  2.41it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 491/556 [03:59<00:27,  2.39it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 492/556 [03:59<00:27,  2.36it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 493/556 [04:00<00:26,  2.37it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 494/556 [04:00<00:25,  2.39it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 495/556 [04:01<00:25,  2.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 496/556 [04:01<00:24,  2.45it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 497/556 [04:01<00:23,  2.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 498/556 [04:02<00:23,  2.48it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 499/556 [04:02<00:22,  2.48it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 500/556 [04:03<00:22,  2.49it/s]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 500/556 [04:03<00:22,  2.49it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 501/556 [04:03<00:22,  2.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 502/556 [04:03<00:22,  2.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 503/556 [04:04<00:21,  2.42it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 504/556 [04:04<00:21,  2.42it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 505/556 [04:05<00:20,  2.45it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 506/556 [04:05<00:20,  2.46it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 507/556 [04:06<00:19,  2.47it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 508/556 [04:06<00:19,  2.48it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 509/556 [04:06<00:19,  2.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 510/556 [04:07<00:18,  2.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 511/556 [04:07<00:18,  2.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 512/556 [04:08<00:17,  2.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 513/556 [04:08<00:17,  2.46it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 514/556 [04:08<00:17,  2.43it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 515/556 [04:09<00:16,  2.42it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 516/556 [04:09<00:16,  2.43it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 517/556 [04:10<00:15,  2.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 518/556 [04:10<00:15,  2.46it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 519/556 [04:10<00:14,  2.47it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 520/556 [04:11<00:14,  2.47it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 521/556 [04:11<00:14,  2.45it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 522/556 [04:12<00:13,  2.46it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 523/556 [04:12<00:13,  2.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 524/556 [04:12<00:13,  2.42it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 525/556 [04:13<00:12,  2.43it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 526/556 [04:13<00:12,  2.45it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 527/556 [04:14<00:11,  2.47it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 528/556 [04:14<00:11,  2.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 529/556 [04:15<00:11,  2.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 530/556 [04:15<00:10,  2.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 531/556 [04:15<00:10,  2.40it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 532/556 [04:16<00:10,  2.37it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 533/556 [04:16<00:09,  2.35it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 534/556 [04:17<00:09,  2.37it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 535/556 [04:17<00:08,  2.39it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 536/556 [04:17<00:08,  2.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 537/556 [04:18<00:07,  2.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 538/556 [04:18<00:07,  2.46it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 539/556 [04:19<00:06,  2.47it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 540/556 [04:19<00:06,  2.48it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 541/556 [04:19<00:06,  2.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 542/556 [04:20<00:05,  2.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 543/556 [04:20<00:05,  2.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 544/556 [04:21<00:04,  2.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 545/556 [04:21<00:04,  2.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 546/556 [04:21<00:04,  2.45it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 547/556 [04:22<00:03,  2.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 548/556 [04:22<00:03,  2.45it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 549/556 [04:23<00:02,  2.46it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 550/556 [04:23<00:02,  2.46it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 551/556 [04:24<00:02,  2.47it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 552/556 [04:24<00:01,  2.46it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 553/556 [04:24<00:01,  2.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 554/556 [04:25<00:00,  2.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 555/556 [04:25<00:00,  2.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 556/556 [04:25<00:00,  3.04it/s][INFO|trainer.py:1987] 2022-11-14 18:12:52,807 >> Saving model checkpoint to chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-556
/fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
[INFO|configuration_utils.py:413] 2022-11-14 18:12:52,816 >> Configuration saved in chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-556/config.json
[INFO|modeling_utils.py:1041] 2022-11-14 18:12:57,578 >> Model weights saved in chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-556/pytorch_model.bin
[INFO|tokenization_utils_base.py:2033] 2022-11-14 18:12:57,585 >> tokenizer config file saved in chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-556/tokenizer_config.json
[INFO|tokenization_utils_base.py:2039] 2022-11-14 18:12:57,589 >> Special tokens file saved in chkpts/aflite_embedding_models/d-snli-roberta-base/checkpoint-556/special_tokens_map.json
[INFO|trainer.py:1401] 2022-11-14 18:13:07,490 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 556/556 [04:40<00:00,  3.04it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 556/556 [04:40<00:00,  1.98it/s]
[INFO|trainer.py:1987] 2022-11-14 18:13:07,491 >> Saving model checkpoint to chkpts/aflite_embedding_models/d-snli-roberta-base
/fs/clip-scratch/nehasrik/miniconda3/envs/para-nlu/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
[INFO|configuration_utils.py:413] 2022-11-14 18:13:07,498 >> Configuration saved in chkpts/aflite_embedding_models/d-snli-roberta-base/config.json
[INFO|modeling_utils.py:1041] 2022-11-14 18:13:12,353 >> Model weights saved in chkpts/aflite_embedding_models/d-snli-roberta-base/pytorch_model.bin
[INFO|tokenization_utils_base.py:2033] 2022-11-14 18:13:12,359 >> tokenizer config file saved in chkpts/aflite_embedding_models/d-snli-roberta-base/tokenizer_config.json
[INFO|tokenization_utils_base.py:2039] 2022-11-14 18:13:12,363 >> Special tokens file saved in chkpts/aflite_embedding_models/d-snli-roberta-base/special_tokens_map.json
{'loss': 0.6918, 'learning_rate': 2.0143884892086333e-06, 'epoch': 1.8}
{'train_runtime': 280.4964, 'train_samples_per_second': 63.259, 'train_steps_per_second': 1.982, 'train_loss': 0.6887636116082719, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.6888
  train_runtime            = 0:04:40.49
  train_samples            =       8872
  train_samples_per_second =     63.259
  train_steps_per_second   =      1.982
11/14/2022 18:13:12 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:541] 2022-11-14 18:13:12,529 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.
[WARNING|training_args.py:888] 2022-11-14 18:13:12,531 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2235] 2022-11-14 18:13:12,531 >> ***** Running Evaluation *****
[INFO|trainer.py:2237] 2022-11-14 18:13:12,531 >>   Num examples = 1005
[INFO|trainer.py:2240] 2022-11-14 18:13:12,531 >>   Batch size = 32
  0%|          | 0/32 [00:00<?, ?it/s]  6%|â–‹         | 2/32 [00:00<00:02, 12.51it/s] 12%|â–ˆâ–Ž        | 4/32 [00:00<00:03,  8.04it/s] 16%|â–ˆâ–Œ        | 5/32 [00:00<00:03,  7.52it/s] 19%|â–ˆâ–‰        | 6/32 [00:00<00:03,  7.21it/s] 22%|â–ˆâ–ˆâ–       | 7/32 [00:00<00:03,  6.97it/s] 25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:01<00:03,  6.83it/s] 28%|â–ˆâ–ˆâ–Š       | 9/32 [00:01<00:03,  6.71it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:01<00:03,  6.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:01<00:03,  6.59it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:01<00:03,  6.55it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:01<00:02,  6.53it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:02<00:02,  6.50it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:02<00:02,  6.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:02<00:02,  6.45it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:02<00:02,  6.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:02<00:02,  6.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:02<00:02,  6.44it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:02<00:01,  6.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:03<00:01,  6.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:03<00:01,  6.45it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:03<00:01,  6.46it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:03<00:01,  6.47it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:03<00:01,  6.48it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:03<00:00,  6.47it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:04<00:00,  6.49it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:04<00:00,  6.46it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:04<00:00,  6.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:04<00:00,  6.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:04<00:00,  6.44it/s][WARNING|training_args.py:888] 2022-11-14 18:13:17,441 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04<00:00,  6.79it/s]
[WARNING|training_args.py:888] 2022-11-14 18:13:17,451 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:888] 2022-11-14 18:13:17,451 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:446] 2022-11-14 18:13:17,503 >> Dropping the following result as it does not have all the necessary field:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.606965184211731}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =      0.607
  eval_loss               =     0.6573
  eval_runtime            = 0:00:04.91
  eval_samples            =       1005
  eval_samples_per_second =    204.654
  eval_steps_per_second   =      6.516
