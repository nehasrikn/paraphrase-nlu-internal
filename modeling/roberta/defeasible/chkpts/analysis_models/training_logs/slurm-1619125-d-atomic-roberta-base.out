10/30/2023 16:35:06 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
10/30/2023 16:35:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=chkpts/analysis_models/d-atomic-roberta-base/runs/Oct30_16-35-06_clip09.umiacs.umd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=chkpts/analysis_models/d-atomic-roberta-base,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=chkpts/analysis_models/d-atomic-roberta-base,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
10/30/2023 16:35:06 - INFO - __main__ - load a local file for train: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/atomic/analysis_model_examples/train_examples.csv
10/30/2023 16:35:06 - INFO - __main__ - load a local file for validation: /fs/clip-projects/rlab/nehasrik/paraphrase-nlu/data_selection/defeasible/atomic/analysis_model_examples/dev_examples.csv
10/30/2023 16:35:07 - WARNING - datasets.builder - Using custom data configuration default-20eb8091327cd7b5
10/30/2023 16:35:07 - INFO - datasets.builder - Generating dataset csv (/fs/clip-projects/rlab/nehasrik/cache/csv/default-20eb8091327cd7b5/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)
Downloading and preparing dataset csv/default to /fs/clip-projects/rlab/nehasrik/cache/csv/default-20eb8091327cd7b5/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 10837.99it/s]10/30/2023 16:35:07 - INFO - datasets.download.download_manager - Downloading took 0.0 min
10/30/2023 16:35:07 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min

Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 79.04it/s]10/30/2023 16:35:07 - INFO - datasets.utils.info_utils - Unable to verify checksums.
10/30/2023 16:35:07 - INFO - datasets.builder - Generating train split

0 tables [00:00, ? tables/s]1 tables [00:00,  9.52 tables/s]                                10/30/2023 16:35:07 - INFO - datasets.builder - Generating validation split
0 tables [00:00, ? tables/s]                            10/30/2023 16:35:07 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /fs/clip-projects/rlab/nehasrik/cache/csv/default-20eb8091327cd7b5/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 182.92it/s]
[INFO|configuration_utils.py:659] 2023-10-30 16:35:07,822 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-projects/rlab/nehasrik/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-10-30 16:35:07,827 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:371] 2023-10-30 16:35:07,870 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:659] 2023-10-30 16:35:07,917 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-projects/rlab/nehasrik/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-10-30 16:35:07,918 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1782] 2023-10-30 16:35:08,180 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /fs/clip-projects/rlab/nehasrik/cache/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1782] 2023-10-30 16:35:08,180 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /fs/clip-projects/rlab/nehasrik/cache/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1782] 2023-10-30 16:35:08,180 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-10-30 16:35:08,180 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1782] 2023-10-30 16:35:08,180 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:659] 2023-10-30 16:35:08,228 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /fs/clip-projects/rlab/nehasrik/cache/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
[INFO|configuration_utils.py:708] 2023-10-30 16:35:08,229 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.4",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1953] 2023-10-30 16:35:08,616 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /fs/clip-projects/rlab/nehasrik/cache/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
[WARNING|modeling_utils.py:2254] 2023-10-30 16:35:14,694 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2265] 2023-10-30 16:35:14,694 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
sentence1 sentence2
S1+S2 KEYS sentence1 sentence2
10/30/2023 16:35:15 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fdd3276b280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on dataset:   0%|          | 0/29 [00:00<?, ?ba/s]10/30/2023 16:35:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-projects/rlab/nehasrik/cache/csv/default-20eb8091327cd7b5/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow
Running tokenizer on dataset:   3%|▎         | 1/29 [00:00<00:10,  2.64ba/s]Running tokenizer on dataset:   7%|▋         | 2/29 [00:00<00:09,  2.91ba/s]Running tokenizer on dataset:  10%|█         | 3/29 [00:00<00:08,  3.12ba/s]Running tokenizer on dataset:  14%|█▍        | 4/29 [00:01<00:07,  3.33ba/s]Running tokenizer on dataset:  17%|█▋        | 5/29 [00:01<00:06,  3.58ba/s]Running tokenizer on dataset:  21%|██        | 6/29 [00:01<00:06,  3.79ba/s]Running tokenizer on dataset:  24%|██▍       | 7/29 [00:02<00:06,  3.30ba/s]Running tokenizer on dataset:  28%|██▊       | 8/29 [00:02<00:06,  3.41ba/s]Running tokenizer on dataset:  31%|███       | 9/29 [00:02<00:05,  3.48ba/s]Running tokenizer on dataset:  34%|███▍      | 10/29 [00:02<00:05,  3.59ba/s]Running tokenizer on dataset:  38%|███▊      | 11/29 [00:03<00:04,  3.80ba/s]Running tokenizer on dataset:  41%|████▏     | 12/29 [00:03<00:04,  3.97ba/s]Running tokenizer on dataset:  45%|████▍     | 13/29 [00:03<00:04,  3.93ba/s]Running tokenizer on dataset:  48%|████▊     | 14/29 [00:03<00:03,  3.83ba/s]Running tokenizer on dataset:  52%|█████▏    | 15/29 [00:04<00:03,  3.79ba/s]Running tokenizer on dataset:  55%|█████▌    | 16/29 [00:04<00:03,  3.77ba/s]Running tokenizer on dataset:  59%|█████▊    | 17/29 [00:04<00:03,  3.87ba/s]Running tokenizer on dataset:  62%|██████▏   | 18/29 [00:04<00:02,  4.03ba/s]Running tokenizer on dataset:  66%|██████▌   | 19/29 [00:05<00:02,  4.16ba/s]Running tokenizer on dataset:  69%|██████▉   | 20/29 [00:05<00:02,  4.01ba/s]Running tokenizer on dataset:  72%|███████▏  | 21/29 [00:05<00:02,  3.89ba/s]Running tokenizer on dataset:  76%|███████▌  | 22/29 [00:05<00:01,  3.87ba/s]Running tokenizer on dataset:  79%|███████▉  | 23/29 [00:06<00:01,  3.83ba/s]Running tokenizer on dataset:  83%|████████▎ | 24/29 [00:06<00:01,  3.97ba/s]Running tokenizer on dataset:  86%|████████▌ | 25/29 [00:06<00:00,  4.11ba/s]Running tokenizer on dataset:  90%|████████▉ | 26/29 [00:06<00:00,  4.18ba/s]Running tokenizer on dataset:  93%|█████████▎| 27/29 [00:07<00:00,  3.46ba/s]Running tokenizer on dataset:  97%|█████████▋| 28/29 [00:07<00:00,  3.53ba/s]Running tokenizer on dataset: 100%|██████████| 29/29 [00:07<00:00,  3.78ba/s]10/30/2023 16:35:24 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fdd327c01f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.

Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]10/30/2023 16:35:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /fs/clip-projects/rlab/nehasrik/cache/csv/default-20eb8091327cd7b5/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow
Running tokenizer on dataset:  33%|███▎      | 1/3 [00:00<00:00,  3.52ba/s]Running tokenizer on dataset:  67%|██████▋   | 2/3 [00:00<00:00,  3.69ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  4.09ba/s]Running tokenizer on dataset: 100%|██████████| 3/3 [00:00<00:00,  3.95ba/s]hi
10/30/2023 16:35:24 - INFO - __main__ - Sample 9012 of the training set: {'sentence1': 'PersonX lives happily ever after As a result, PersonX wants to build a house', 'sentence2': 'They are a contractor with house building experience', 'label': 1, 'input_ids': [0, 41761, 1000, 1074, 16534, 655, 71, 287, 10, 898, 6, 18404, 1000, 1072, 7, 1119, 10, 790, 2, 2, 1213, 32, 10, 9254, 19, 790, 745, 676, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
10/30/2023 16:35:24 - INFO - __main__ - Sample 8024 of the training set: {'sentence1': 'PersonX hands it to PersonY As a result, PersonX feels sincere', 'sentence2': 'PersonX gives a smile to PerosnY', 'label': 1, 'input_ids': [0, 41761, 1000, 1420, 24, 7, 18404, 975, 287, 10, 898, 6, 18404, 1000, 2653, 19255, 2, 2, 41761, 1000, 2029, 10, 6675, 7, 2595, 366, 282, 975, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
10/30/2023 16:35:24 - INFO - __main__ - Sample 7314 of the training set: {'sentence1': 'PersonX wears flip flops As a result, PersonX wants to walk around the house', 'sentence2': 'PersonX is in their bedroom', 'label': 1, 'input_ids': [0, 41761, 1000, 15033, 11113, 2342, 5090, 287, 10, 898, 6, 18404, 1000, 1072, 7, 1656, 198, 5, 790, 2, 2, 41761, 1000, 16, 11, 49, 8140, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.

[INFO|trainer.py:622] 2023-10-30 16:35:36,712 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
/fs/clip-projects/rlab/nehasrik/miniconda3/envs/para-nlu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1419] 2023-10-30 16:35:36,728 >> ***** Running training *****
[INFO|trainer.py:1420] 2023-10-30 16:35:36,728 >>   Num examples = 28350
[INFO|trainer.py:1421] 2023-10-30 16:35:36,728 >>   Num Epochs = 2
[INFO|trainer.py:1422] 2023-10-30 16:35:36,728 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1423] 2023-10-30 16:35:36,728 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1424] 2023-10-30 16:35:36,728 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1425] 2023-10-30 16:35:36,728 >>   Total optimization steps = 886
[WARNING|training_args.py:1095] 2023-10-30 16:35:36,742 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
  0%|          | 0/886 [00:00<?, ?it/s]  0%|          | 1/886 [00:06<1:31:26,  6.20s/it]  0%|          | 2/886 [00:12<1:29:55,  6.10s/it]  0%|          | 3/886 [00:18<1:29:23,  6.07s/it]  0%|          | 4/886 [00:24<1:29:07,  6.06s/it]  1%|          | 5/886 [00:30<1:28:54,  6.06s/it]  1%|          | 6/886 [00:36<1:28:45,  6.05s/it]  1%|          | 7/886 [00:42<1:28:37,  6.05s/it]  1%|          | 8/886 [00:48<1:28:36,  6.06s/it]  1%|          | 9/886 [00:54<1:28:28,  6.05s/it]  1%|          | 10/886 [01:00<1:28:21,  6.05s/it]  1%|          | 11/886 [01:06<1:28:15,  6.05s/it]  1%|▏         | 12/886 [01:12<1:28:08,  6.05s/it]  1%|▏         | 13/886 [01:18<1:28:01,  6.05s/it]  2%|▏         | 14/886 [01:24<1:27:55,  6.05s/it]  2%|▏         | 15/886 [01:30<1:27:49,  6.05s/it]  2%|▏         | 16/886 [01:36<1:27:43,  6.05s/it]  2%|▏         | 17/886 [01:42<1:27:37,  6.05s/it]  2%|▏         | 18/886 [01:49<1:27:34,  6.05s/it]  2%|▏         | 19/886 [01:55<1:27:28,  6.05s/it]  2%|▏         | 20/886 [02:01<1:27:23,  6.05s/it]  2%|▏         | 21/886 [02:07<1:27:16,  6.05s/it]  2%|▏         | 22/886 [02:13<1:27:11,  6.06s/it]  3%|▎         | 23/886 [02:19<1:27:05,  6.06s/it]  3%|▎         | 24/886 [02:25<1:27:09,  6.07s/it]  3%|▎         | 25/886 [02:31<1:27:14,  6.08s/it]  3%|▎         | 26/886 [02:37<1:27:16,  6.09s/it]  3%|▎         | 27/886 [02:43<1:27:15,  6.10s/it]  3%|▎         | 28/886 [02:49<1:27:13,  6.10s/it]  3%|▎         | 29/886 [02:55<1:27:10,  6.10s/it]  3%|▎         | 30/886 [03:02<1:27:06,  6.11s/it]  3%|▎         | 31/886 [03:08<1:27:01,  6.11s/it]  4%|▎         | 32/886 [03:14<1:26:55,  6.11s/it]  4%|▎         | 33/886 [03:20<1:26:49,  6.11s/it]  4%|▍         | 34/886 [03:26<1:26:44,  6.11s/it]  4%|▍         | 35/886 [03:32<1:26:38,  6.11s/it]  4%|▍         | 36/886 [03:38<1:26:32,  6.11s/it]  4%|▍         | 37/886 [03:44<1:26:27,  6.11s/it]  4%|▍         | 38/886 [03:50<1:26:21,  6.11s/it]  4%|▍         | 39/886 [03:57<1:26:14,  6.11s/it]  5%|▍         | 40/886 [04:03<1:26:09,  6.11s/it]  5%|▍         | 41/886 [04:09<1:26:02,  6.11s/it]  5%|▍         | 42/886 [04:15<1:25:56,  6.11s/it]  5%|▍         | 43/886 [04:21<1:25:51,  6.11s/it]  5%|▍         | 44/886 [04:27<1:25:45,  6.11s/it]  5%|▌         | 45/886 [04:33<1:25:39,  6.11s/it]  5%|▌         | 46/886 [04:39<1:25:33,  6.11s/it]  5%|▌         | 47/886 [04:45<1:25:26,  6.11s/it]  5%|▌         | 48/886 [04:52<1:25:21,  6.11s/it]  6%|▌         | 49/886 [04:58<1:25:15,  6.11s/it]  6%|▌         | 50/886 [05:04<1:25:10,  6.11s/it]  6%|▌         | 51/886 [05:10<1:25:04,  6.11s/it]  6%|▌         | 52/886 [05:16<1:24:57,  6.11s/it]  6%|▌         | 53/886 [05:22<1:24:51,  6.11s/it]  6%|▌         | 54/886 [05:28<1:24:45,  6.11s/it]  6%|▌         | 55/886 [05:34<1:24:39,  6.11s/it]  6%|▋         | 56/886 [05:40<1:24:34,  6.11s/it]  6%|▋         | 57/886 [05:47<1:24:29,  6.12s/it]  7%|▋         | 58/886 [05:53<1:24:24,  6.12s/it]  7%|▋         | 59/886 [05:59<1:24:18,  6.12s/it]  7%|▋         | 60/886 [06:05<1:24:11,  6.12s/it]  7%|▋         | 61/886 [06:11<1:24:05,  6.12s/it]  7%|▋         | 62/886 [06:17<1:23:58,  6.11s/it]  7%|▋         | 63/886 [06:23<1:23:51,  6.11s/it]  7%|▋         | 64/886 [06:29<1:23:45,  6.11s/it]  7%|▋         | 65/886 [06:35<1:23:39,  6.11s/it]  7%|▋         | 66/886 [06:42<1:23:33,  6.11s/it]  8%|▊         | 67/886 [06:48<1:23:27,  6.11s/it]  8%|▊         | 68/886 [06:54<1:23:21,  6.11s/it]  8%|▊         | 69/886 [07:00<1:23:15,  6.11s/it]  8%|▊         | 70/886 [07:06<1:23:09,  6.11s/it]  8%|▊         | 71/886 [07:12<1:23:03,  6.12s/it]  8%|▊         | 72/886 [07:18<1:22:57,  6.11s/it]  8%|▊         | 73/886 [07:24<1:22:51,  6.12s/it]  8%|▊         | 74/886 [07:31<1:22:45,  6.11s/it]  8%|▊         | 75/886 [07:37<1:22:39,  6.11s/it]  9%|▊         | 76/886 [07:43<1:22:33,  6.12s/it]  9%|▊         | 77/886 [07:49<1:22:27,  6.12s/it]  9%|▉         | 78/886 [07:55<1:22:21,  6.12s/it]  9%|▉         | 79/886 [08:01<1:22:15,  6.12s/it]  9%|▉         | 80/886 [08:07<1:22:08,  6.12s/it]  9%|▉         | 81/886 [08:13<1:22:02,  6.11s/it]  9%|▉         | 82/886 [08:19<1:21:56,  6.12s/it]  9%|▉         | 83/886 [08:26<1:21:51,  6.12s/it]  9%|▉         | 84/886 [08:32<1:21:45,  6.12s/it] 10%|▉         | 85/886 [08:38<1:21:38,  6.12s/it] 10%|▉         | 86/886 [08:44<1:21:34,  6.12s/it] 10%|▉         | 87/886 [08:50<1:21:27,  6.12s/it] 10%|▉         | 88/886 [08:56<1:21:20,  6.12s/it] 10%|█         | 89/886 [09:02<1:21:14,  6.12s/it] 10%|█         | 90/886 [09:08<1:21:08,  6.12s/it] 10%|█         | 91/886 [09:14<1:21:02,  6.12s/it] 10%|█         | 92/886 [09:21<1:20:56,  6.12s/it] 10%|█         | 93/886 [09:27<1:20:49,  6.12s/it] 11%|█         | 94/886 [09:33<1:20:43,  6.12s/it] 11%|█         | 95/886 [09:39<1:20:37,  6.12s/it] 11%|█         | 96/886 [09:45<1:20:31,  6.12s/it] 11%|█         | 97/886 [09:51<1:20:24,  6.12s/it] 11%|█         | 98/886 [09:57<1:20:18,  6.12s/it] 11%|█         | 99/886 [10:03<1:20:13,  6.12s/it] 11%|█▏        | 100/886 [10:10<1:20:07,  6.12s/it] 11%|█▏        | 101/886 [10:16<1:20:01,  6.12s/it] 12%|█▏        | 102/886 [10:22<1:19:54,  6.12s/it] 12%|█▏        | 103/886 [10:28<1:19:47,  6.11s/it] 12%|█▏        | 104/886 [10:34<1:19:41,  6.11s/it] 12%|█▏        | 105/886 [10:40<1:19:36,  6.12s/it] 12%|█▏        | 106/886 [10:46<1:19:30,  6.12s/it] 12%|█▏        | 107/886 [10:52<1:19:24,  6.12s/it] 12%|█▏        | 108/886 [10:58<1:19:19,  6.12s/it] 12%|█▏        | 109/886 [11:05<1:19:13,  6.12s/it] 12%|█▏        | 110/886 [11:11<1:19:06,  6.12s/it] 13%|█▎        | 111/886 [11:17<1:19:00,  6.12s/it] 13%|█▎        | 112/886 [11:23<1:18:53,  6.12s/it] 13%|█▎        | 113/886 [11:29<1:18:47,  6.12s/it] 13%|█▎        | 114/886 [11:35<1:18:40,  6.12s/it] 13%|█▎        | 115/886 [11:41<1:18:35,  6.12s/it] 13%|█▎        | 116/886 [11:47<1:18:28,  6.12s/it] 13%|█▎        | 117/886 [11:54<1:18:23,  6.12s/it] 13%|█▎        | 118/886 [12:00<1:18:17,  6.12s/it] 13%|█▎        | 119/886 [12:06<1:18:11,  6.12s/it] 14%|█▎        | 120/886 [12:12<1:18:04,  6.12s/it] 14%|█▎        | 121/886 [12:18<1:17:58,  6.12s/it] 14%|█▍        | 122/886 [12:24<1:17:52,  6.12s/it] 14%|█▍        | 123/886 [12:30<1:17:45,  6.11s/it] 14%|█▍        | 124/886 [12:36<1:17:39,  6.11s/it] 14%|█▍        | 125/886 [12:42<1:17:33,  6.11s/it] 14%|█▍        | 126/886 [12:49<1:17:27,  6.11s/it] 14%|█▍        | 127/886 [12:55<1:17:20,  6.11s/it] 14%|█▍        | 128/886 [13:01<1:17:16,  6.12s/it] 15%|█▍        | 129/886 [13:07<1:17:09,  6.12s/it] 15%|█▍        | 130/886 [13:13<1:17:03,  6.12s/it] 15%|█▍        | 131/886 [13:19<1:16:57,  6.12s/it] 15%|█▍        | 132/886 [13:25<1:16:51,  6.12s/it] 15%|█▌        | 133/886 [13:31<1:16:44,  6.12s/it] 15%|█▌        | 134/886 [13:37<1:16:37,  6.11s/it] 15%|█▌        | 135/886 [13:44<1:16:32,  6.12s/it] 15%|█▌        | 136/886 [13:50<1:16:27,  6.12s/it] 15%|█▌        | 137/886 [13:56<1:16:20,  6.12s/it] 16%|█▌        | 138/886 [14:02<1:16:14,  6.12s/it] 16%|█▌        | 139/886 [14:08<1:16:08,  6.12s/it] 16%|█▌        | 140/886 [14:14<1:16:01,  6.11s/it] 16%|█▌        | 141/886 [14:20<1:15:55,  6.11s/it] 16%|█▌        | 142/886 [14:26<1:15:49,  6.12s/it] 16%|█▌        | 143/886 [14:33<1:15:43,  6.12s/it] 16%|█▋        | 144/886 [14:39<1:15:37,  6.11s/it] 16%|█▋        | 145/886 [14:45<1:15:30,  6.11s/it] 16%|█▋        | 146/886 [14:51<1:15:24,  6.11s/it] 17%|█▋        | 147/886 [14:57<1:15:17,  6.11s/it] 17%|█▋        | 148/886 [15:03<1:15:11,  6.11s/it] 17%|█▋        | 149/886 [15:09<1:15:05,  6.11s/it] 17%|█▋        | 150/886 [15:15<1:14:59,  6.11s/it] 17%|█▋        | 151/886 [15:21<1:14:52,  6.11s/it] 17%|█▋        | 152/886 [15:28<1:14:46,  6.11s/it] 17%|█▋        | 153/886 [15:34<1:14:40,  6.11s/it] 17%|█▋        | 154/886 [15:40<1:14:35,  6.11s/it] 17%|█▋        | 155/886 [15:46<1:14:29,  6.11s/it] 18%|█▊        | 156/886 [15:52<1:14:23,  6.11s/it] 18%|█▊        | 157/886 [15:58<1:14:16,  6.11s/it] 18%|█▊        | 158/886 [16:04<1:14:09,  6.11s/it] 18%|█▊        | 159/886 [16:10<1:14:04,  6.11s/it] 18%|█▊        | 160/886 [16:16<1:13:57,  6.11s/it] 18%|█▊        | 161/886 [16:23<1:13:51,  6.11s/it] 18%|█▊        | 162/886 [16:29<1:13:45,  6.11s/it] 18%|█▊        | 163/886 [16:35<1:13:39,  6.11s/it] 19%|█▊        | 164/886 [16:41<1:13:33,  6.11s/it] 19%|█▊        | 165/886 [16:47<1:13:27,  6.11s/it] 19%|█▊        | 166/886 [16:53<1:13:22,  6.11s/it] 19%|█▉        | 167/886 [16:59<1:13:16,  6.11s/it] 19%|█▉        | 168/886 [17:05<1:13:10,  6.11s/it] 19%|█▉        | 169/886 [17:11<1:13:03,  6.11s/it] 19%|█▉        | 170/886 [17:18<1:12:57,  6.11s/it] 19%|█▉        | 171/886 [17:24<1:12:51,  6.11s/it] 19%|█▉        | 172/886 [17:30<1:12:45,  6.11s/it] 20%|█▉        | 173/886 [17:36<1:12:38,  6.11s/it] 20%|█▉        | 174/886 [17:42<1:12:32,  6.11s/it] 20%|█▉        | 175/886 [17:48<1:12:26,  6.11s/it] 20%|█▉        | 176/886 [17:54<1:12:20,  6.11s/it] 20%|█▉        | 177/886 [18:00<1:12:14,  6.11s/it] 20%|██        | 178/886 [18:06<1:12:09,  6.12s/it] 20%|██        | 179/886 [18:13<1:12:03,  6.12s/it] 20%|██        | 180/886 [18:19<1:11:56,  6.11s/it] 20%|██        | 181/886 [18:25<1:11:50,  6.11s/it] 21%|██        | 182/886 [18:31<1:11:45,  6.12s/it] 21%|██        | 183/886 [18:37<1:11:38,  6.11s/it] 21%|██        | 184/886 [18:43<1:11:32,  6.12s/it] 21%|██        | 185/886 [18:49<1:11:26,  6.12s/it] 21%|██        | 186/886 [18:55<1:11:20,  6.11s/it] 21%|██        | 187/886 [19:02<1:11:13,  6.11s/it] 21%|██        | 188/886 [19:08<1:11:07,  6.11s/it] 21%|██▏       | 189/886 [19:14<1:11:01,  6.11s/it] 21%|██▏       | 190/886 [19:20<1:10:55,  6.11s/it] 22%|██▏       | 191/886 [19:26<1:10:49,  6.12s/it] 22%|██▏       | 192/886 [19:32<1:10:43,  6.11s/it] 22%|██▏       | 193/886 [19:38<1:10:36,  6.11s/it] 22%|██▏       | 194/886 [19:44<1:10:31,  6.11s/it] 22%|██▏       | 195/886 [19:50<1:10:26,  6.12s/it] 22%|██▏       | 196/886 [19:57<1:10:20,  6.12s/it] 22%|██▏       | 197/886 [20:03<1:10:13,  6.12s/it] 22%|██▏       | 198/886 [20:09<1:10:08,  6.12s/it] 22%|██▏       | 199/886 [20:15<1:10:02,  6.12s/it] 23%|██▎       | 200/886 [20:21<1:09:56,  6.12s/it] 23%|██▎       | 201/886 [20:27<1:09:49,  6.12s/it] 23%|██▎       | 202/886 [20:33<1:09:42,  6.11s/it] 23%|██▎       | 203/886 [20:39<1:09:36,  6.11s/it] 23%|██▎       | 204/886 [20:45<1:09:29,  6.11s/it] 23%|██▎       | 205/886 [20:52<1:09:23,  6.11s/it] 23%|██▎       | 206/886 [20:58<1:09:17,  6.11s/it] 23%|██▎       | 207/886 [21:04<1:09:11,  6.11s/it] 23%|██▎       | 208/886 [21:10<1:09:06,  6.12s/it] 24%|██▎       | 209/886 [21:16<1:09:00,  6.12s/it] 24%|██▎       | 210/886 [21:22<1:08:53,  6.11s/it] 24%|██▍       | 211/886 [21:28<1:08:48,  6.12s/it] 24%|██▍       | 212/886 [21:34<1:08:41,  6.11s/it] 24%|██▍       | 213/886 [21:41<1:08:35,  6.11s/it] 24%|██▍       | 214/886 [21:47<1:08:30,  6.12s/it] 24%|██▍       | 215/886 [21:53<1:08:23,  6.12s/it] 24%|██▍       | 216/886 [21:59<1:08:16,  6.11s/it] 24%|██▍       | 217/886 [22:05<1:08:10,  6.11s/it] 25%|██▍       | 218/886 [22:11<1:08:04,  6.11s/it] 25%|██▍       | 219/886 [22:17<1:07:58,  6.11s/it] 25%|██▍       | 220/886 [22:23<1:07:52,  6.11s/it] 25%|██▍       | 221/886 [22:29<1:07:46,  6.12s/it] 25%|██▌       | 222/886 [22:36<1:07:40,  6.11s/it] 25%|██▌       | 223/886 [22:42<1:07:33,  6.11s/it] 25%|██▌       | 224/886 [22:48<1:07:27,  6.11s/it] 25%|██▌       | 225/886 [22:54<1:07:21,  6.11s/it] 26%|██▌       | 226/886 [23:00<1:07:15,  6.11s/it] 26%|██▌       | 227/886 [23:06<1:07:08,  6.11s/it] 26%|██▌       | 228/886 [23:12<1:07:03,  6.11s/it] 26%|██▌       | 229/886 [23:18<1:06:56,  6.11s/it] 26%|██▌       | 230/886 [23:24<1:06:50,  6.11s/it] 26%|██▌       | 231/886 [23:31<1:06:44,  6.11s/it] 26%|██▌       | 232/886 [23:37<1:06:38,  6.11s/it] 26%|██▋       | 233/886 [23:43<1:06:32,  6.11s/it] 26%|██▋       | 234/886 [23:49<1:06:26,  6.11s/it] 27%|██▋       | 235/886 [23:55<1:06:20,  6.11s/it] 27%|██▋       | 236/886 [24:01<1:06:13,  6.11s/it] 27%|██▋       | 237/886 [24:07<1:06:08,  6.11s/it] 27%|██▋       | 238/886 [24:13<1:06:02,  6.11s/it] 27%|██▋       | 239/886 [24:19<1:05:56,  6.11s/it] 27%|██▋       | 240/886 [24:26<1:05:50,  6.11s/it] 27%|██▋       | 241/886 [24:32<1:05:43,  6.11s/it] 27%|██▋       | 242/886 [24:38<1:05:37,  6.11s/it] 27%|██▋       | 243/886 [24:44<1:05:31,  6.11s/it] 28%|██▊       | 244/886 [24:50<1:05:24,  6.11s/it] 28%|██▊       | 245/886 [24:56<1:05:18,  6.11s/it] 28%|██▊       | 246/886 [25:02<1:05:13,  6.11s/it] 28%|██▊       | 247/886 [25:08<1:05:07,  6.11s/it] 28%|██▊       | 248/886 [25:15<1:05:02,  6.12s/it] 28%|██▊       | 249/886 [25:21<1:04:54,  6.11s/it] 28%|██▊       | 250/886 [25:27<1:04:48,  6.11s/it] 28%|██▊       | 251/886 [25:33<1:04:41,  6.11s/it] 28%|██▊       | 252/886 [25:39<1:04:35,  6.11s/it] 29%|██▊       | 253/886 [25:45<1:04:29,  6.11s/it] 29%|██▊       | 254/886 [25:51<1:04:24,  6.11s/it] 29%|██▉       | 255/886 [25:57<1:04:18,  6.11s/it] 29%|██▉       | 256/886 [26:03<1:04:11,  6.11s/it] 29%|██▉       | 257/886 [26:10<1:04:05,  6.11s/it] 29%|██▉       | 258/886 [26:16<1:03:59,  6.11s/it] 29%|██▉       | 259/886 [26:22<1:03:53,  6.11s/it] 29%|██▉       | 260/886 [26:28<1:03:47,  6.11s/it] 29%|██▉       | 261/886 [26:34<1:03:40,  6.11s/it] 30%|██▉       | 262/886 [26:40<1:03:34,  6.11s/it] 30%|██▉       | 263/886 [26:46<1:03:28,  6.11s/it] 30%|██▉       | 264/886 [26:52<1:03:22,  6.11s/it] 30%|██▉       | 265/886 [26:58<1:03:15,  6.11s/it] 30%|███       | 266/886 [27:05<1:03:10,  6.11s/it] 30%|███       | 267/886 [27:11<1:03:04,  6.11s/it] 30%|███       | 268/886 [27:17<1:02:59,  6.11s/it] 30%|███       | 269/886 [27:23<1:02:52,  6.12s/it] 30%|███       | 270/886 [27:29<1:02:47,  6.12s/it] 31%|███       | 271/886 [27:35<1:02:41,  6.12s/it] 31%|███       | 272/886 [27:41<1:02:34,  6.12s/it] 31%|███       | 273/886 [27:47<1:02:28,  6.11s/it] 31%|███       | 274/886 [27:53<1:02:21,  6.11s/it] 31%|███       | 275/886 [28:00<1:02:15,  6.11s/it] 31%|███       | 276/886 [28:06<1:02:09,  6.11s/it] 31%|███▏      | 277/886 [28:12<1:02:03,  6.11s/it] 31%|███▏      | 278/886 [28:18<1:01:57,  6.11s/it] 31%|███▏      | 279/886 [28:24<1:01:50,  6.11s/it] 32%|███▏      | 280/886 [28:30<1:01:44,  6.11s/it] 32%|███▏      | 281/886 [28:36<1:01:38,  6.11s/it] 32%|███▏      | 282/886 [28:42<1:01:31,  6.11s/it] 32%|███▏      | 283/886 [28:48<1:01:25,  6.11s/it] 32%|███▏      | 284/886 [28:55<1:01:19,  6.11s/it] 32%|███▏      | 285/886 [29:01<1:01:13,  6.11s/it] 32%|███▏      | 286/886 [29:07<1:01:08,  6.11s/it] 32%|███▏      | 287/886 [29:13<1:01:01,  6.11s/it] 33%|███▎      | 288/886 [29:19<1:00:55,  6.11s/it] 33%|███▎      | 289/886 [29:25<1:00:49,  6.11s/it] 33%|███▎      | 290/886 [29:31<1:00:43,  6.11s/it] 33%|███▎      | 291/886 [29:37<1:00:37,  6.11s/it] 33%|███▎      | 292/886 [29:44<1:00:31,  6.11s/it] 33%|███▎      | 293/886 [29:50<1:00:25,  6.11s/it] 33%|███▎      | 294/886 [29:56<1:00:18,  6.11s/it] 33%|███▎      | 295/886 [30:02<1:00:12,  6.11s/it] 33%|███▎      | 296/886 [30:08<1:00:06,  6.11s/it] 34%|███▎      | 297/886 [30:14<1:00:00,  6.11s/it] 34%|███▎      | 298/886 [30:20<59:54,  6.11s/it]   34%|███▎      | 299/886 [30:26<59:48,  6.11s/it] 34%|███▍      | 300/886 [30:32<59:41,  6.11s/it] 34%|███▍      | 301/886 [30:39<59:35,  6.11s/it] 34%|███▍      | 302/886 [30:45<59:30,  6.11s/it] 34%|███▍      | 303/886 [30:51<59:24,  6.11s/it] 34%|███▍      | 304/886 [30:57<59:18,  6.11s/it] 34%|███▍      | 305/886 [31:03<59:12,  6.11s/it] 35%|███▍      | 306/886 [31:09<59:05,  6.11s/it] 35%|███▍      | 307/886 [31:15<58:59,  6.11s/it] 35%|███▍      | 308/886 [31:21<58:56,  6.12s/it] 35%|███▍      | 309/886 [31:27<58:49,  6.12s/it] 35%|███▍      | 310/886 [31:34<58:42,  6.12s/it] 35%|███▌      | 311/886 [31:40<58:36,  6.12s/it] 35%|███▌      | 312/886 [31:46<58:29,  6.11s/it] 35%|███▌      | 313/886 [31:52<58:23,  6.11s/it] 35%|███▌      | 314/886 [31:58<58:17,  6.11s/it] 36%|███▌      | 315/886 [32:04<58:11,  6.11s/it] 36%|███▌      | 316/886 [32:10<58:05,  6.11s/it] 36%|███▌      | 317/886 [32:16<57:58,  6.11s/it] 36%|███▌      | 318/886 [32:22<57:56,  6.12s/it] 36%|███▌      | 319/886 [32:29<57:49,  6.12s/it] 36%|███▌      | 320/886 [32:35<57:42,  6.12s/it] 36%|███▌      | 321/886 [32:41<57:36,  6.12s/it] 36%|███▋      | 322/886 [32:47<57:29,  6.12s/it] 36%|███▋      | 323/886 [32:53<57:23,  6.12s/it] 37%|███▋      | 324/886 [32:59<57:16,  6.12s/it] 37%|███▋      | 325/886 [33:05<57:10,  6.12s/it] 37%|███▋      | 326/886 [33:11<57:04,  6.11s/it] 37%|███▋      | 327/886 [33:18<56:57,  6.11s/it] 37%|███▋      | 328/886 [33:24<56:51,  6.11s/it] 37%|███▋      | 329/886 [33:30<56:45,  6.11s/it] 37%|███▋      | 330/886 [33:36<56:39,  6.12s/it] 37%|███▋      | 331/886 [33:42<56:33,  6.12s/it] 37%|███▋      | 332/886 [33:48<56:27,  6.12s/it] 38%|███▊      | 333/886 [33:54<56:21,  6.12s/it] 38%|███▊      | 334/886 [34:00<56:15,  6.11s/it] 38%|███▊      | 335/886 [34:06<56:09,  6.11s/it] 38%|███▊      | 336/886 [34:13<56:02,  6.11s/it] 38%|███▊      | 337/886 [34:19<55:56,  6.11s/it] 38%|███▊      | 338/886 [34:25<55:50,  6.11s/it] 38%|███▊      | 339/886 [34:31<55:44,  6.11s/it] 38%|███▊      | 340/886 [34:37<55:38,  6.11s/it] 38%|███▊      | 341/886 [34:43<55:31,  6.11s/it] 39%|███▊      | 342/886 [34:49<55:26,  6.11s/it] 39%|███▊      | 343/886 [34:55<55:19,  6.11s/it] 39%|███▉      | 344/886 [35:01<55:13,  6.11s/it] 39%|███▉      | 345/886 [35:08<55:07,  6.11s/it] 39%|███▉      | 346/886 [35:14<55:01,  6.11s/it] 39%|███▉      | 347/886 [35:20<54:55,  6.11s/it] 39%|███▉      | 348/886 [35:26<54:49,  6.11s/it] 39%|███▉      | 349/886 [35:32<54:43,  6.12s/it] 40%|███▉      | 350/886 [35:38<54:36,  6.11s/it] 40%|███▉      | 351/886 [35:44<54:30,  6.11s/it] 40%|███▉      | 352/886 [35:50<54:24,  6.11s/it] 40%|███▉      | 353/886 [35:56<54:18,  6.11s/it] 40%|███▉      | 354/886 [36:03<54:12,  6.11s/it] 40%|████      | 355/886 [36:09<54:05,  6.11s/it] 40%|████      | 356/886 [36:15<53:59,  6.11s/it] 40%|████      | 357/886 [36:21<53:54,  6.11s/it] 40%|████      | 358/886 [36:27<53:47,  6.11s/it] 41%|████      | 359/886 [36:33<53:42,  6.11s/it] 41%|████      | 360/886 [36:39<53:36,  6.11s/it] 41%|████      | 361/886 [36:45<53:30,  6.12s/it] 41%|████      | 362/886 [36:52<53:24,  6.12s/it] 41%|████      | 363/886 [36:58<53:18,  6.12s/it] 41%|████      | 364/886 [37:04<53:12,  6.12s/it] 41%|████      | 365/886 [37:10<53:06,  6.12s/it] 41%|████▏     | 366/886 [37:16<53:00,  6.12s/it] 41%|████▏     | 367/886 [37:22<52:53,  6.11s/it] 42%|████▏     | 368/886 [37:28<52:47,  6.12s/it] 42%|████▏     | 369/886 [37:34<52:41,  6.12s/it] 42%|████▏     | 370/886 [37:40<52:35,  6.12s/it] 42%|████▏     | 371/886 [37:47<52:29,  6.11s/it] 42%|████▏     | 372/886 [37:53<52:23,  6.12s/it] 42%|████▏     | 373/886 [37:59<52:17,  6.12s/it] 42%|████▏     | 374/886 [38:05<52:11,  6.12s/it] 42%|████▏     | 375/886 [38:11<52:05,  6.12s/it] 42%|████▏     | 376/886 [38:17<51:59,  6.12s/it] 43%|████▎     | 377/886 [38:23<51:52,  6.12s/it] 43%|████▎     | 378/886 [38:29<51:46,  6.12s/it] 43%|████▎     | 379/886 [38:35<51:40,  6.11s/it] 43%|████▎     | 380/886 [38:42<51:34,  6.12s/it] 43%|████▎     | 381/886 [38:48<51:29,  6.12s/it] 43%|████▎     | 382/886 [38:54<51:22,  6.12s/it] 43%|████▎     | 383/886 [39:00<51:16,  6.12s/it] 43%|████▎     | 384/886 [39:06<51:09,  6.11s/it] 43%|████▎     | 385/886 [39:12<51:03,  6.11s/it] 44%|████▎     | 386/886 [39:18<50:57,  6.11s/it] 44%|████▎     | 387/886 [39:24<50:50,  6.11s/it] 44%|████▍     | 388/886 [39:31<50:45,  6.12s/it] 44%|████▍     | 389/886 [39:37<50:39,  6.12s/it] 44%|████▍     | 390/886 [39:43<50:32,  6.11s/it] 44%|████▍     | 391/886 [39:49<50:26,  6.11s/it] 44%|████▍     | 392/886 [39:55<50:20,  6.11s/it] 44%|████▍     | 393/886 [40:01<50:14,  6.11s/it] 44%|████▍     | 394/886 [40:07<50:08,  6.11s/it] 45%|████▍     | 395/886 [40:13<50:02,  6.11s/it] 45%|████▍     | 396/886 [40:19<49:56,  6.12s/it] 45%|████▍     | 397/886 [40:26<49:50,  6.12s/it] 45%|████▍     | 398/886 [40:32<49:45,  6.12s/it] 45%|████▌     | 399/886 [40:38<49:38,  6.12s/it] 45%|████▌     | 400/886 [40:44<49:31,  6.12s/it] 45%|████▌     | 401/886 [40:50<49:25,  6.11s/it] 45%|████▌     | 402/886 [40:56<49:19,  6.11s/it] 45%|████▌     | 403/886 [41:02<49:13,  6.11s/it] 46%|████▌     | 404/886 [41:08<49:07,  6.11s/it] 46%|████▌     | 405/886 [41:14<49:00,  6.11s/it] 46%|████▌     | 406/886 [41:21<48:54,  6.11s/it] 46%|████▌     | 407/886 [41:27<48:49,  6.12s/it] 46%|████▌     | 408/886 [41:33<48:43,  6.12s/it] 46%|████▌     | 409/886 [41:39<48:37,  6.12s/it] 46%|████▋     | 410/886 [41:45<48:30,  6.12s/it] 46%|████▋     | 411/886 [41:51<48:24,  6.12s/it] 47%|████▋     | 412/886 [41:57<48:18,  6.12s/it] 47%|████▋     | 413/886 [42:03<48:12,  6.12s/it] 47%|████▋     | 414/886 [42:10<48:06,  6.11s/it] 47%|████▋     | 415/886 [42:16<47:59,  6.11s/it] 47%|████▋     | 416/886 [42:22<47:54,  6.12s/it] 47%|████▋     | 417/886 [42:28<47:48,  6.12s/it] 47%|████▋     | 418/886 [42:34<47:41,  6.11s/it] 47%|████▋     | 419/886 [42:40<47:35,  6.11s/it] 47%|████▋     | 420/886 [42:46<47:29,  6.11s/it] 48%|████▊     | 421/886 [42:52<47:23,  6.11s/it] 48%|████▊     | 422/886 [42:58<47:17,  6.12s/it] 48%|████▊     | 423/886 [43:05<47:11,  6.12s/it] 48%|████▊     | 424/886 [43:11<47:05,  6.12s/it] 48%|████▊     | 425/886 [43:17<47:00,  6.12s/it] 48%|████▊     | 426/886 [43:23<46:53,  6.12s/it] 48%|████▊     | 427/886 [43:29<46:46,  6.12s/it] 48%|████▊     | 428/886 [43:35<46:40,  6.12s/it] 48%|████▊     | 429/886 [43:41<46:35,  6.12s/it] 49%|████▊     | 430/886 [43:47<46:29,  6.12s/it] 49%|████▊     | 431/886 [43:53<46:22,  6.12s/it] 49%|████▉     | 432/886 [44:00<46:16,  6.12s/it] 49%|████▉     | 433/886 [44:06<46:10,  6.12s/it] 49%|████▉     | 434/886 [44:12<46:04,  6.12s/it] 49%|████▉     | 435/886 [44:18<45:59,  6.12s/it] 49%|████▉     | 436/886 [44:24<45:52,  6.12s/it] 49%|████▉     | 437/886 [44:30<45:46,  6.12s/it] 49%|████▉     | 438/886 [44:36<45:40,  6.12s/it] 50%|████▉     | 439/886 [44:42<45:34,  6.12s/it] 50%|████▉     | 440/886 [44:49<45:27,  6.12s/it] 50%|████▉     | 441/886 [44:55<45:21,  6.12s/it] 50%|████▉     | 442/886 [45:01<45:15,  6.12s/it] 50%|█████     | 443/886 [45:07<44:52,  6.08s/it][INFO|trainer.py:2340] 2023-10-30 17:20:44,034 >> Saving model checkpoint to chkpts/analysis_models/d-atomic-roberta-base/checkpoint-443
[INFO|configuration_utils.py:446] 2023-10-30 17:20:44,043 >> Configuration saved in chkpts/analysis_models/d-atomic-roberta-base/checkpoint-443/config.json
[INFO|modeling_utils.py:1542] 2023-10-30 17:20:50,341 >> Model weights saved in chkpts/analysis_models/d-atomic-roberta-base/checkpoint-443/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-10-30 17:20:50,348 >> tokenizer config file saved in chkpts/analysis_models/d-atomic-roberta-base/checkpoint-443/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-10-30 17:20:50,356 >> Special tokens file saved in chkpts/analysis_models/d-atomic-roberta-base/checkpoint-443/special_tokens_map.json
 50%|█████     | 444/886 [45:31<1:24:11, 11.43s/it] 50%|█████     | 445/886 [45:37<1:12:16,  9.83s/it] 50%|█████     | 446/886 [45:43<1:03:55,  8.72s/it] 50%|█████     | 447/886 [45:49<58:04,  7.94s/it]   51%|█████     | 448/886 [45:55<53:56,  7.39s/it] 51%|█████     | 449/886 [46:01<51:02,  7.01s/it] 51%|█████     | 450/886 [46:07<48:58,  6.74s/it] 51%|█████     | 451/886 [46:13<47:30,  6.55s/it] 51%|█████     | 452/886 [46:20<46:26,  6.42s/it] 51%|█████     | 453/886 [46:26<45:40,  6.33s/it] 51%|█████     | 454/886 [46:32<45:06,  6.27s/it] 51%|█████▏    | 455/886 [46:38<44:41,  6.22s/it] 51%|█████▏    | 456/886 [46:44<44:21,  6.19s/it] 52%|█████▏    | 457/886 [46:50<44:05,  6.17s/it] 52%|█████▏    | 458/886 [46:56<43:53,  6.15s/it] 52%|█████▏    | 459/886 [47:02<43:42,  6.14s/it] 52%|█████▏    | 460/886 [47:09<43:32,  6.13s/it] 52%|█████▏    | 461/886 [47:15<43:24,  6.13s/it] 52%|█████▏    | 462/886 [47:21<43:16,  6.12s/it] 52%|█████▏    | 463/886 [47:27<43:09,  6.12s/it] 52%|█████▏    | 464/886 [47:33<43:02,  6.12s/it] 52%|█████▏    | 465/886 [47:39<42:56,  6.12s/it] 53%|█████▎    | 466/886 [47:45<42:49,  6.12s/it] 53%|█████▎    | 467/886 [47:51<42:43,  6.12s/it] 53%|█████▎    | 468/886 [47:57<42:37,  6.12s/it] 53%|█████▎    | 469/886 [48:04<42:30,  6.12s/it] 53%|█████▎    | 470/886 [48:10<42:24,  6.12s/it] 53%|█████▎    | 471/886 [48:16<42:18,  6.12s/it] 53%|█████▎    | 472/886 [48:22<42:11,  6.12s/it] 53%|█████▎    | 473/886 [48:28<42:05,  6.12s/it] 53%|█████▎    | 474/886 [48:34<41:59,  6.12s/it] 54%|█████▎    | 475/886 [48:40<41:53,  6.11s/it] 54%|█████▎    | 476/886 [48:46<41:46,  6.11s/it] 54%|█████▍    | 477/886 [48:52<41:41,  6.12s/it] 54%|█████▍    | 478/886 [48:59<41:34,  6.11s/it] 54%|█████▍    | 479/886 [49:05<41:28,  6.11s/it] 54%|█████▍    | 480/886 [49:11<41:22,  6.12s/it] 54%|█████▍    | 481/886 [49:17<41:17,  6.12s/it] 54%|█████▍    | 482/886 [49:23<41:10,  6.12s/it] 55%|█████▍    | 483/886 [49:29<41:04,  6.12s/it] 55%|█████▍    | 484/886 [49:35<40:58,  6.12s/it] 55%|█████▍    | 485/886 [49:41<40:52,  6.12s/it] 55%|█████▍    | 486/886 [49:48<40:46,  6.12s/it] 55%|█████▍    | 487/886 [49:54<40:40,  6.12s/it] 55%|█████▌    | 488/886 [50:00<40:34,  6.12s/it] 55%|█████▌    | 489/886 [50:06<40:28,  6.12s/it] 55%|█████▌    | 490/886 [50:12<40:22,  6.12s/it] 55%|█████▌    | 491/886 [50:18<40:15,  6.12s/it] 56%|█████▌    | 492/886 [50:24<40:09,  6.12s/it] 56%|█████▌    | 493/886 [50:30<40:03,  6.12s/it] 56%|█████▌    | 494/886 [50:36<39:57,  6.12s/it] 56%|█████▌    | 495/886 [50:43<39:52,  6.12s/it] 56%|█████▌    | 496/886 [50:49<39:45,  6.12s/it] 56%|█████▌    | 497/886 [50:55<39:39,  6.12s/it] 56%|█████▌    | 498/886 [51:01<39:33,  6.12s/it] 56%|█████▋    | 499/886 [51:07<39:26,  6.12s/it] 56%|█████▋    | 500/886 [51:13<39:20,  6.12s/it]                                                  56%|█████▋    | 500/886 [51:13<39:20,  6.12s/it] 57%|█████▋    | 501/886 [51:19<39:19,  6.13s/it] 57%|█████▋    | 502/886 [51:25<39:12,  6.13s/it] 57%|█████▋    | 503/886 [51:32<39:04,  6.12s/it] 57%|█████▋    | 504/886 [51:38<38:57,  6.12s/it] 57%|█████▋    | 505/886 [51:44<38:51,  6.12s/it] 57%|█████▋    | 506/886 [51:50<38:44,  6.12s/it] 57%|█████▋    | 507/886 [51:56<38:38,  6.12s/it] 57%|█████▋    | 508/886 [52:02<38:32,  6.12s/it] 57%|█████▋    | 509/886 [52:08<38:26,  6.12s/it] 58%|█████▊    | 510/886 [52:14<38:19,  6.12s/it] 58%|█████▊    | 511/886 [52:20<38:13,  6.12s/it] 58%|█████▊    | 512/886 [52:27<38:07,  6.12s/it] 58%|█████▊    | 513/886 [52:33<38:01,  6.12s/it] 58%|█████▊    | 514/886 [52:39<37:55,  6.12s/it] 58%|█████▊    | 515/886 [52:45<37:51,  6.12s/it] 58%|█████▊    | 516/886 [52:51<37:44,  6.12s/it] 58%|█████▊    | 517/886 [52:57<37:37,  6.12s/it] 58%|█████▊    | 518/886 [53:03<37:30,  6.12s/it] 59%|█████▊    | 519/886 [53:09<37:24,  6.12s/it] 59%|█████▊    | 520/886 [53:16<37:18,  6.12s/it] 59%|█████▉    | 521/886 [53:22<37:12,  6.12s/it] 59%|█████▉    | 522/886 [53:28<37:06,  6.12s/it] 59%|█████▉    | 523/886 [53:34<37:00,  6.12s/it] 59%|█████▉    | 524/886 [53:40<36:54,  6.12s/it] 59%|█████▉    | 525/886 [53:46<36:48,  6.12s/it] 59%|█████▉    | 526/886 [53:52<36:42,  6.12s/it] 59%|█████▉    | 527/886 [53:58<36:35,  6.12s/it] 60%|█████▉    | 528/886 [54:04<36:29,  6.12s/it] 60%|█████▉    | 529/886 [54:11<36:23,  6.12s/it] 60%|█████▉    | 530/886 [54:17<36:17,  6.12s/it] 60%|█████▉    | 531/886 [54:23<36:11,  6.12s/it] 60%|██████    | 532/886 [54:29<36:04,  6.11s/it] 60%|██████    | 533/886 [54:35<35:58,  6.11s/it] 60%|██████    | 534/886 [54:41<35:52,  6.11s/it] 60%|██████    | 535/886 [54:47<35:46,  6.12s/it] 60%|██████    | 536/886 [54:53<35:40,  6.12s/it] 61%|██████    | 537/886 [54:59<35:34,  6.12s/it] 61%|██████    | 538/886 [55:06<35:28,  6.12s/it] 61%|██████    | 539/886 [55:12<35:22,  6.12s/it] 61%|██████    | 540/886 [55:18<35:16,  6.12s/it] 61%|██████    | 541/886 [55:24<35:10,  6.12s/it] 61%|██████    | 542/886 [55:30<35:03,  6.11s/it] 61%|██████▏   | 543/886 [55:36<34:58,  6.12s/it] 61%|██████▏   | 544/886 [55:42<34:52,  6.12s/it] 62%|██████▏   | 545/886 [55:48<34:45,  6.12s/it] 62%|██████▏   | 546/886 [55:55<34:39,  6.12s/it] 62%|██████▏   | 547/886 [56:01<34:33,  6.12s/it] 62%|██████▏   | 548/886 [56:07<34:27,  6.12s/it] 62%|██████▏   | 549/886 [56:13<34:21,  6.12s/it] 62%|██████▏   | 550/886 [56:19<34:14,  6.12s/it] 62%|██████▏   | 551/886 [56:25<34:08,  6.12s/it] 62%|██████▏   | 552/886 [56:31<34:02,  6.11s/it] 62%|██████▏   | 553/886 [56:37<33:55,  6.11s/it] 63%|██████▎   | 554/886 [56:43<33:50,  6.11s/it] 63%|██████▎   | 555/886 [56:50<33:43,  6.11s/it] 63%|██████▎   | 556/886 [56:56<33:37,  6.11s/it] 63%|██████▎   | 557/886 [57:02<33:32,  6.12s/it] 63%|██████▎   | 558/886 [57:08<33:25,  6.12s/it] 63%|██████▎   | 559/886 [57:14<33:19,  6.11s/it] 63%|██████▎   | 560/886 [57:20<33:13,  6.11s/it] 63%|██████▎   | 561/886 [57:26<33:07,  6.11s/it] 63%|██████▎   | 562/886 [57:32<33:00,  6.11s/it] 64%|██████▎   | 563/886 [57:38<32:55,  6.11s/it] 64%|██████▎   | 564/886 [57:45<32:48,  6.11s/it] 64%|██████▍   | 565/886 [57:51<32:42,  6.11s/it] 64%|██████▍   | 566/886 [57:57<32:37,  6.12s/it] 64%|██████▍   | 567/886 [58:03<32:30,  6.12s/it] 64%|██████▍   | 568/886 [58:09<32:24,  6.11s/it] 64%|██████▍   | 569/886 [58:15<32:18,  6.11s/it] 64%|██████▍   | 570/886 [58:21<32:12,  6.11s/it] 64%|██████▍   | 571/886 [58:27<32:06,  6.11s/it] 65%|██████▍   | 572/886 [58:34<31:59,  6.11s/it] 65%|██████▍   | 573/886 [58:40<31:53,  6.11s/it] 65%|██████▍   | 574/886 [58:46<31:47,  6.11s/it] 65%|██████▍   | 575/886 [58:52<31:41,  6.12s/it] 65%|██████▌   | 576/886 [58:58<31:35,  6.12s/it] 65%|██████▌   | 577/886 [59:04<31:29,  6.12s/it] 65%|██████▌   | 578/886 [59:10<31:23,  6.12s/it] 65%|██████▌   | 579/886 [59:16<31:17,  6.11s/it] 65%|██████▌   | 580/886 [59:22<31:11,  6.11s/it] 66%|██████▌   | 581/886 [59:29<31:04,  6.11s/it] 66%|██████▌   | 582/886 [59:35<30:58,  6.11s/it] 66%|██████▌   | 583/886 [59:41<30:52,  6.12s/it] 66%|██████▌   | 584/886 [59:47<30:46,  6.12s/it] 66%|██████▌   | 585/886 [59:53<30:40,  6.12s/it] 66%|██████▌   | 586/886 [59:59<30:34,  6.11s/it] 66%|██████▋   | 587/886 [1:00:05<30:28,  6.11s/it] 66%|██████▋   | 588/886 [1:00:11<30:22,  6.12s/it] 66%|██████▋   | 589/886 [1:00:17<30:16,  6.11s/it] 67%|██████▋   | 590/886 [1:00:24<30:09,  6.11s/it] 67%|██████▋   | 591/886 [1:00:30<30:03,  6.11s/it] 67%|██████▋   | 592/886 [1:00:36<29:57,  6.11s/it] 67%|██████▋   | 593/886 [1:00:42<29:51,  6.11s/it] 67%|██████▋   | 594/886 [1:00:48<29:44,  6.11s/it] 67%|██████▋   | 595/886 [1:00:54<29:39,  6.11s/it] 67%|██████▋   | 596/886 [1:01:00<29:33,  6.11s/it] 67%|██████▋   | 597/886 [1:01:06<29:27,  6.11s/it] 67%|██████▋   | 598/886 [1:01:13<29:21,  6.12s/it] 68%|██████▊   | 599/886 [1:01:19<29:15,  6.12s/it] 68%|██████▊   | 600/886 [1:01:25<29:08,  6.11s/it] 68%|██████▊   | 601/886 [1:01:31<29:02,  6.11s/it] 68%|██████▊   | 602/886 [1:01:37<28:56,  6.11s/it] 68%|██████▊   | 603/886 [1:01:43<28:50,  6.12s/it] 68%|██████▊   | 604/886 [1:01:49<28:44,  6.11s/it] 68%|██████▊   | 605/886 [1:01:55<28:38,  6.12s/it] 68%|██████▊   | 606/886 [1:02:01<28:32,  6.12s/it] 69%|██████▊   | 607/886 [1:02:08<28:26,  6.12s/it] 69%|██████▊   | 608/886 [1:02:14<28:19,  6.11s/it] 69%|██████▊   | 609/886 [1:02:20<28:13,  6.12s/it] 69%|██████▉   | 610/886 [1:02:26<28:07,  6.11s/it] 69%|██████▉   | 611/886 [1:02:32<28:01,  6.11s/it] 69%|██████▉   | 612/886 [1:02:38<27:55,  6.11s/it] 69%|██████▉   | 613/886 [1:02:44<27:49,  6.11s/it] 69%|██████▉   | 614/886 [1:02:50<27:42,  6.11s/it] 69%|██████▉   | 615/886 [1:02:56<27:37,  6.11s/it] 70%|██████▉   | 616/886 [1:03:03<27:31,  6.11s/it] 70%|██████▉   | 617/886 [1:03:09<27:25,  6.12s/it] 70%|██████▉   | 618/886 [1:03:15<27:18,  6.12s/it] 70%|██████▉   | 619/886 [1:03:21<27:12,  6.12s/it] 70%|██████▉   | 620/886 [1:03:27<27:06,  6.12s/it] 70%|███████   | 621/886 [1:03:33<27:00,  6.12s/it] 70%|███████   | 622/886 [1:03:39<26:54,  6.12s/it] 70%|███████   | 623/886 [1:03:45<26:48,  6.11s/it] 70%|███████   | 624/886 [1:03:51<26:41,  6.11s/it] 71%|███████   | 625/886 [1:03:58<26:36,  6.11s/it] 71%|███████   | 626/886 [1:04:04<26:30,  6.12s/it] 71%|███████   | 627/886 [1:04:10<26:24,  6.12s/it] 71%|███████   | 628/886 [1:04:16<26:18,  6.12s/it] 71%|███████   | 629/886 [1:04:22<26:11,  6.12s/it] 71%|███████   | 630/886 [1:04:28<26:05,  6.12s/it] 71%|███████   | 631/886 [1:04:34<25:59,  6.12s/it] 71%|███████▏  | 632/886 [1:04:40<25:53,  6.12s/it] 71%|███████▏  | 633/886 [1:04:47<25:47,  6.11s/it] 72%|███████▏  | 634/886 [1:04:53<25:41,  6.12s/it] 72%|███████▏  | 635/886 [1:04:59<25:38,  6.13s/it] 72%|███████▏  | 636/886 [1:05:05<25:31,  6.13s/it] 72%|███████▏  | 637/886 [1:05:11<25:24,  6.12s/it] 72%|███████▏  | 638/886 [1:05:17<25:17,  6.12s/it] 72%|███████▏  | 639/886 [1:05:23<25:11,  6.12s/it] 72%|███████▏  | 640/886 [1:05:29<25:04,  6.12s/it] 72%|███████▏  | 641/886 [1:05:36<24:58,  6.12s/it] 72%|███████▏  | 642/886 [1:05:42<24:52,  6.12s/it] 73%|███████▎  | 643/886 [1:05:48<24:46,  6.12s/it] 73%|███████▎  | 644/886 [1:05:54<24:39,  6.12s/it] 73%|███████▎  | 645/886 [1:06:00<24:34,  6.12s/it] 73%|███████▎  | 646/886 [1:06:06<24:27,  6.12s/it] 73%|███████▎  | 647/886 [1:06:12<24:21,  6.12s/it] 73%|███████▎  | 648/886 [1:06:18<24:15,  6.11s/it] 73%|███████▎  | 649/886 [1:06:24<24:09,  6.11s/it] 73%|███████▎  | 650/886 [1:06:31<24:02,  6.11s/it] 73%|███████▎  | 651/886 [1:06:37<23:56,  6.11s/it] 74%|███████▎  | 652/886 [1:06:43<23:50,  6.11s/it] 74%|███████▎  | 653/886 [1:06:49<23:44,  6.11s/it] 74%|███████▍  | 654/886 [1:06:55<23:38,  6.11s/it] 74%|███████▍  | 655/886 [1:07:01<23:33,  6.12s/it] 74%|███████▍  | 656/886 [1:07:07<23:27,  6.12s/it] 74%|███████▍  | 657/886 [1:07:13<23:20,  6.12s/it] 74%|███████▍  | 658/886 [1:07:19<23:14,  6.12s/it] 74%|███████▍  | 659/886 [1:07:26<23:08,  6.11s/it] 74%|███████▍  | 660/886 [1:07:32<23:01,  6.11s/it] 75%|███████▍  | 661/886 [1:07:38<22:55,  6.11s/it] 75%|███████▍  | 662/886 [1:07:44<22:49,  6.11s/it] 75%|███████▍  | 663/886 [1:07:50<22:43,  6.11s/it] 75%|███████▍  | 664/886 [1:07:56<22:37,  6.11s/it] 75%|███████▌  | 665/886 [1:08:02<22:31,  6.11s/it] 75%|███████▌  | 666/886 [1:08:08<22:25,  6.12s/it] 75%|███████▌  | 667/886 [1:08:15<22:19,  6.12s/it] 75%|███████▌  | 668/886 [1:08:21<22:13,  6.12s/it] 76%|███████▌  | 669/886 [1:08:27<22:07,  6.12s/it] 76%|███████▌  | 670/886 [1:08:33<22:01,  6.12s/it] 76%|███████▌  | 671/886 [1:08:39<21:55,  6.12s/it] 76%|███████▌  | 672/886 [1:08:45<21:49,  6.12s/it] 76%|███████▌  | 673/886 [1:08:51<21:42,  6.12s/it] 76%|███████▌  | 674/886 [1:08:57<21:36,  6.12s/it] 76%|███████▌  | 675/886 [1:09:03<21:30,  6.12s/it] 76%|███████▋  | 676/886 [1:09:10<21:24,  6.12s/it] 76%|███████▋  | 677/886 [1:09:16<21:18,  6.12s/it] 77%|███████▋  | 678/886 [1:09:22<21:11,  6.11s/it] 77%|███████▋  | 679/886 [1:09:28<21:05,  6.11s/it] 77%|███████▋  | 680/886 [1:09:34<20:59,  6.11s/it] 77%|███████▋  | 681/886 [1:09:40<20:53,  6.11s/it] 77%|███████▋  | 682/886 [1:09:46<20:47,  6.11s/it] 77%|███████▋  | 683/886 [1:09:52<20:40,  6.11s/it] 77%|███████▋  | 684/886 [1:09:58<20:35,  6.11s/it] 77%|███████▋  | 685/886 [1:10:05<20:29,  6.12s/it] 77%|███████▋  | 686/886 [1:10:11<20:23,  6.12s/it] 78%|███████▊  | 687/886 [1:10:17<20:16,  6.12s/it] 78%|███████▊  | 688/886 [1:10:23<20:10,  6.12s/it] 78%|███████▊  | 689/886 [1:10:29<20:04,  6.12s/it] 78%|███████▊  | 690/886 [1:10:35<19:58,  6.12s/it] 78%|███████▊  | 691/886 [1:10:41<19:52,  6.12s/it] 78%|███████▊  | 692/886 [1:10:47<19:46,  6.11s/it] 78%|███████▊  | 693/886 [1:10:54<19:40,  6.11s/it] 78%|███████▊  | 694/886 [1:11:00<19:34,  6.11s/it] 78%|███████▊  | 695/886 [1:11:06<19:27,  6.11s/it] 79%|███████▊  | 696/886 [1:11:12<19:21,  6.12s/it] 79%|███████▊  | 697/886 [1:11:18<19:15,  6.12s/it] 79%|███████▉  | 698/886 [1:11:24<19:09,  6.12s/it] 79%|███████▉  | 699/886 [1:11:30<19:03,  6.12s/it] 79%|███████▉  | 700/886 [1:11:36<18:57,  6.12s/it] 79%|███████▉  | 701/886 [1:11:42<18:51,  6.12s/it] 79%|███████▉  | 702/886 [1:11:49<18:45,  6.12s/it] 79%|███████▉  | 703/886 [1:11:55<18:39,  6.12s/it] 79%|███████▉  | 704/886 [1:12:01<18:33,  6.12s/it] 80%|███████▉  | 705/886 [1:12:07<18:27,  6.12s/it] 80%|███████▉  | 706/886 [1:12:13<18:20,  6.12s/it] 80%|███████▉  | 707/886 [1:12:19<18:14,  6.12s/it] 80%|███████▉  | 708/886 [1:12:25<18:08,  6.12s/it] 80%|████████  | 709/886 [1:12:31<18:02,  6.12s/it] 80%|████████  | 710/886 [1:12:37<17:56,  6.11s/it] 80%|████████  | 711/886 [1:12:44<17:50,  6.11s/it] 80%|████████  | 712/886 [1:12:50<17:43,  6.11s/it] 80%|████████  | 713/886 [1:12:56<17:37,  6.11s/it] 81%|████████  | 714/886 [1:13:02<17:31,  6.11s/it] 81%|████████  | 715/886 [1:13:08<17:25,  6.12s/it] 81%|████████  | 716/886 [1:13:14<17:19,  6.11s/it] 81%|████████  | 717/886 [1:13:20<17:13,  6.12s/it] 81%|████████  | 718/886 [1:13:26<17:07,  6.12s/it] 81%|████████  | 719/886 [1:13:33<17:01,  6.12s/it] 81%|████████▏ | 720/886 [1:13:39<16:55,  6.12s/it] 81%|████████▏ | 721/886 [1:13:45<16:49,  6.12s/it] 81%|████████▏ | 722/886 [1:13:51<16:42,  6.12s/it] 82%|████████▏ | 723/886 [1:13:57<16:36,  6.11s/it] 82%|████████▏ | 724/886 [1:14:03<16:30,  6.11s/it] 82%|████████▏ | 725/886 [1:14:09<16:24,  6.11s/it] 82%|████████▏ | 726/886 [1:14:15<16:18,  6.12s/it] 82%|████████▏ | 727/886 [1:14:21<16:12,  6.11s/it] 82%|████████▏ | 728/886 [1:14:28<16:06,  6.12s/it] 82%|████████▏ | 729/886 [1:14:34<16:00,  6.12s/it] 82%|████████▏ | 730/886 [1:14:40<15:54,  6.12s/it] 83%|████████▎ | 731/886 [1:14:46<15:47,  6.12s/it] 83%|████████▎ | 732/886 [1:14:52<15:41,  6.11s/it] 83%|████████▎ | 733/886 [1:14:58<15:35,  6.12s/it] 83%|████████▎ | 734/886 [1:15:04<15:29,  6.12s/it] 83%|████████▎ | 735/886 [1:15:10<15:24,  6.12s/it] 83%|████████▎ | 736/886 [1:15:16<15:17,  6.12s/it] 83%|████████▎ | 737/886 [1:15:23<15:11,  6.12s/it] 83%|████████▎ | 738/886 [1:15:29<15:05,  6.12s/it] 83%|████████▎ | 739/886 [1:15:35<14:59,  6.12s/it] 84%|████████▎ | 740/886 [1:15:41<14:53,  6.12s/it] 84%|████████▎ | 741/886 [1:15:47<14:46,  6.12s/it] 84%|████████▎ | 742/886 [1:15:53<14:40,  6.12s/it] 84%|████████▍ | 743/886 [1:15:59<14:34,  6.12s/it] 84%|████████▍ | 744/886 [1:16:05<14:28,  6.12s/it] 84%|████████▍ | 745/886 [1:16:12<14:22,  6.12s/it] 84%|████████▍ | 746/886 [1:16:18<14:16,  6.12s/it] 84%|████████▍ | 747/886 [1:16:24<14:10,  6.12s/it] 84%|████████▍ | 748/886 [1:16:30<14:04,  6.12s/it] 85%|████████▍ | 749/886 [1:16:36<13:58,  6.12s/it] 85%|████████▍ | 750/886 [1:16:42<13:51,  6.12s/it] 85%|████████▍ | 751/886 [1:16:48<13:45,  6.12s/it] 85%|████████▍ | 752/886 [1:16:54<13:39,  6.12s/it] 85%|████████▍ | 753/886 [1:17:00<13:33,  6.12s/it] 85%|████████▌ | 754/886 [1:17:07<13:27,  6.12s/it] 85%|████████▌ | 755/886 [1:17:13<13:21,  6.12s/it] 85%|████████▌ | 756/886 [1:17:19<13:15,  6.12s/it] 85%|████████▌ | 757/886 [1:17:25<13:09,  6.12s/it] 86%|████████▌ | 758/886 [1:17:31<13:03,  6.12s/it] 86%|████████▌ | 759/886 [1:17:37<12:57,  6.12s/it] 86%|████████▌ | 760/886 [1:17:43<12:50,  6.12s/it] 86%|████████▌ | 761/886 [1:17:49<12:44,  6.12s/it] 86%|████████▌ | 762/886 [1:17:56<12:38,  6.12s/it] 86%|████████▌ | 763/886 [1:18:02<12:32,  6.12s/it] 86%|████████▌ | 764/886 [1:18:08<12:26,  6.12s/it] 86%|████████▋ | 765/886 [1:18:14<12:20,  6.12s/it] 86%|████████▋ | 766/886 [1:18:20<12:14,  6.12s/it] 87%|████████▋ | 767/886 [1:18:26<12:07,  6.12s/it] 87%|████████▋ | 768/886 [1:18:32<12:01,  6.12s/it] 87%|████████▋ | 769/886 [1:18:38<11:55,  6.12s/it] 87%|████████▋ | 770/886 [1:18:44<11:49,  6.12s/it] 87%|████████▋ | 771/886 [1:18:51<11:43,  6.12s/it] 87%|████████▋ | 772/886 [1:18:57<11:37,  6.12s/it] 87%|████████▋ | 773/886 [1:19:03<11:31,  6.12s/it] 87%|████████▋ | 774/886 [1:19:09<11:25,  6.12s/it] 87%|████████▋ | 775/886 [1:19:15<11:18,  6.12s/it] 88%|████████▊ | 776/886 [1:19:21<11:12,  6.12s/it] 88%|████████▊ | 777/886 [1:19:27<11:06,  6.12s/it] 88%|████████▊ | 778/886 [1:19:33<11:00,  6.12s/it] 88%|████████▊ | 779/886 [1:19:40<10:54,  6.12s/it] 88%|████████▊ | 780/886 [1:19:46<10:48,  6.12s/it] 88%|████████▊ | 781/886 [1:19:52<10:42,  6.11s/it] 88%|████████▊ | 782/886 [1:19:58<10:35,  6.11s/it] 88%|████████▊ | 783/886 [1:20:04<10:29,  6.11s/it] 88%|████████▊ | 784/886 [1:20:10<10:23,  6.12s/it] 89%|████████▊ | 785/886 [1:20:16<10:17,  6.12s/it] 89%|████████▊ | 786/886 [1:20:22<10:11,  6.12s/it] 89%|████████▉ | 787/886 [1:20:28<10:05,  6.12s/it] 89%|████████▉ | 788/886 [1:20:35<09:59,  6.11s/it] 89%|████████▉ | 789/886 [1:20:41<09:53,  6.12s/it] 89%|████████▉ | 790/886 [1:20:47<09:47,  6.12s/it] 89%|████████▉ | 791/886 [1:20:53<09:40,  6.12s/it] 89%|████████▉ | 792/886 [1:20:59<09:34,  6.12s/it] 90%|████████▉ | 793/886 [1:21:05<09:28,  6.12s/it] 90%|████████▉ | 794/886 [1:21:11<09:22,  6.12s/it] 90%|████████▉ | 795/886 [1:21:17<09:16,  6.12s/it] 90%|████████▉ | 796/886 [1:21:23<09:10,  6.12s/it] 90%|████████▉ | 797/886 [1:21:30<09:04,  6.12s/it] 90%|█████████ | 798/886 [1:21:36<08:58,  6.12s/it] 90%|█████████ | 799/886 [1:21:42<08:52,  6.12s/it] 90%|█████████ | 800/886 [1:21:48<08:45,  6.12s/it] 90%|█████████ | 801/886 [1:21:54<08:39,  6.12s/it] 91%|█████████ | 802/886 [1:22:00<08:33,  6.12s/it] 91%|█████████ | 803/886 [1:22:06<08:27,  6.12s/it] 91%|█████████ | 804/886 [1:22:12<08:21,  6.12s/it] 91%|█████████ | 805/886 [1:22:19<08:15,  6.12s/it] 91%|█████████ | 806/886 [1:22:25<08:09,  6.11s/it] 91%|█████████ | 807/886 [1:22:31<08:03,  6.12s/it] 91%|█████████ | 808/886 [1:22:37<07:56,  6.12s/it] 91%|█████████▏| 809/886 [1:22:43<07:50,  6.12s/it] 91%|█████████▏| 810/886 [1:22:49<07:44,  6.11s/it] 92%|█████████▏| 811/886 [1:22:55<07:38,  6.12s/it] 92%|█████████▏| 812/886 [1:23:01<07:32,  6.11s/it] 92%|█████████▏| 813/886 [1:23:07<07:26,  6.11s/it] 92%|█████████▏| 814/886 [1:23:14<07:20,  6.12s/it] 92%|█████████▏| 815/886 [1:23:20<07:14,  6.12s/it] 92%|█████████▏| 816/886 [1:23:26<07:08,  6.12s/it] 92%|█████████▏| 817/886 [1:23:32<07:02,  6.12s/it] 92%|█████████▏| 818/886 [1:23:38<06:55,  6.12s/it] 92%|█████████▏| 819/886 [1:23:44<06:49,  6.12s/it] 93%|█████████▎| 820/886 [1:23:50<06:43,  6.12s/it] 93%|█████████▎| 821/886 [1:23:56<06:37,  6.12s/it] 93%|█████████▎| 822/886 [1:24:02<06:31,  6.11s/it] 93%|█████████▎| 823/886 [1:24:09<06:25,  6.11s/it] 93%|█████████▎| 824/886 [1:24:15<06:19,  6.11s/it] 93%|█████████▎| 825/886 [1:24:21<06:12,  6.11s/it] 93%|█████████▎| 826/886 [1:24:27<06:06,  6.12s/it] 93%|█████████▎| 827/886 [1:24:33<06:00,  6.12s/it] 93%|█████████▎| 828/886 [1:24:39<05:54,  6.12s/it] 94%|█████████▎| 829/886 [1:24:45<05:48,  6.12s/it] 94%|█████████▎| 830/886 [1:24:51<05:42,  6.12s/it] 94%|█████████▍| 831/886 [1:24:58<05:36,  6.12s/it] 94%|█████████▍| 832/886 [1:25:04<05:30,  6.12s/it] 94%|█████████▍| 833/886 [1:25:10<05:24,  6.12s/it] 94%|█████████▍| 834/886 [1:25:16<05:18,  6.12s/it] 94%|█████████▍| 835/886 [1:25:22<05:11,  6.12s/it] 94%|█████████▍| 836/886 [1:25:28<05:05,  6.12s/it] 94%|█████████▍| 837/886 [1:25:34<04:59,  6.12s/it] 95%|█████████▍| 838/886 [1:25:40<04:53,  6.12s/it] 95%|█████████▍| 839/886 [1:25:46<04:47,  6.12s/it] 95%|█████████▍| 840/886 [1:25:53<04:41,  6.11s/it] 95%|█████████▍| 841/886 [1:25:59<04:35,  6.12s/it] 95%|█████████▌| 842/886 [1:26:05<04:29,  6.12s/it] 95%|█████████▌| 843/886 [1:26:11<04:22,  6.12s/it] 95%|█████████▌| 844/886 [1:26:17<04:16,  6.12s/it] 95%|█████████▌| 845/886 [1:26:23<04:10,  6.12s/it] 95%|█████████▌| 846/886 [1:26:29<04:04,  6.12s/it] 96%|█████████▌| 847/886 [1:26:35<03:58,  6.12s/it] 96%|█████████▌| 848/886 [1:26:41<03:52,  6.12s/it] 96%|█████████▌| 849/886 [1:26:48<03:46,  6.12s/it] 96%|█████████▌| 850/886 [1:26:54<03:40,  6.12s/it] 96%|█████████▌| 851/886 [1:27:00<03:34,  6.12s/it] 96%|█████████▌| 852/886 [1:27:06<03:27,  6.12s/it] 96%|█████████▋| 853/886 [1:27:12<03:21,  6.12s/it] 96%|█████████▋| 854/886 [1:27:18<03:15,  6.12s/it] 97%|█████████▋| 855/886 [1:27:24<03:09,  6.12s/it] 97%|█████████▋| 856/886 [1:27:30<03:03,  6.12s/it] 97%|█████████▋| 857/886 [1:27:37<02:57,  6.12s/it] 97%|█████████▋| 858/886 [1:27:43<02:51,  6.12s/it] 97%|█████████▋| 859/886 [1:27:49<02:45,  6.11s/it] 97%|█████████▋| 860/886 [1:27:55<02:38,  6.11s/it] 97%|█████████▋| 861/886 [1:28:01<02:32,  6.12s/it] 97%|█████████▋| 862/886 [1:28:07<02:26,  6.12s/it] 97%|█████████▋| 863/886 [1:28:13<02:20,  6.12s/it] 98%|█████████▊| 864/886 [1:28:19<02:14,  6.12s/it] 98%|█████████▊| 865/886 [1:28:25<02:08,  6.12s/it] 98%|█████████▊| 866/886 [1:28:32<02:02,  6.12s/it] 98%|█████████▊| 867/886 [1:28:38<01:56,  6.12s/it] 98%|█████████▊| 868/886 [1:28:44<01:50,  6.12s/it] 98%|█████████▊| 869/886 [1:28:50<01:43,  6.12s/it] 98%|█████████▊| 870/886 [1:28:56<01:37,  6.12s/it] 98%|█████████▊| 871/886 [1:29:02<01:31,  6.12s/it] 98%|█████████▊| 872/886 [1:29:08<01:25,  6.11s/it] 99%|█████████▊| 873/886 [1:29:14<01:19,  6.12s/it] 99%|█████████▊| 874/886 [1:29:20<01:13,  6.12s/it] 99%|█████████▉| 875/886 [1:29:27<01:07,  6.12s/it] 99%|█████████▉| 876/886 [1:29:33<01:01,  6.12s/it] 99%|█████████▉| 877/886 [1:29:39<00:55,  6.12s/it] 99%|█████████▉| 878/886 [1:29:45<00:48,  6.12s/it] 99%|█████████▉| 879/886 [1:29:51<00:42,  6.12s/it] 99%|█████████▉| 880/886 [1:29:57<00:36,  6.12s/it] 99%|█████████▉| 881/886 [1:30:03<00:30,  6.12s/it]100%|█████████▉| 882/886 [1:30:09<00:24,  6.12s/it]100%|█████████▉| 883/886 [1:30:16<00:18,  6.12s/it]100%|█████████▉| 884/886 [1:30:22<00:12,  6.12s/it]100%|█████████▉| 885/886 [1:30:28<00:06,  6.11s/it]100%|██████████| 886/886 [1:30:34<00:00,  6.08s/it][INFO|trainer.py:2340] 2023-10-30 18:06:11,030 >> Saving model checkpoint to chkpts/analysis_models/d-atomic-roberta-base/checkpoint-886
[INFO|configuration_utils.py:446] 2023-10-30 18:06:11,037 >> Configuration saved in chkpts/analysis_models/d-atomic-roberta-base/checkpoint-886/config.json
[INFO|modeling_utils.py:1542] 2023-10-30 18:06:16,023 >> Model weights saved in chkpts/analysis_models/d-atomic-roberta-base/checkpoint-886/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-10-30 18:06:16,030 >> tokenizer config file saved in chkpts/analysis_models/d-atomic-roberta-base/checkpoint-886/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-10-30 18:06:16,036 >> Special tokens file saved in chkpts/analysis_models/d-atomic-roberta-base/checkpoint-886/special_tokens_map.json
[INFO|trainer.py:1662] 2023-10-30 18:06:26,222 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 886/886 [1:30:49<00:00,  6.08s/it]100%|██████████| 886/886 [1:30:49<00:00,  6.15s/it]
[INFO|trainer.py:2340] 2023-10-30 18:06:26,237 >> Saving model checkpoint to chkpts/analysis_models/d-atomic-roberta-base
[INFO|configuration_utils.py:446] 2023-10-30 18:06:26,242 >> Configuration saved in chkpts/analysis_models/d-atomic-roberta-base/config.json
[INFO|modeling_utils.py:1542] 2023-10-30 18:06:31,216 >> Model weights saved in chkpts/analysis_models/d-atomic-roberta-base/pytorch_model.bin
[INFO|tokenization_utils_base.py:2108] 2023-10-30 18:06:31,221 >> tokenizer config file saved in chkpts/analysis_models/d-atomic-roberta-base/tokenizer_config.json
[INFO|tokenization_utils_base.py:2114] 2023-10-30 18:06:31,225 >> Special tokens file saved in chkpts/analysis_models/d-atomic-roberta-base/special_tokens_map.json
{'loss': 0.6374, 'learning_rate': 2.178329571106095e-06, 'epoch': 1.13}
{'train_runtime': 5449.4944, 'train_samples_per_second': 10.405, 'train_steps_per_second': 0.163, 'train_loss': 0.5945035017370909, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     0.5945
  train_runtime            = 1:30:49.49
  train_samples            =      28350
  train_samples_per_second =     10.405
  train_steps_per_second   =      0.163
10/30/2023 18:06:31 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:622] 2023-10-30 18:06:31,401 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[WARNING|training_args.py:1095] 2023-10-30 18:06:31,402 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-10-30 18:06:31,403 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|trainer.py:2590] 2023-10-30 18:06:31,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2592] 2023-10-30 18:06:31,403 >>   Num examples = 2839
[INFO|trainer.py:2595] 2023-10-30 18:06:31,403 >>   Batch size = 64
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:02<00:45,  1.05s/it]  7%|▋         | 3/45 [00:04<01:02,  1.49s/it]  9%|▉         | 4/45 [00:06<01:10,  1.72s/it] 11%|█         | 5/45 [00:08<01:14,  1.85s/it] 13%|█▎        | 6/45 [00:10<01:15,  1.93s/it] 16%|█▌        | 7/45 [00:12<01:15,  1.99s/it] 18%|█▊        | 8/45 [00:14<01:14,  2.02s/it] 20%|██        | 9/45 [00:16<01:13,  2.05s/it] 22%|██▏       | 10/45 [00:18<01:12,  2.07s/it] 24%|██▍       | 11/45 [00:21<01:10,  2.08s/it] 27%|██▋       | 12/45 [00:23<01:08,  2.09s/it] 29%|██▉       | 13/45 [00:25<01:06,  2.09s/it] 31%|███       | 14/45 [00:27<01:04,  2.09s/it] 33%|███▎      | 15/45 [00:29<01:02,  2.10s/it] 36%|███▌      | 16/45 [00:31<01:00,  2.10s/it] 38%|███▊      | 17/45 [00:33<00:58,  2.10s/it] 40%|████      | 18/45 [00:35<00:56,  2.10s/it] 42%|████▏     | 19/45 [00:37<00:54,  2.10s/it] 44%|████▍     | 20/45 [00:39<00:52,  2.10s/it] 47%|████▋     | 21/45 [00:42<00:50,  2.10s/it] 49%|████▉     | 22/45 [00:44<00:48,  2.10s/it] 51%|█████     | 23/45 [00:46<00:46,  2.10s/it] 53%|█████▎    | 24/45 [00:48<00:44,  2.10s/it] 56%|█████▌    | 25/45 [00:50<00:42,  2.10s/it] 58%|█████▊    | 26/45 [00:52<00:39,  2.10s/it] 60%|██████    | 27/45 [00:54<00:37,  2.10s/it] 62%|██████▏   | 28/45 [00:56<00:35,  2.10s/it] 64%|██████▍   | 29/45 [00:58<00:33,  2.10s/it] 67%|██████▋   | 30/45 [01:00<00:31,  2.10s/it] 69%|██████▉   | 31/45 [01:03<00:29,  2.10s/it] 71%|███████   | 32/45 [01:05<00:27,  2.10s/it] 73%|███████▎  | 33/45 [01:07<00:25,  2.10s/it] 76%|███████▌  | 34/45 [01:09<00:23,  2.10s/it] 78%|███████▊  | 35/45 [01:11<00:21,  2.10s/it] 80%|████████  | 36/45 [01:13<00:18,  2.10s/it] 82%|████████▏ | 37/45 [01:15<00:16,  2.10s/it] 84%|████████▍ | 38/45 [01:17<00:14,  2.10s/it] 87%|████████▋ | 39/45 [01:19<00:12,  2.10s/it] 89%|████████▉ | 40/45 [01:22<00:10,  2.10s/it] 91%|█████████ | 41/45 [01:24<00:08,  2.10s/it] 93%|█████████▎| 42/45 [01:26<00:06,  2.10s/it] 96%|█████████▌| 43/45 [01:28<00:04,  2.10s/it] 98%|█████████▊| 44/45 [01:30<00:02,  2.10s/it]100%|██████████| 45/45 [01:31<00:00,  1.71s/it][WARNING|training_args.py:1095] 2023-10-30 18:08:04,736 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
100%|██████████| 45/45 [01:31<00:00,  2.03s/it]
[WARNING|training_args.py:1095] 2023-10-30 18:08:04,774 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[WARNING|training_args.py:1095] 2023-10-30 18:08:04,774 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.
[INFO|modelcard.py:460] 2023-10-30 18:08:04,956 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7315956354141235}]}
***** eval metrics *****
  epoch                   =        2.0
  eval_accuracy           =     0.7316
  eval_loss               =     0.5312
  eval_runtime            = 0:01:33.33
  eval_samples            =       2839
  eval_samples_per_second =     30.418
  eval_steps_per_second   =      0.482
