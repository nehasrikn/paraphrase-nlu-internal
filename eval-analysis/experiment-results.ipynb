{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1193f9",
   "metadata": {},
   "source": [
    "## Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69c4eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nehasrikanth/miniconda3/envs/para-nlu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import TrainedModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_base = TrainedModel(\n",
    "    '../modeling/chkpts/roberta-base-anli/',\n",
    "    cache_dir='../modeling/hf-cache/',\n",
    ")\n",
    "\n",
    "roberta_large = TrainedModel(\n",
    "    '../modeling/chkpts/roberta-large-anli/',\n",
    "    cache_dir='../modeling/hf-cache/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1e1a1",
   "metadata": {},
   "source": [
    "### Evaluate on full test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5bb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('/Users/nehasrikanth/Documents/paraphrase-nlu/raw-data/anli/test.jsonl', lines=True)\n",
    "test['label'] = pd.read_csv('/Users/nehasrikanth/Documents/paraphrase-nlu/raw-data/anli/test-labels.lst', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['base_pred_prob'] = test.progress_apply(\n",
    "    lambda row: roberta_base._get_prediction(\n",
    "        obs1=row['obs1'], obs2=row['obs2'], hyp1=row['hyp1'], hyp2=row['hyp2']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "test['large_pred_prob'] = test.progress_apply(\n",
    "    lambda row: roberta_large._get_prediction(\n",
    "        obs1=row['obs1'], obs2=row['obs2'], hyp1=row['hyp1'], hyp2=row['hyp2']\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(test.label, test['base-pred'].map(lambda a: np.argmax(a) + 1)))\n",
    "print(accuracy_score(test.label, test['large-pred'].map(lambda a: np.argmax(a) + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02ce8a",
   "metadata": {},
   "source": [
    "Looks like RoBERTa large does significantly better, let's proceed with analysis using that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bd1cc",
   "metadata": {},
   "source": [
    "## Load paraphrase data and evaluate on original examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f398b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_paraphrases = pd.read_csv('../annotated-data/paraphrased_pilot.csv')\n",
    "pilot_paraphrases['paraphrases_by_worker'] = pilot_paraphrases.paraphrases.map(eval)\n",
    "pilot_paraphrases = pilot_paraphrases.drop(columns=['paraphrases', 'processed_assignments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_paraphrases['hyp1_paraphrases'] = pilot_paraphrases.paraphrases_by_worker.map(\n",
    "    lambda x: [p for w in x for p in w['hyp1_paraphrases']]\n",
    ")\n",
    "pilot_paraphrases['hyp2_paraphrases'] = pilot_paraphrases.paraphrases_by_worker.map(\n",
    "    lambda x: [p for w in x for p in w['hyp2_paraphrases']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed40bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pilot_paraphrases['large_pred_prob'] = pilot_paraphrases.progress_apply(\n",
    "    lambda row: roberta_large._get_prediction(\n",
    "        obs1=row['obs1'], obs2=row['obs2'], hyp1=row['hyp1'], hyp2=row['hyp2']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "print(accuracy_score(pilot_paraphrases.label, pilot_paraphrases['large_pred_prob'].map(lambda a: np.argmax(a) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_paraphrases['large_pred'] = pilot_paraphrases['large_pred_prob'].map(lambda a: np.argmax(a) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_paraphrased_example_h1_h2_intra_worker(row):\n",
    "    worker_paraphrases = row.paraphrases_by_worker[random.randrange(0, 3)]\n",
    "    return {\n",
    "        'obs1': row['obs1'], 'obs2': row['obs2'],\n",
    "        'hyp1': worker_paraphrases['hyp1_paraphrases'][random.randrange(0, 3)], \n",
    "        'hyp2': worker_paraphrases['hyp2_paraphrases'][random.randrange(0, 3)]\n",
    "    }\n",
    "\n",
    "def get_random_paraphrased_example_h1_h2_inter_worker(row):\n",
    "    h1_paraphrase = row.paraphrases_by_worker[random.randrange(0, 3)]['hyp1_paraphrases'][random.randrange(0, 3)]\n",
    "    h2_paraphrase = row.paraphrases_by_worker[random.randrange(0, 3)]['hyp2_paraphrases'][random.randrange(0, 3)]\n",
    "    return {\n",
    "        'obs1': row['obs1'], 'obs2': row['obs2'],\n",
    "        'hyp1': h1_paraphrase,\n",
    "        'hyp2': h2_paraphrase,\n",
    "    }\n",
    "    \n",
    "def get_random_paraphrased_example_h1(row):\n",
    "    idx = random.randrange(0, 9)\n",
    "    return {\n",
    "        'obs1': row['obs1'], 'obs2': row['obs2'],\n",
    "        'hyp1': row.hyp1_paraphrases[idx], 'hyp2': row['hyp2']\n",
    "    }\n",
    "\n",
    "def get_random_paraphrased_example_h2(row):\n",
    "    idx = random.randrange(0, 9)\n",
    "    return {\n",
    "        'obs1': row['obs1'], 'obs2': row['obs2'],\n",
    "        'hyp1': row['hyp1'], 'hyp2': row.hyp2_paraphrases[idx]\n",
    "    }\n",
    "\n",
    "def get_zipped_examples(row):\n",
    "    return [{'obs1': row['obs1'], 'obs2': row['obs2'], 'hyp1': h1, 'hyp2': h2} \n",
    "            for h1, h2 in zip(row.hyp1_paraphrases, row.hyp2_paraphrases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_paraphrases['rand_intra_worker_paraphrased_h1_h2'] = pilot_paraphrases.apply(\n",
    "    get_random_paraphrased_example_h1_h2_intra_worker, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_paraphrases['rand_intra_worker_paraphrased_h1_h2_pred'] = pilot_paraphrases['rand_intra_worker_paraphrased_h1_h2'].progress_map(\n",
    "    lambda r: np.argmax(roberta_large._get_prediction(**r)) + 1\n",
    ")\n",
    "accuracy_score(pilot_paraphrases.label, pilot_paraphrases['rand_intra_worker_paraphrased_h1_h2_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flip_rate(old_pred, new_pred):\n",
    "    return sum(old_pred != new_pred) / len(old_pred)\n",
    "\n",
    "print(calculate_flip_rate(pilot_paraphrases['large_pred'], pilot_paraphrases['rand_intra_worker_paraphrased_h1_h2_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712da842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('✅, ✅', len(pilot_paraphrases[\n",
    "    (pilot_paraphrases.large_pred == pilot_paraphrases.label) & \n",
    "    (pilot_paraphrases.rand_intra_worker_paraphrased_h1_h2_pred == pilot_paraphrases.label)\n",
    "])/len(pilot_paraphrases))\n",
    "\n",
    "print('❌, ✅', len(pilot_paraphrases[\n",
    "    (pilot_paraphrases.large_pred != pilot_paraphrases.label) & \n",
    "    (pilot_paraphrases.rand_intra_worker_paraphrased_h1_h2_pred == pilot_paraphrases.label)\n",
    "])/len(pilot_paraphrases))\n",
    "\n",
    "print('✅, ❌', len(pilot_paraphrases[\n",
    "    (pilot_paraphrases.large_pred == pilot_paraphrases.label) & \n",
    "    (pilot_paraphrases.rand_intra_worker_paraphrased_h1_h2_pred != pilot_paraphrases.label)\n",
    "])/len(pilot_paraphrases))\n",
    "print('❌, ❌', len(pilot_paraphrases[\n",
    "    (pilot_paraphrases.large_pred != pilot_paraphrases.label) & \n",
    "    (pilot_paraphrases.rand_intra_worker_paraphrased_h1_h2_pred != pilot_paraphrases.label)\n",
    "])/len(pilot_paraphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_paraphrases['rand_inter_worker_paraphrased_h1_h2'] = pilot_paraphrases.apply(\n",
    "    get_random_paraphrased_example_h1_h2_inter_worker, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_paraphrases['rand_inter_worker_paraphrased_h1_h2_pred'] = pilot_paraphrases['rand_inter_worker_paraphrased_h1_h2'].progress_map(\n",
    "    lambda r: np.argmax(roberta_large._get_prediction(**r)) + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pilot_paraphrases.label, pilot_paraphrases['rand_inter_worker_paraphrased_h1_h2_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a24025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(calculate_flip_rate(pilot_paraphrases['large_pred'], pilot_paraphrases['rand_intra_worker_paraphrased_h1_h2_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afd0ef",
   "metadata": {},
   "source": [
    "### Transformation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_wrong = pilot_paraphrases[\n",
    "    (pilot_paraphrases.large_pred == pilot_paraphrases.label) & \n",
    "    (pilot_paraphrases.rand_intra_worker_paraphrased_h1_h2_pred != pilot_paraphrases.label)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(flipped_wrong.apply(lambda i: {\n",
    "    'obs1': i.obs1,\n",
    "    'obs2': i.obs2,\n",
    "    'hyp1': i.hyp1,\n",
    "    'hyp1-para': i.rand_intra_worker_paraphrased_h1_h2['hyp1'],\n",
    "    'hyp2': i.hyp2,\n",
    "    'hyp2-para': i.rand_intra_worker_paraphrased_h1_h2['hyp2']}, axis=1))).to_csv('flipped_ex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a1145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25830e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc0eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63677a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_examples = pilot_paraphrases.apply(get_zipped_examples, axis=1)\n",
    "\n",
    "zipped_examples = pd.concat([\n",
    "    pilot_paraphrases[['label', 'large-pred', 'hyp1', 'hyp2']],\n",
    "    zipped_examples.progress_map(lambda example_list: [roberta_large._get_prediction (**e) for e in example_list])\n",
    "], axis=1\n",
    ").rename(columns={0: 'para-preds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033324da",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_examples['para-preds-argmax'] = zipped_examples['para-preds'].map(lambda x: [np.argmax(i) + 1 for i in x])\n",
    "\n",
    "zipped_examples.apply(\n",
    "    lambda row: sum(1 for i in row['para-preds-argmax'] if i == row.label)/len(row['para-preds-argmax']), axis=1\n",
    ").plot.hist(bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5f6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db14b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_sample = []\n",
    "for i, row in pilot_paraphrases.iterrows():\n",
    "    for h1, h2 in list(zip(row.hyp1_paraphrases, row.hyp2_paraphrases)):\n",
    "        zipped_sample.append({\n",
    "            'id': i,\n",
    "            'example_id': row.example_id,\n",
    "            'obs1': row.obs1,\n",
    "            'obs2': row.obs2,\n",
    "            'original_h1': row.hyp1,\n",
    "            'original_h2': row.hyp2,\n",
    "            'hyp1': h1,\n",
    "            'hyp2': h2,\n",
    "            'label': row.label\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unaligned_sample = []\n",
    "for i, row in pilot_paraphrases.iterrows():\n",
    "    w1 = random.randint(0, 2)\n",
    "    w2 = random.choice(list(set([0, 1, 2]) - set([w1])))\n",
    "    \n",
    "    unaligned_sample.append({\n",
    "        'id': i,\n",
    "        'example_id': row.example_id,\n",
    "        'obs1': row.obs1,\n",
    "        'obs2': row.obs2,\n",
    "        'original_h1': row.hyp1,\n",
    "        'original_h2': row.hyp2,\n",
    "        'hyp1': random.choice(row.paraphrases_by_worker[w1]['hyp1_paraphrases']),\n",
    "        'hyp2': random.choice(row.paraphrases_by_worker[w2]['hyp2_paraphrases']),\n",
    "        'label': row.label\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b738d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zipped_sample).sample(100, random_state=42).to_csv('zipped_examples_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(unaligned_sample).sample(100, random_state=42).to_csv('unaligned_examples_validation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:para-nlu]",
   "language": "python",
   "name": "conda-env-para-nlu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
